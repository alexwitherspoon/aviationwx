name: Deploy to Production

on:
  workflow_run:
    # Only trigger on Quality Assurance Tests completion (typically runs last)
    # The check-prerequisites job will verify BOTH workflows have succeeded
    # IMPORTANT: Only trigger on successful completion to prevent loops
    workflows: ["Quality Assurance Tests"]
    types:
      - completed
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      skip_prerequisites:
        description: 'Skip prerequisite checks (DANGEROUS - only use in emergencies)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      deployment_reason:
        description: 'Reason for manual deployment'
        required: true
        type: string

# Ensure only one deployment runs at a time
concurrency:
  group: deploy-production
  cancel-in-progress: false

# Permissions for deployment
permissions:
  contents: read
  actions: read
  checks: read

jobs:
  check-prerequisites:
    runs-on: ubuntu-latest
    timeout-minutes: 35  # Prevent infinite loops
    outputs:
      test-success: ${{ steps.check.outputs.test-success }}
      qa-success: ${{ steps.check.outputs.qa-success }}
      skip-checks: ${{ steps.check.outputs.skip-checks }}
    steps:
      - name: Wait for and check required workflows status
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            const skipInput = '${{ github.event.inputs.skip_prerequisites }}';
            const isManualDispatch = '${{ github.event_name }}' === 'workflow_dispatch';
            const deploymentReason = '${{ github.event.inputs.deployment_reason }}';
            
            // CRITICAL: Check if the triggering QA workflow run succeeded
            // This prevents loops when QA tests fail - we should not deploy
            if (!isManualDispatch && '${{ github.event.workflow_run.conclusion }}' !== 'success') {
              const conclusion = '${{ github.event.workflow_run.conclusion }}' || 'unknown';
              core.info(`⏭️  Skipping deployment - QA workflow did not succeed (conclusion: ${conclusion})`);
              core.info('Only successful QA test runs trigger deployment to prevent loops');
              core.setOutput('test-success', 'false');
              core.setOutput('qa-success', 'false');
              core.setOutput('skip-checks', 'false');
              return;
            }
            
            if (isManualDispatch) {
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              core.info(`Manual deployment triggered`);
              core.info(`Reason: ${deploymentReason || 'Not provided'}`);
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              
              if (skipInput === 'true') {
                core.warning('');
                core.warning('⚠️  WARNING: Prerequisite checks are being SKIPPED');
                core.warning('This is DANGEROUS and should only be used in emergencies');
                core.warning('Deployment will proceed without verifying tests passed');
                core.warning('');
                core.setOutput('skip-checks', 'true');
                core.setOutput('test-success', 'true');  // Fake success to allow deployment
                core.setOutput('qa-success', 'true');
                return;
              } else {
                core.info('Prerequisite checks will be performed (recommended)');
                core.setOutput('skip-checks', 'false');
              }
            } else {
              core.setOutput('skip-checks', 'false');
            }
            
            // Only deploy on push events (commits/merges to main), not on pull_request events
            // workflow_run event contains the event that triggered the source workflow
            const triggeringEvent = '${{ github.event.workflow_run && github.event.workflow_run.event || github.event_name }}';
            
            core.info(`Triggering event: ${triggeringEvent}`);
            
            // Skip deployment if triggered by a pull_request event
            if (triggeringEvent === 'pull_request') {
              core.info('⏭️  Skipping deployment - triggered by pull_request event (only deploy on push/merge to main)');
              core.setOutput('test-success', 'false');
              core.setOutput('qa-success', 'false');
              return;
            }
            
            // For manual dispatch, we've already handled it above
            // For workflow_run, verify this is a push event
            if (triggeringEvent !== 'push' && !isManualDispatch) {
              core.warning(`⚠️  Unexpected event type: ${triggeringEvent}. Only push events trigger deployment.`);
              core.setOutput('test-success', 'false');
              core.setOutput('qa-success', 'false');
              return;
            }
            
            // Get the commit SHA - use workflow_run head_sha if available, otherwise context.sha
            const commitSha = '${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}';
            
            // Get the triggering workflow run ID if available (from workflow_run event)
            // GitHub Actions expressions can't use empty strings directly, so we check in JavaScript
            const triggeringWorkflowRunIdRaw = '${{ github.event.workflow_run && github.event.workflow_run.id }}';
            const triggeringWorkflowRunId = (triggeringWorkflowRunIdRaw && triggeringWorkflowRunIdRaw !== '') ? triggeringWorkflowRunIdRaw : null;
            
            core.info(`Checking workflow statuses for commit: ${commitSha}`);
            if (triggeringWorkflowRunId) {
              core.info(`Triggered by workflow run ID: ${triggeringWorkflowRunId}`);
            }
            
            // Prevent duplicate deployments: Check if this commit is already being deployed
            // This prevents loops when multiple workflow_run events fire for the same commit
            if (!isManualDispatch) {
              const deployWorkflowPath = '.github/workflows/deploy-docker.yml';
              const existingDeployRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: deployWorkflowPath,
                head_sha: commitSha,
                per_page: 5
              });
              
              // Count in-progress or queued deployments for this commit
              const inProgressDeploys = existingDeployRuns.data.workflow_runs.filter(run => 
                run.status === 'in_progress' || run.status === 'queued'
              );
              
              if (inProgressDeploys.length > 0) {
                core.info(`⏭️  Skipping deployment - another deployment for commit ${commitSha.substring(0, 7)} is already in progress`);
                core.info(`Found ${inProgressDeploys.length} in-progress deployment(s) for this commit`);
                core.setOutput('test-success', 'false');
                core.setOutput('qa-success', 'false');
                core.setOutput('skip-checks', 'false');
                return;
              }
            }
            
            const maxWaitTime = 30 * 60 * 1000; // 30 minutes in milliseconds
            const checkInterval = 30 * 1000; // Check every 30 seconds
            const startTime = Date.now();
            
            let testRun = null;
            let qaRun = null;
            let testSuccess = false;
            let qaSuccess = false;
            
            // Use workflow file paths instead of IDs (more reliable)
            const testWorkflowPath = '.github/workflows/test.yml';
            const qaWorkflowPath = '.github/workflows/quality-assurance-tests.yml';
            
            core.info(`Looking for workflows: ${testWorkflowPath} and ${qaWorkflowPath}`);
            
            // Wait for both workflows to complete
            while (Date.now() - startTime < maxWaitTime) {
              // Get workflow runs by workflow file path for more specific filtering
              // This ensures we get runs for the specific workflow, not just by name
              const testRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: testWorkflowPath,
                head_sha: commitSha,
                per_page: 10
              });
              
              const qaRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: qaWorkflowPath,
                head_sha: commitSha,
                per_page: 10
              });
              
              // Find the most recent run for this commit (should be the first one)
              // Filter to ensure it's from a push event (not pull_request) and matches the exact commit SHA
              testRun = testRuns.data.workflow_runs.find(run => 
                run.head_sha === commitSha && 
                (run.event === 'push' || run.event === 'workflow_dispatch')
              );
              
              // If no exact match, try the first run (might be a race condition)
              if (!testRun && testRuns.data.workflow_runs.length > 0) {
                const firstRun = testRuns.data.workflow_runs[0];
                // Only use if it's a push event and matches commit (within reasonable time window)
                if (firstRun.event === 'push' && firstRun.head_sha === commitSha) {
                  testRun = firstRun;
                }
              }
              
              // If we have a triggering workflow run ID, use that to find the exact QA run
              if (triggeringWorkflowRunId) {
                qaRun = qaRuns.data.workflow_runs.find(run => 
                  run.id.toString() === triggeringWorkflowRunId
                );
              }
              
              // If not found by ID, try finding by commit SHA and event type
              if (!qaRun) {
                qaRun = qaRuns.data.workflow_runs.find(run => 
                  run.head_sha === commitSha && 
                  (run.event === 'push' || run.event === 'workflow_dispatch')
                );
              }
              
              // If no exact match, try the first run
              if (!qaRun && qaRuns.data.workflow_runs.length > 0) {
                const firstRun = qaRuns.data.workflow_runs[0];
                // Only use if it's a push event and matches commit (within reasonable time window)
                if (firstRun.event === 'push' && firstRun.head_sha === commitSha) {
                  qaRun = firstRun;
                }
              }
              
              const testStatus = testRun?.status; // queued, in_progress, completed
              const testConclusion = testRun?.conclusion; // success, failure, cancelled, etc.
              const qaStatus = qaRun?.status;
              const qaConclusion = qaRun?.conclusion;
              
              core.info(`Test and Lint: ${testStatus || 'not found'} (${testConclusion || 'pending'}) - Run ID: ${testRun?.id || 'N/A'}`);
              core.info(`Quality Assurance Tests: ${qaStatus || 'not found'} (${qaConclusion || 'pending'}) - Run ID: ${qaRun?.id || 'N/A'}`);
              
              // If workflows haven't started yet (queued), wait a bit longer
              if ((testStatus === 'queued' || !testRun) && (qaStatus === 'queued' || !qaRun)) {
                core.info('Workflows are queued or not found yet, waiting...');
                await new Promise(resolve => setTimeout(resolve, checkInterval));
                continue;
              }
              
              // Check if both workflows have completed
              if (testStatus === 'completed' && qaStatus === 'completed') {
                testSuccess = testConclusion === 'success';
                qaSuccess = qaConclusion === 'success';
                break;
              }
              
              // If one has failed, we can stop waiting (but still check both conclusions)
              if (testStatus === 'completed' && testConclusion !== 'success' && testConclusion !== null) {
                testSuccess = false;
                if (qaStatus === 'completed') {
                  qaSuccess = qaConclusion === 'success';
                  break;
                }
              }
              
              if (qaStatus === 'completed' && qaConclusion !== 'success' && qaConclusion !== null) {
                qaSuccess = false;
                if (testStatus === 'completed') {
                  testSuccess = testConclusion === 'success';
                  break;
                }
              }
              
              // Wait before checking again
              core.info('Waiting for both workflows to complete...');
              await new Promise(resolve => setTimeout(resolve, checkInterval));
            }
            
            // Final check - ensure we have both workflows
            if (!testRun || !qaRun) {
              // Get diagnostic info for debugging
              const allTestRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: testWorkflowPath,
                per_page: 5
              });
              
              const allQaRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: qaWorkflowPath,
                per_page: 5
              });
              
              core.error(`❌ Could not find one or both required workflows for commit ${commitSha}`);
              core.error(`Recent Test and Lint runs: ${allTestRuns.data.workflow_runs.map(r => `${r.head_sha.substring(0, 7)} (${r.status})`).join(', ')}`);
              core.error(`Recent Quality Assurance Tests runs: ${allQaRuns.data.workflow_runs.map(r => `${r.head_sha.substring(0, 7)} (${r.status})`).join(', ')}`);
              core.error(`Looking for commit: ${commitSha}`);
              
              if (!testRun) {
                core.error(`Test and Lint: No run found for commit ${commitSha}`);
              }
              if (!qaRun) {
                core.error(`Quality Assurance Tests: No run found for commit ${commitSha}`);
              }
              
              core.setFailed('❌ Could not find one or both required workflows for this commit');
              return;
            }
            
            if (testRun.status !== 'completed' || qaRun.status !== 'completed') {
              core.setFailed(`❌ One or both workflows are still running after wait period:
                - Test and Lint: ${testRun.status} (${testRun.conclusion || 'pending'})
                - Quality Assurance Tests: ${qaRun.status} (${qaRun.conclusion || 'pending'})`);
              return;
            }
            
            core.setOutput('test-success', testSuccess ? 'true' : 'false');
            core.setOutput('qa-success', qaSuccess ? 'true' : 'false');
            
            core.info(`Test and Lint: ${testRun.conclusion || 'unknown'} (ID: ${testRun.id})`);
            core.info(`Quality Assurance Tests: ${qaRun.conclusion || 'unknown'} (ID: ${qaRun.id})`);
            
            if (!testSuccess) {
              core.error(`❌ Test and Lint workflow did not succeed for commit ${commitSha}`);
            }
            if (!qaSuccess) {
              core.error(`❌ Quality Assurance Tests workflow did not succeed for commit ${commitSha}`);
            }
            
            if (testSuccess && qaSuccess) {
              core.info('✅ Both required workflows succeeded - deployment will proceed');
            } else {
              core.setFailed('❌ Not all required workflows succeeded. Deployment blocked.');
            }

  deploy:
    runs-on: ubuntu-latest
    needs: check-prerequisites
    timeout-minutes: 60  # Prevent infinite loops - deployment should complete within 60 minutes
    # Only deploy if both workflows succeeded (or manual dispatch with skip option)
    if: |
      (needs.check-prerequisites.outputs.test-success == 'true' && 
       needs.check-prerequisites.outputs.qa-success == 'true') || 
      (github.event_name == 'workflow_dispatch' && needs.check-prerequisites.outputs.skip-checks == 'true')
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # When triggered by workflow_run, checkout the commit from that workflow
          # Otherwise use the commit SHA from the prerequisites check
          ref: ${{ github.event.workflow_run.head_sha || github.sha }}
          fetch-depth: 0

      - name: Load deployment helpers
        run: |
          # Source deployment helper functions for consistent error handling
          source scripts/deploy-helpers.sh
          echo "✓ Deployment helpers loaded"

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: |
            ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add host to known_hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.HOST }} >> ~/.ssh/known_hosts

      - name: Pre-deployment validation (block bad code)
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Pre-Deployment Validation"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Blocking bad code before it reaches production..."
          echo ""
          
          VALIDATION_FAILED=false
          FAILURE_REASONS=()
          
          # Validation 1: Check for common silent failure patterns
          echo "1. Checking for critical files..."
          CRITICAL_FILES=(
            "api/weather.php"
            "lib/config.php"
            "lib/rate-limit.php"
            "lib/circuit-breaker.php"
            "lib/constants.php"
            "scripts/process-push-webcams.php"
            "scripts/sync-push-config.php"
            "scripts/create-sftp-user.sh"
            "scripts/service-watchdog.sh"
            "lib/push-webcam-validator.php"
            "pages/config-generator.php"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-compose.prod.yml"
            "docker/Dockerfile"
          )
          
          for file in "${CRITICAL_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Critical file missing: $file")
            fi
          done
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ All critical files present"
          fi
          
          # Validation 2: Check for syntax errors that might be missed
          echo ""
          echo "2. Checking PHP syntax..."
          PHP_ERRORS=$(find . -name "*.php" -not -path "./vendor/*" -exec php -l {} \; 2>&1 | grep -v "No syntax errors" || true)
          if [ -n "$PHP_ERRORS" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("PHP syntax errors found")
            echo "❌ PHP syntax errors:"
            echo "$PHP_ERRORS"
          else
            echo "✓ PHP syntax valid"
          fi
          
          # Validation 3: Verify required functions exist
          echo ""
          echo "3. Verifying required functions exist..."
          if ! grep -q "function checkRateLimit" lib/rate-limit.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkRateLimit function missing in lib/rate-limit.php")
          fi
          if ! grep -q "function validateAirportId" lib/config.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("validateAirportId function missing in lib/config.php")
          fi
          if ! grep -q "function checkWeatherCircuitBreaker" lib/circuit-breaker.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkWeatherCircuitBreaker function missing in lib/circuit-breaker.php")
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Required functions present"
          fi
          
          # Validation 4: Check Docker configuration
          echo ""
          echo "4. Validating Docker configuration..."
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("docker-compose.prod.yml missing")
          fi
          if [ ! -f "docker/Dockerfile" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("Dockerfile missing")
          fi
          
          # Check Dockerfile has required components
          if [ -f "docker/Dockerfile" ]; then
            if ! grep -q "FROM php:" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing PHP base image")
            fi
            if ! grep -q "fail2ban" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing fail2ban installation")
            fi
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Docker configuration valid"
          fi
          
          # Validation 5: Check JSON files are valid
          echo ""
          echo "5. Validating JSON configuration files..."
          if [ -f "config/airports.json.example" ]; then
            if ! php -r "json_decode(file_get_contents('config/airports.json.example'), true); if (json_last_error() !== JSON_ERROR_NONE) { exit(1); }" 2>/dev/null; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("config/airports.json.example is invalid JSON")
            else
              echo "✓ config/airports.json.example is valid JSON"
            fi
          fi
          
          # Final check
          if [ "$VALIDATION_FAILED" = "true" ]; then
            echo ""
            # Use deployment helper for consistent error formatting
            REASONS_TEXT=$(printf "  ❌ %s\n" "${FAILURE_REASONS[@]}")
            deployment_error \
              "Pre-Deployment Validation" \
              "The following issues were found:\n${REASONS_TEXT}" \
              "1. Review the validation errors above
2. Fix syntax errors, missing files, or configuration issues
3. Ensure all required functions exist in their respective files
4. Verify Docker configuration files are present and valid
5. This validation prevents silent failures from reaching production"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Pre-deployment validation passed - code is ready for production"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      
      - name: Update cache version for cache busting
        run: |
          # Generate a unique version based on commit SHA and timestamp
          DEPLOY_VERSION="$(git rev-parse --short HEAD)-$(date +%s)"
          echo "Updating cache version to: ${DEPLOY_VERSION}"
          
          # Update the cache version in public/js/service-worker.js
          # The script handles missing files and verification
          bash scripts/deploy-update-cache-version.sh "${DEPLOY_VERSION}"
          
          # Note: We don't commit service-worker.js changes during deployment
          # The updated file will be synced to the server via rsync
          # This avoids creating unnecessary commits and deployment loops
          echo "✓ Cache version updated (will be synced to server)"

      - name: Clean up old directory structure on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Clean up leftover docker/docker directory from old path issue (now fixed)
          # This is defensive cleanup - the directory should no longer be created
          if [ -d docker/docker ] || [ -f docker/docker/nginx.conf ]; then
            echo "Cleaning up old docker/docker directory structure..."
            rm -rf docker/docker 2>/dev/null || true
            echo "✓ Cleanup complete"
          else
            echo "✓ No cleanup needed (old directory structure not found)"
          fi
          EOF

      - name: Tag current working deployment and save state
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          ROLLBACK_STATE_FILE="/tmp/aviationwx-rollback-state.json"
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Saving Rollback State"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Capturing current working state before deployment..."
          echo ""
          
          # Get current image tag (if exists)
          CURRENT_IMAGE=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].Image // "null"' || echo "null")
          
          # Get current commit SHA from container (if available)
          CURRENT_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          # Get current container status
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].State // "unknown"' || echo "unknown")
          
          # Generate rollback tag
          ROLLBACK_TAG="rollback-$(date +%Y%m%d-%H%M%S)"
          
          # Save rollback state
          ROLLBACK_STATE=$(cat << STATE_JSON
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "image": "${CURRENT_IMAGE}",
            "commit_sha": "${CURRENT_SHA}",
            "container_status": "${CONTAINER_STATUS}",
            "rollback_tag": "${ROLLBACK_TAG}"
          }
          STATE_JSON
          )
          
          echo "$ROLLBACK_STATE" > "$ROLLBACK_STATE_FILE"
          
          if [ "$CURRENT_IMAGE" != "null" ] && [ -n "$CURRENT_IMAGE" ]; then
            # Tag current image as rollback target
            echo "Tagging current image for rollback: $ROLLBACK_TAG"
            if docker tag "$CURRENT_IMAGE" "aviationwx:$ROLLBACK_TAG" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
              
              # Keep only last 3 rollback tags (cleanup old ones)
              docker images "aviationwx:rollback-*" --format "{{.Tag}}" 2>/dev/null | sort -r | tail -n +4 | while read tag; do
                echo "Removing old rollback tag: $tag"
                docker rmi "aviationwx:$tag" 2>/dev/null || true
              done
            else
              echo "⚠️  Warning: Failed to tag current image (may not exist yet)"
            fi
          else
            echo "⚠️  Warning: No current image found - cannot create rollback image"
            echo "Rollback will restore files only (no image rollback)"
          fi
          
          echo ""
          echo "Rollback state saved:"
          echo "$ROLLBACK_STATE" | jq '.' 2>/dev/null || echo "$ROLLBACK_STATE"
          echo ""
          echo "✓ Rollback state saved to: $ROLLBACK_STATE_FILE"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF

      - name: Ensure SSL certificates are in place
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Verify we're running as the correct user
          CURRENT_USER=$(whoami)
          echo "Running as user: $CURRENT_USER"
          
          # Create SSL directory if it doesn't exist
          mkdir -p ssl
          
          # Copy SSL certificates from Let's Encrypt if they exist
          # This ensures certificates are available for Nginx container
          # All operations run as aviationwx user, using sudo for protected operations
          # Use sudo test to check if files exist (directory permissions are root-only)
          if sudo test -f /etc/letsencrypt/live/aviationwx.org/fullchain.pem && sudo test -f /etc/letsencrypt/live/aviationwx.org/privkey.pem; then
            echo "Copying SSL certificates from Let's Encrypt..."
            # Copy certificates (requires sudo to read from /etc/letsencrypt/)
            if sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ssl/; then
              if sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ssl/; then
                # Set correct ownership and permissions
                sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/
                sudo chmod 644 ssl/fullchain.pem
                sudo chmod 600 ssl/privkey.pem
                echo "✓ SSL certificates copied successfully"
              else
                echo "⚠️  Failed to copy privkey.pem"
                exit 1
              fi
            else
              echo "⚠️  Failed to copy fullchain.pem"
              exit 1
            fi
          elif [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates already exist in ~/aviationwx/ssl/"
            # Verify permissions are correct
            sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/ || true
            sudo chmod 644 ssl/fullchain.pem || true
            sudo chmod 600 ssl/privkey.pem || true
          else
            echo "⚠️  SSL certificates not found in /etc/letsencrypt/live/aviationwx.org/ or ~/aviationwx/ssl/"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "SSL CERTIFICATES MISSING"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Nginx container will fail to start without SSL certificates."
            echo "You need to generate certificates with Certbot first."
            echo ""
            
            # Check if Certbot is installed
            if command -v certbot >/dev/null 2>&1; then
              echo "✓ Certbot is installed"
              echo ""
              echo "To generate wildcard certificates (DNS-01 challenge):"
              echo ""
              echo "  1. Install Cloudflare DNS plugin:"
              echo "     sudo apt install certbot python3-certbot-dns-cloudflare -y"
              echo ""
              echo "  2. Create Cloudflare API token (scoped to zone):"
              echo "     - Permissions: Zone → DNS → Edit; Zone → Zone → Read"
              echo "     - Resources: Include → Specific zone → aviationwx.org"
              echo ""
              echo "  3. Store token:"
              echo "     mkdir -p ~/.secrets"
              echo "     printf 'dns_cloudflare_api_token = %s\\n' 'YOUR_TOKEN' > ~/.secrets/cloudflare.ini"
              echo "     chmod 600 ~/.secrets/cloudflare.ini"
              echo ""
              echo "  4. Generate certificates:"
              echo "     sudo certbot certonly \\"
              echo "       --dns-cloudflare \\"
              echo "       --dns-cloudflare-credentials ~/.secrets/cloudflare.ini \\"
              echo "       -d aviationwx.org -d '*.aviationwx.org' \\"
              echo "       --non-interactive --agree-tos -m your@email.com"
              echo ""
              echo "  5. Copy certificates:"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ~/aviationwx/ssl/"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ~/aviationwx/ssl/"
              echo "     sudo chown -R $CURRENT_USER:$CURRENT_USER ~/aviationwx/ssl"
              echo "     sudo chmod 644 ~/aviationwx/ssl/fullchain.pem"
              echo "     sudo chmod 600 ~/aviationwx/ssl/privkey.pem"
            else
              echo "⚠️  Certbot is not installed"
              echo ""
              echo "To install Certbot and generate certificates:"
              echo ""
              echo "  1. Install Certbot with Cloudflare DNS plugin:"
              echo "     sudo apt update && sudo apt install -y certbot python3-certbot-dns-cloudflare"
              echo ""
              echo "  2. Follow steps 2-5 above (create Cloudflare token, generate certs, copy them)"
            fi
            
            echo ""
            echo "After certificates are in place, redeploy to continue."
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            # Fail the deployment since Nginx won't start without certificates
            exit 1
          fi
          EOF

      - name: Configure firewall ports
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Configuring firewall ports for all services..."
          
          # Run firewall configuration script
          if [ -f scripts/deploy-configure-firewall.sh ]; then
            chmod +x scripts/deploy-configure-firewall.sh
            ./scripts/deploy-configure-firewall.sh
          else
            echo "⚠️  Firewall script not found, configuring ports manually..."
            # Fallback: configure ports directly (should match deploy-configure-firewall.sh)
            sudo ufw allow 80/tcp comment 'HTTP (Nginx)' || true
            sudo ufw allow 443/tcp comment 'HTTPS (Nginx)' || true
            sudo ufw allow 2121/tcp comment 'FTP/FTPS (Push webcams)' || true
            sudo ufw allow 2222/tcp comment 'SFTP (Push webcams)' || true
            sudo ufw allow 50000:50100/tcp comment 'FTP passive mode (Push webcams)' || true
            sudo ufw allow 22/tcp comment 'SSH (System access)' || true
            sudo ufw allow 500/udp comment 'IPsec IKE (VPN)' || true
            sudo ufw allow 4500/udp comment 'IPsec NAT-T (VPN)' || true
            sudo ufw status numbered
          fi
          EOF

      - name: Ensure cache directory exists
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          # Get www-data UID/GID from container (typically 33:33)
          # If container is running, get actual UID/GID from it
          WWW_DATA_UID=33
          WWW_DATA_GID=33
          if docker ps --format '{{.Names}}' | grep -q '^aviationwx-web$'; then
            # Container is running - get actual UID/GID
            CONTAINER_UID=$(docker exec aviationwx-web id -u www-data 2>/dev/null || echo "33")
            CONTAINER_GID=$(docker exec aviationwx-web id -g www-data 2>/dev/null || echo "33")
            WWW_DATA_UID=${CONTAINER_UID}
            WWW_DATA_GID=${CONTAINER_GID}
            echo "Detected www-data UID/GID from container: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          else
            echo "Container not running, using default www-data UID/GID: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          fi
          
          # Check if parent directory exists and who owns it
          if [ -d /tmp/aviationwx-cache ]; then
            CURRENT_OWNER=$(stat -c '%U:%G' /tmp/aviationwx-cache 2>/dev/null || stat -f '%Su:%Sg' /tmp/aviationwx-cache 2>/dev/null || echo "unknown")
            echo "Existing cache directory owner: ${CURRENT_OWNER}"
          fi
          
          # Try to create directory structure without sudo first
          if mkdir -p /tmp/aviationwx-cache/webcams 2>/dev/null; then
            echo "✓ Created cache directory without sudo"
          else
            echo "⚠️  Failed to create directory without sudo, trying with sudo..."
            # Use sudo to create directory structure
            sudo mkdir -p /tmp/aviationwx-cache/webcams || {
              echo "❌ Failed to create cache directory even with sudo"
              exit 1
            }
            echo "✓ Created cache directory with sudo"
          fi
          
          # Set ownership to match container's www-data user
          # Try without sudo first, then with sudo if needed
          if chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache 2>/dev/null; then
            echo "✓ Changed ownership without sudo"
          else
            echo "⚠️  Failed to change ownership without sudo, trying with sudo..."
            sudo chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache || {
              echo "⚠️  Failed to change ownership with sudo, using fallback permissions"
              # Fallback: make world-writable (less secure but works)
              sudo chmod -R 777 /tmp/aviationwx-cache || true
            }
          fi
          
          # Ensure directory permissions allow www-data to write
          # Use 775 for webcams (group writable) and 755 for parent
          sudo chmod 755 /tmp/aviationwx-cache 2>/dev/null || chmod 755 /tmp/aviationwx-cache || true
          sudo chmod 775 /tmp/aviationwx-cache/webcams 2>/dev/null || chmod 775 /tmp/aviationwx-cache/webcams || true
          
          # Verify directory was created and is writable
          if [ ! -d /tmp/aviationwx-cache/webcams ]; then
            echo "❌ Failed to create cache directory"
            exit 1
          fi
          
          # Show final permissions
          echo "Final directory permissions:"
          ls -ld /tmp/aviationwx-cache
          ls -ld /tmp/aviationwx-cache/webcams
          
          # Test write access (as current user, which should work if permissions are correct)
          if touch /tmp/aviationwx-cache/webcams/.test_write 2>/dev/null; then
            rm -f /tmp/aviationwx-cache/webcams/.test_write
            echo "✓ Cache directory is writable by deployment user"
          else
            echo "⚠️  Cache directory may not be writable by deployment user"
            echo "   (This is OK if container user can write)"
          fi
          
          # Final verification
          echo "✓ Cache directory created at /tmp/aviationwx-cache/webcams"
          echo "✓ Ownership set to ${WWW_DATA_UID}:${WWW_DATA_GID} (www-data)"
          EOF

      - name: Sync repository to server
        run: |
          echo "Syncing repository to server..."
          # Exclude docker/docker from rsync (defensive - should not exist after path fix)
          # Exclude uploads directory (ephemeral, created inside container)
          # Exclude ssl directory (contains SSL certificates copied from Let's Encrypt, not in git)
          if ! rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' --exclude 'ssl' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/; then
            deployment_error \
              "File Synchronization (Rsync)" \
              "Rsync failed - files may not be synced correctly. This is critical - deployment cannot proceed without files being synced." \
              "1. Check network connectivity: ping ${{ secrets.HOST }}
2. Verify SSH connection: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'echo Connection OK'
3. Check server disk space: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'df -h'
4. Verify permissions on server: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'ls -la ~/aviationwx'
5. Check rsync logs for specific errors
6. Retry deployment after resolving issues"
          fi
          echo "✓ Repository synced successfully"

      - name: Verify and restore SSL certificates after rsync
        run: |
          # Verify SSL certificates are still present after rsync
          # (rsync --delete could remove them if ssl/ wasn't excluded)
          echo ""
          echo "Verifying SSL certificates are still present after rsync..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          # Check if certificates exist
          if [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates verified after rsync"
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  WARNING: SSL certificates missing after rsync"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Rsync may have deleted the ssl/ directory. Restoring certificates..."
            
            # Use helper script to restore certificates
            if [ -f scripts/restore-ssl-certificates.sh ]; then
              chmod +x scripts/restore-ssl-certificates.sh
              ./scripts/restore-ssl-certificates.sh
            else
              # Source deployment helper if available
              if [ -f scripts/deploy-helpers.sh ]; then
                source scripts/deploy-helpers.sh
                deployment_error \
                  "SSL Certificate Restoration" \
                  "restore-ssl-certificates.sh script not found. This should not happen - script should be synced via rsync." \
                  "1. Verify script exists: ls -la scripts/restore-ssl-certificates.sh
2. Check if rsync excluded the scripts directory
3. Verify file permissions
4. Manually restore certificates if needed: sudo cp /etc/letsencrypt/live/aviationwx.org/*.pem ssl/"
              else
                echo "❌ ERROR: restore-ssl-certificates.sh script not found"
                echo "This should not happen - script should be synced via rsync"
                exit 1
              fi
            fi
          fi
          EOF

      - name: Check for Docker config changes
        id: docker-config-changed
        run: |
          echo "Checking for Docker configuration file changes..."
          
          # Get the previous commit SHA (if available)
          PREV_SHA="${{ github.event.before }}"
          CURRENT_SHA="${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}"
          
          # Docker config files that require image rebuild
          DOCKER_CONFIG_FILES=(
            "docker/Dockerfile"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-entrypoint.sh"
            "docker/pam-vsftpd"
            "docker/logrotate-vsftpd"
            "docker/logrotate-sshd"
            "docker/fail2ban-jail.conf"
            "docker/fail2ban-vsftpd.conf"
            "docker/fail2ban-sshd.conf"
            "docker/docker-compose.prod.yml"
            "scripts/service-watchdog.sh"
            "scripts/setup-letsencrypt.sh"
            "scripts/enable-vsftpd-ssl.sh"
            "scripts/create-sftp-user.sh"
            "config/crontab"
          )
          
          CONFIG_CHANGED="false"
          
          # If we have a previous commit, check what changed
          if [ -n "$PREV_SHA" ] && [ "$PREV_SHA" != "0000000000000000000000000000000000000000" ]; then
            echo "Comparing changes from $PREV_SHA to $CURRENT_SHA..."
            for file in "${DOCKER_CONFIG_FILES[@]}"; do
              if git diff --name-only "$PREV_SHA" "$CURRENT_SHA" | grep -q "^${file}$"; then
                echo "  ✓ $file changed"
                CONFIG_CHANGED="true"
              fi
            done
          else
            # No previous commit (initial deploy or can't determine), assume config might have changed
            echo "⚠️  Cannot determine previous commit, will check for config changes on server"
            CONFIG_CHANGED="unknown"
          fi
          
          if [ "$CONFIG_CHANGED" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  Docker configuration files changed - will force rebuild"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          elif [ "$CONFIG_CHANGED" = "unknown" ]; then
            echo "⚠️  Will verify config changes on server and rebuild if needed"
          else
            echo "✓ No Docker configuration file changes detected"
          fi
          
          echo "config_changed=$CONFIG_CHANGED" >> $GITHUB_OUTPUT

      - name: Deploy via Docker Compose
        run: |
          # Get commit SHA from GitHub Actions context (7 characters to match GitHub's short SHA display)
          # Note: .git directory is excluded during rsync, so we can't get SHA from server's git repo
          GIT_SHA="${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}"
          GIT_SHA="${GIT_SHA:0:7}"
          echo "Deploying with GIT_SHA: ${GIT_SHA}"
          
          # Determine if we need to force rebuild
          FORCE_REBUILD="${{ steps.docker-config-changed.outputs.config_changed }}"
          
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          # Set GIT_SHA as environment variable for docker compose
          export GIT_SHA="$GIT_SHA"
          # Enable BuildKit for better caching (if not already enabled)
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Check for Docker config changes on server (fallback if git diff didn't work)
          if [ "$FORCE_REBUILD" = "unknown" ] || [ "$FORCE_REBUILD" = "false" ]; then
            echo "Verifying Docker config files on server..."
            CONFIG_CHANGED="false"
            
            # Docker config files that require image rebuild
            DOCKER_CONFIG_FILES=(
              "docker/Dockerfile"
              "docker/vsftpd.conf"
              "docker/sshd_config"
              "docker/docker-entrypoint.sh"
              "docker/pam-vsftpd"
              "docker/logrotate-vsftpd"
              "docker/logrotate-sshd"
              "docker/fail2ban-jail.conf"
              "docker/fail2ban-vsftpd.conf"
              "docker/fail2ban-sshd.conf"
              "docker/docker-compose.prod.yml"
              "scripts/service-watchdog.sh"
              "scripts/setup-letsencrypt.sh"
              "scripts/enable-vsftpd-ssl.sh"
              "scripts/create-sftp-user.sh"
              "config/crontab"
            )
            
            # Check if any config file was modified more recently than the Docker image
            if docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1 | grep -q .; then
              IMAGE_TIME=\$(docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1)
              IMAGE_TIMESTAMP=\$(date -d "\$IMAGE_TIME" +%s 2>/dev/null || echo 0)
              
              for file in "\${DOCKER_CONFIG_FILES[@]}"; do
                if [ -f "\$file" ]; then
                  FILE_TIMESTAMP=\$(stat -c %Y "\$file" 2>/dev/null || stat -f %m "\$file" 2>/dev/null || echo 0)
                  if [ "\$FILE_TIMESTAMP" -gt "\$IMAGE_TIMESTAMP" ]; then
                    echo "  ✓ \$file is newer than Docker image"
                    CONFIG_CHANGED="true"
                  fi
                fi
              done
            else
              # No image exists, must rebuild
              CONFIG_CHANGED="true"
            fi
            
            if [ "\$CONFIG_CHANGED" = "true" ]; then
              FORCE_REBUILD="true"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "⚠️  Docker configuration files changed - will force rebuild"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            fi
          fi
          
          # Build arguments for cache invalidation
          if [ "$FORCE_REBUILD" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Forcing rebuild without cache due to config changes..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            # Force rebuild without cache
            if ! docker compose -f docker/docker-compose.prod.yml build --no-cache web; then
              deployment_error \
                "Docker Build (No Cache)" \
                "Docker build failed when forcing rebuild without cache" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config
2. Check disk space: df -h
3. Check Docker daemon: systemctl status docker
4. Review build logs above for specific errors
5. Try manual build: docker compose -f docker/docker-compose.prod.yml build --no-cache web"
            fi
            # Start containers with the newly built image
            if ! docker compose -f docker/docker-compose.prod.yml up -d --pull; then
              echo "⚠️  Warning: Start with --pull failed, trying without --pull..."
              if ! docker compose -f docker/docker-compose.prod.yml up -d; then
                deployment_error \
                  "Container Startup" \
                  "Failed to start containers after build" \
                  "1. Check container logs: docker compose -f docker/docker-compose.prod.yml logs
2. Verify container status: docker compose -f docker/docker-compose.prod.yml ps
3. Check port conflicts: netstat -tuln | grep -E ':(80|443|2121|2222)'
4. Review docker-compose.prod.yml configuration
5. Check Docker daemon logs: journalctl -u docker"
              fi
            fi
            echo "✓ Containers rebuilt and started successfully (no cache)"
          elif ! docker compose -f docker/docker-compose.prod.yml up -d --build --pull; then
            echo "⚠️  Warning: Build with --pull failed, trying without --pull..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d --build; then
              deployment_error \
                "Docker Build/Start" \
                "Both build attempts (with --pull and without) failed" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config
2. Verify all required files are present (check Dockerfile COPY commands)
3. Check network connectivity for pulling base images: ping registry-1.docker.io
4. Check disk space: df -h
5. Check Docker daemon: systemctl status docker
6. Review Docker logs: journalctl -u docker
7. Try manual build: docker compose -f docker/docker-compose.prod.yml build web"
            fi
          fi
          
          echo "✓ Containers built and started successfully"
          
          # Wait for web container to be ready (needed for certificate generation)
          echo "Waiting for web container to be ready..."
          sleep 10
          
          # Verify container is running before attempting certificate operations
          if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
            echo "⚠️  Warning: Web container is not running, skipping certificate generation"
          else
            # Generate FTPS SSL certificates (one-time setup)
            # This runs after containers are started so the web server is available for HTTP-01 challenge
            echo "Checking for upload.aviationwx.org SSL certificates..."
            
            # Check if certificates already exist in container
            # This is a one-time setup that only runs if certificates don't exist
            if docker compose -f docker/docker-compose.prod.yml exec -T web test -f /etc/letsencrypt/live/upload.aviationwx.org/fullchain.pem 2>/dev/null && \
               docker compose -f docker/docker-compose.prod.yml exec -T web test -f /etc/letsencrypt/live/upload.aviationwx.org/privkey.pem 2>/dev/null; then
            echo "✓ SSL certificates for upload.aviationwx.org already exist"
            echo "  Skipping certificate generation (one-time setup already complete)"
          else
            echo "SSL certificates for upload.aviationwx.org not found"
            echo "Attempting one-time certificate generation..."
            echo ""
            echo "Requirements:"
            echo "  - Domain upload.aviationwx.org must resolve to this server"
            echo "  - Port 80 must be accessible (for Let's Encrypt validation)"
            echo "  - Web server must be running"
            echo ""
            
            # Try to generate certificates inside the container
            # Use LETSENCRYPT_EMAIL environment variable
            if docker compose -f docker/docker-compose.prod.yml exec -T -e LETSENCRYPT_EMAIL=alex@alexwitherspoon.com web /usr/local/bin/setup-letsencrypt.sh 2>&1; then
              echo "✓ SSL certificates generated successfully for upload.aviationwx.org"
              echo "  FTPS will be automatically enabled on next container start"
              # Restart container to pick up SSL configuration
              docker compose -f docker/docker-compose.prod.yml restart web || true
            else
              echo "⚠️  Warning: Failed to generate SSL certificates for upload.aviationwx.org"
              echo "  This may be due to:"
              echo "    - Domain not resolving to this server"
              echo "    - Port 80 not accessible"
              echo "    - Let's Encrypt rate limits"
              echo ""
              echo "  Deployment will continue, but FTPS will not be available until certificates are generated."
              echo "  You can manually generate certificates by running:"
              echo "    docker compose -f docker/docker-compose.prod.yml exec web /usr/local/bin/setup-letsencrypt.sh"
              echo ""
              # Don't fail deployment - just warn
            fi
          fi
          fi
          
          # Sync FTP/SFTP/FTPS configuration (runs on every deployment)
          # This ensures users and directories are created/updated immediately after deployment
          echo "Syncing FTP/SFTP/FTPS configuration..."
          if docker compose -f docker/docker-compose.prod.yml exec -T web /usr/local/bin/php /var/www/html/scripts/sync-push-config.php 2>&1; then
            echo "✓ FTP/SFTP/FTPS configuration synced successfully"
          else
            echo "⚠️  Warning: FTP/SFTP/FTPS configuration sync failed"
            echo "  This may be due to:"
            echo "    - Config file not yet available"
            echo "    - Permissions issues"
            echo "    - Database corruption (will be auto-recovered on next run)"
            echo ""
            echo "  Deployment will continue, but FTP/SFTP users may not be configured."
            echo "  The sync will retry on container startup."
            # Don't fail deployment - just warn
          fi
          
          # Clean up Docker resources conditionally (only if disk usage is high)
          # Check disk usage before cleanup to avoid unnecessary operations
          DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
          DISK_THRESHOLD=80  # Cleanup if disk usage > 80%
          
          echo "Checking disk usage before cleanup..."
          echo "Current disk usage: ${DISK_USAGE}% (threshold: ${DISK_THRESHOLD}%)"
          
          if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
            echo "Disk usage is above threshold - running Docker cleanup..."
            # Clean up Docker resources using the dedicated cleanup script
            # This modularizes cleanup logic and keeps the workflow concise
            if [ -f scripts/deploy-docker-cleanup.sh ]; then
              echo "Running Docker cleanup script..."
              chmod +x scripts/deploy-docker-cleanup.sh
              ./scripts/deploy-docker-cleanup.sh
            else
              echo "⚠️  Cleanup script not found, running basic cleanup..."
              docker builder prune -f --filter "until=24h" || true
              docker image prune -f || true
              docker system prune -f --filter "until=168h" || true
              docker system df
            fi
          else
            echo "Disk usage is below threshold - skipping cleanup to save time"
            echo "Current Docker disk usage:"
            docker system df 2>/dev/null || echo "Could not retrieve Docker disk usage"
          fi
          EOF

      - name: Restart Nginx container to pick up config changes
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Restarting Nginx container..."
          # Restart Nginx container to ensure it picks up new nginx.conf
          # This is safer than reload since config file is mounted as volume
          if ! docker compose -f docker/docker-compose.prod.yml restart nginx; then
            echo "Restart failed, trying up -d..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d nginx; then
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: Failed to start Nginx container"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "Container logs:"
              docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
              exit 1
            fi
          fi
          
          # Wait for Nginx to be ready
          sleep 3
          
          echo "Testing Nginx configuration..."
          # Verify Nginx configuration is valid
          CONFIG_TEST_OUTPUT=$(docker compose -f docker/docker-compose.prod.yml exec -T nginx nginx -t 2>&1 || echo "FAILED")
          
          if echo "$CONFIG_TEST_OUTPUT" | grep -qE "syntax is ok|test is successful"; then
            echo "✓ Nginx configuration is valid"
          else
            deployment_error \
              "Nginx Configuration Validation" \
              "Nginx configuration test failed - configuration is invalid" \
              "1. Review Nginx config test output above
2. Check nginx.conf syntax: docker compose -f docker/docker-compose.prod.yml exec nginx nginx -t
3. Verify all referenced files exist
4. Check for missing semicolons, brackets, or typos
5. Review docker/nginx.conf and docker/nginx-main.conf
6. Fix configuration errors and retry deployment"
            echo "$CONFIG_TEST_OUTPUT"
            echo ""
            echo "Container logs:"
            docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
            echo ""
            echo "Fix the Nginx configuration and retry deployment."
            exit 1
          fi
          
          # Verify Nginx is responding (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Nginx not responding on port 80 (may be expected if SSL only)"
          else
            echo "✓ Nginx is responding"
          fi
          
          # Verify SSL certificates are accessible inside nginx container
          echo ""
          echo "Verifying SSL certificates are accessible in nginx container..."
          if docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/fullchain.pem && \
             docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/privkey.pem; then
            echo "✓ SSL certificates are accessible in nginx container"
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ ERROR: SSL certificates not accessible in nginx container"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "This indicates a volume mount problem."
            echo "Checking host-side certificates..."
            ls -la ssl/ || echo "ssl/ directory not found on host"
            echo ""
            echo "Container volume mounts:"
            docker inspect aviationwx-nginx | grep -A 20 Mounts || echo "Could not inspect container"
            echo ""
            echo "Fix: Ensure ssl/ directory exists on host and contains certificates."
            exit 1
          fi
          EOF

      - name: Purge browser and service worker caches on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Force service worker update by updating public/js/service-worker.js timestamp
          # This ensures clients will download new version
          # Done after Nginx restart to ensure fresh content is served
          touch public/js/service-worker.js || true
          echo "✓ Updated service worker file timestamp to force cache bust"
          
          # If using Nginx proxy_cache, we would purge here
          # Currently not using proxy_cache, but keeping this for future reference
          # docker exec nginx nginx -s reload || true
          EOF

      - name: Post-deployment verification (quick check)
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Verification (Quick Check)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Verify deployed commit SHA
          EXPECTED_SHA="${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}"
          EXPECTED_SHA_SHORT="${EXPECTED_SHA:0:7}"
          
          echo "Verifying deployed commit SHA..."
          echo "Expected: ${EXPECTED_SHA_SHORT}"
          
          # Get actual SHA from container environment variable
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -z "$ACTUAL_SHA" ]; then
            echo "⚠️  Warning: GIT_SHA environment variable not set in container"
            echo "This may indicate the deployment didn't set the variable correctly"
          else
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            echo "Actual: ${ACTUAL_SHA_SHORT}"
            
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: Deployed commit SHA does not match expected SHA"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "Expected: ${EXPECTED_SHA_SHORT}"
              echo "Actual: ${ACTUAL_SHA_SHORT}"
              echo ""
              echo "This indicates a partial deployment or deployment of wrong code."
              exit 1
            fi
            
            echo "✓ Deployed commit SHA matches expected SHA"
          fi
          
          # Quick service check
          echo ""
          echo "Quick service response check..."
          if ! curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Service not responding on https://aviationwx.org/"
            echo "This may be expected if services are still starting"
            echo "Full health check will verify this in the next step"
          else
            echo "✓ Service is responding"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✓ Post-deployment verification complete"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF

      - name: Verify fail2ban in container
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Verifying fail2ban in container..."
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Wait for container to be fully ready
          echo "Waiting for container services to initialize..."
          sleep 15  # Increased from 5s to allow fail2ban time to start
          
          # Retry logic: check up to 3 times with 5s intervals
          MAX_RETRIES=3
          RETRY_INTERVAL=5
          FAIL2BAN_RUNNING=false
          
          for i in $(seq 1 $MAX_RETRIES); do
            echo ""
            echo "Attempt $i/$MAX_RETRIES: Checking fail2ban..."
            
            # Check if process is running
            if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x fail2ban-server > /dev/null 2>&1; then
              echo "✓ fail2ban-server process is running"
              
              # Verify we can connect to fail2ban client
              if docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client ping > /dev/null 2>&1; then
                echo "✓ fail2ban-client is responsive"
                
                # Get status and verify jails are active
                STATUS_OUTPUT=$(docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client status 2>&1 || echo "")
                
                if echo "$STATUS_OUTPUT" | grep -q "Jail list"; then
                  JAIL_LIST=$(echo "$STATUS_OUTPUT" | grep -A 10 "Jail list" | tail -n +2 | tr -d ' \t' || echo "")
                  
                  if [ -n "$JAIL_LIST" ]; then
                    echo "✓ Active jails found:"
                    echo "$JAIL_LIST" | sed 's/^/  - /'
                    
                    # Verify at least one jail is enabled (vsftpd or sshd-sftp)
                    if echo "$JAIL_LIST" | grep -qE "(vsftpd|sshd-sftp)"; then
                      echo "✓ Required jails (vsftpd/sshd-sftp) are active"
                      FAIL2BAN_RUNNING=true
                      break
                    else
                      echo "⚠️  Warning: No required jails found in jail list"
                      echo "Jail list: $JAIL_LIST"
                    fi
                  else
                    echo "⚠️  Warning: Jail list is empty"
                  fi
                else
                  echo "⚠️  Warning: Could not get jail list from status"
                  echo "Status output: $STATUS_OUTPUT"
                fi
              else
                echo "⚠️  Warning: fail2ban-client is not responsive"
              fi
            else
              echo "⚠️  Warning: fail2ban-server process not found"
            fi
            
            if [ $i -lt $MAX_RETRIES ]; then
              echo "Waiting ${RETRY_INTERVAL}s before retry..."
              sleep $RETRY_INTERVAL
            fi
          done
          
          # Fail deployment if fail2ban is not running properly
          if [ "$FAIL2BAN_RUNNING" != "true" ]; then
            # Collect debugging information before calling deployment_error
            FAIL2BAN_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs web | grep -i fail2ban | tail -20 || echo "No fail2ban logs found")
            FAIL2BAN_PROCESSES=$(docker compose -f docker/docker-compose.prod.yml exec -T web ps aux | grep -E "(fail2ban|vsftpd|sshd)" || echo "No related processes found")
            FAIL2BAN_CONFIG=$(docker compose -f docker/docker-compose.prod.yml exec -T web ls -la /etc/fail2ban/jail.d/ 2>/dev/null || echo "Jail directory not found")
            FAIL2BAN_STATUS=$(docker compose -f docker/docker-compose.prod.yml exec -T web systemctl status fail2ban 2>/dev/null || echo "systemctl not available")
            
            deployment_error \
              "fail2ban Verification" \
              "fail2ban verification failed after $MAX_RETRIES attempts. This is a CRITICAL security feature - deployment cannot proceed without fail2ban." \
              "1. Check container logs (fail2ban related):
   ${FAIL2BAN_LOGS}
2. Check container processes:
   ${FAIL2BAN_PROCESSES}
3. Verify fail2ban configuration:
   ${FAIL2BAN_CONFIG}
4. Check fail2ban service status:
   ${FAIL2BAN_STATUS}
5. Review Dockerfile for fail2ban installation
6. Check entrypoint script for fail2ban startup
7. Verify fail2ban configuration files are copied correctly"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✓ fail2ban verification complete - all checks passed"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""
          echo "Note: fail2ban runs inside the container with NET_ADMIN capability"
          echo "to manage iptables rules for brute-force protection."
          EOF
      
      - name: Post-deployment health check
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Health Checks"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Checking for partial deployment failures..."
          echo ""
          
          HEALTH_CHECK_FAILED=false
          FAILURE_REASONS=()
          PARTIAL_DEPLOYMENT=false
          
          # Check 1: Verify deployment completed (not partial)
          echo "1. Checking deployment completeness..."
          
          # Check if files were synced
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Files not synced - partial deployment detected")
          fi
          
          # Check if containers exist
          if ! docker compose -f docker/docker-compose.prod.yml ps web > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Containers not found - partial deployment detected")
          fi
          
          # Wait for containers to be ready (give them time to start)
          echo ""
          echo "2. Waiting for containers to start..."
          sleep 8
          
          # Check 3: Containers running
          echo "3. Checking container status..."
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps --format json 2>/dev/null || docker compose -f docker/docker-compose.prod.yml ps)
          
          # Count running containers (both web and nginx should be up)
          RUNNING_COUNT=$(docker compose -f docker/docker-compose.prod.yml ps --status running --format json 2>/dev/null | grep -c '"State":"running"' || docker compose -f docker/docker-compose.prod.yml ps | grep -c "Up" || echo "0")
          
          # Check that we have at least the web container (nginx is optional but should be running too)
          if [ "$RUNNING_COUNT" -lt "1" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Web container not running")
          else
            echo "✓ Containers are running (${RUNNING_COUNT} container(s) up)"
          fi
          
          # Check 4: Container health status
          echo ""
          echo "4. Checking container health..."
          MAX_WAIT=60
          WAIT_TIME=0
          while [ $WAIT_TIME -lt $MAX_WAIT ]; do
            HEALTH=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | grep -o '"Health":"[^"]*"' | cut -d'"' -f4 || echo "unknown")
            if [ "$HEALTH" = "healthy" ]; then
              echo "✓ Web container is healthy"
              break
            elif [ "$HEALTH" = "unhealthy" ]; then
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Container health status: unhealthy")
              break
            else
              echo "  Container health: $HEALTH (waiting...)"
              sleep 5
              WAIT_TIME=$((WAIT_TIME + 5))
            fi
          done
          
          if [ $WAIT_TIME -ge $MAX_WAIT ]; then
            HEALTH_CHECK_FAILED=true
            FAILURE_REASONS+=("Container did not become healthy within ${MAX_WAIT} seconds")
          fi
          
          # Additional wait for services to fully initialize
          sleep 5
          
          # Check 5: Service responding
          echo ""
          echo "5. Checking service response..."
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s --max-time 5 http://localhost/ > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            FAILURE_REASONS+=("Homepage not accessible (internal check failed)")
          else
            echo "✓ Web container is responding (internal check passed)"
          fi
          
          # Check 6: Verify commit SHA matches (detect partial code deployment)
          echo ""
          echo "6. Verifying deployed code..."
          EXPECTED_SHA="${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}"
          EXPECTED_SHA_SHORT="${EXPECTED_SHA:0:7}"
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -n "$ACTUAL_SHA" ]; then
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Code mismatch - expected ${EXPECTED_SHA_SHORT}, got ${ACTUAL_SHA_SHORT}")
            else
              echo "✓ Deployed code matches expected commit SHA"
            fi
          fi
          
          # Check 7: API responding
          echo ""
          echo "7. Checking API response..."
          APP_PORT=${APP_PORT:-8080}
          if ! curl -f -s --max-time 10 "https://aviationwx.org/api/weather.php?id=kspb" > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            FAILURE_REASONS+=("Weather API not responding")
          else
            echo "✓ Weather API responding"
          fi
          
          # Final check
          if [ "$HEALTH_CHECK_FAILED" = "true" ]; then
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ HEALTH CHECKS FAILED"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            if [ "$PARTIAL_DEPLOYMENT" = "true" ]; then
              echo "⚠️  PARTIAL DEPLOYMENT DETECTED"
              echo "Some deployment steps completed, others failed."
              echo "System may be in inconsistent state."
              echo ""
            fi
            
            echo "Failure reasons:"
            for reason in "${FAILURE_REASONS[@]}"; do
              echo "  ❌ $reason"
            done
            echo ""
            
            echo "Container status:"
            docker compose -f docker/docker-compose.prod.yml ps
            echo ""
            
            echo "Container logs (last 50 lines):"
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            echo ""
            
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Triggering automatic rollback..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            exit 1  # This will trigger the rollback step
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ All health checks passed"
          echo "✅ Deployment completed successfully (no partial deployment detected)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Check if diagnostics page loads (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1:${APP_PORT}/diagnostics.php > /dev/null 2>&1; then
            echo "⚠️  Diagnostics page not accessible (may be expected)"
          else
            echo "✓ Diagnostics page accessible"
          fi
          
          # Check if config-utils.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/config.php; then
            echo "❌ config-utils.php not found in container"
            exit 1
          fi
          
          echo "✓ config-utils.php present"
          
          # Check if rate-limit.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/rate-limit.php; then
            echo "❌ rate-limit.php not found in container"
            exit 1
          fi
          
          echo "✓ rate-limit.php present"
          
          # Check APCu extension
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web php -m | grep -q apcu; then
            echo "⚠️  APCu extension not loaded (rate limiting will be disabled)"
          else
            echo "✓ APCu extension loaded"
          fi
          
          # Check push webcam scripts exist
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/process-push-webcams.php; then
            echo "❌ process-push-webcams.php not found in container"
            exit 1
          fi
          echo "✓ process-push-webcams.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/sync-push-config.php; then
            echo "❌ sync-push-config.php not found in container"
            exit 1
          fi
          echo "✓ sync-push-config.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/push-webcam-validator.php; then
            echo "❌ push-webcam-validator.php not found in container"
            exit 1
          fi
          echo "✓ push-webcam-validator.php present"
          
          # Check FTP/SFTP services (may not be running if no push cameras configured)
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x vsftpd > /dev/null 2>&1; then
            echo "✓ vsftpd service running"
          else
            echo "ℹ️  vsftpd service not running (expected if no push cameras configured)"
          fi
          
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x sshd > /dev/null 2>&1; then
            echo "✓ sshd service running"
          else
            echo "ℹ️  sshd service not running (expected if no push cameras configured)"
          fi
          
          echo "✅ All health checks passed"
          
          # Test webcam endpoints (smoke test)
          echo "Testing webcam endpoints..."
          # Test JPG endpoint for sample airport (assuming kspb exists)
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" > /tmp/webcam_test.jpg 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.jpg 2>/dev/null || stat -c%s /tmp/webcam_test.jpg 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/jpeg"; then
              echo "✓ Webcam JPG endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.jpg
            else
              echo "⚠️  Webcam JPG endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "⚠️  Webcam JPG endpoint not accessible (may be expected if no webcams configured)"
          fi
          
          # Test WEBP endpoint
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" > /tmp/webcam_test.webp 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.webp 2>/dev/null || stat -c%s /tmp/webcam_test.webp 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/webp"; then
              echo "✓ Webcam WEBP endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.webp
            else
              echo "⚠️  Webcam WEBP endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "ℹ️  Webcam WEBP endpoint not accessible (may be expected if WEBP not generated yet)"
          fi
          
          EOF

      - name: Rollback to previous version (if needed)
        if: failure()
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          ROLLBACK_STATE_FILE="/tmp/aviationwx-rollback-state.json"
          
          if [ ! -f "$ROLLBACK_STATE_FILE" ]; then
            echo "⚠️  No rollback state file found - cannot rollback automatically"
            echo "Manual intervention required"
            exit 1
          fi
          
          ROLLBACK_STATE=$(cat "$ROLLBACK_STATE_FILE")
          ROLLBACK_TAG=$(echo "$ROLLBACK_STATE" | jq -r '.rollback_tag // "null"')
          PREVIOUS_SHA=$(echo "$ROLLBACK_STATE" | jq -r '.commit_sha // "unknown"')
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "AUTOMATIC ROLLBACK INITIATED"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Rolling back to previous working state..."
          echo "Previous commit: ${PREVIOUS_SHA}"
          echo "Rollback tag: ${ROLLBACK_TAG}"
          echo ""
          
          # Step 1: Stop current containers (even if partially started)
          echo "Step 1: Stopping current containers (cleaning up partial deployment)..."
          docker compose -f docker/docker-compose.prod.yml down || true
          sleep 2
          
          # Step 2: Check if we have rollback image
          if [ "$ROLLBACK_TAG" != "null" ] && docker images "aviationwx:$ROLLBACK_TAG" --format "{{.Repository}}:{{.Tag}}" 2>/dev/null | grep -q "aviationwx:$ROLLBACK_TAG"; then
            echo "Step 2: Restoring previous Docker image..."
            if docker tag "aviationwx:$ROLLBACK_TAG" "aviationwx:latest" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
              
              # Step 3: Start containers with rollback image
              echo "Step 3: Starting containers with rollback image..."
              export DOCKER_BUILDKIT=1
              export COMPOSE_DOCKER_CLI_BUILD=1
              
              if ! docker compose -f docker/docker-compose.prod.yml up -d; then
                echo "❌ ERROR: Failed to start containers with rollback image"
                echo "Manual intervention required"
                exit 1
              fi
            else
              echo "❌ ERROR: Failed to tag rollback image"
              echo "Attempting to start containers with current code (may be broken)..."
              docker compose -f docker/docker-compose.prod.yml up -d || {
                echo "❌ ERROR: Cannot start containers - manual intervention required"
                exit 1
              }
            fi
          else
            echo "⚠️  Warning: No rollback image available"
            echo "Attempting to start containers with current code (may be broken)..."
            docker compose -f docker/docker-compose.prod.yml up -d || {
              echo "❌ ERROR: Cannot start containers - manual intervention required"
              exit 1
            }
          fi
          
          # Step 4: Wait for services to be ready
          echo "Step 4: Waiting for services to be ready..."
          MAX_WAIT=30
          WAIT_TIME=0
          while [ $WAIT_TIME -lt $MAX_WAIT ]; do
            if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              # Check if container is actually responding
              if docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s http://localhost/ > /dev/null 2>&1; then
                echo "✓ Services are ready after ${WAIT_TIME}s"
                break
              fi
            fi
            sleep 2
            WAIT_TIME=$((WAIT_TIME + 2))
          done
          
          # Step 5: Verify rollback was successful
          if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
            echo "Step 5: Verifying rollback..."
            
            # Verify service is responding
            if curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
              echo "✓ Service is responding after rollback"
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "✅ Rollback completed successfully"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "System restored to previous working state"
            else
              echo "⚠️  Warning: Service not responding after rollback"
              echo "Container is running but service may not be fully functional"
              echo "Check logs: docker compose -f docker/docker-compose.prod.yml logs web"
            fi
          else
            echo "❌ ERROR: Containers not running after rollback"
            echo "Manual intervention required"
            echo ""
            echo "Debugging info:"
            docker compose -f docker/docker-compose.prod.yml ps
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            exit 1
          fi
          EOF

