name: Deploy to Production

on:
  workflow_run:
    workflows: ["Quality Assurance Tests"]
    types:
      - completed
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      skip_prerequisites:
        description: 'Skip prerequisite checks (DANGEROUS - only use in emergencies)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      deployment_reason:
        description: 'Reason for manual deployment'
        required: true
        type: string
      deploy_sha:
        description: 'Commit SHA to deploy (leave empty to deploy latest)'
        required: false
        type: string

# Prevent concurrent deployments - only one deployment at a time per SHA
# This prevents conflicts when multiple commits are pushed rapidly
# Using SHA in concurrency group ensures each commit gets its own deployment slot
# For workflow_run events, use the triggering workflow's SHA; for others, use current SHA
concurrency:
  group: deploy-production-${{ github.event.workflow_run.head_sha || github.sha }}
  cancel-in-progress: false  # Don't cancel - queue deployments to prevent conflicts

permissions:
  contents: read
  actions: read
  checks: read

jobs:
  check-prerequisites:
    runs-on: ubuntu-latest
    timeout-minutes: 35
    outputs:
      qa-success: ${{ steps.check.outputs.qa-success }}
      skip-checks: ${{ steps.check.outputs.skip-checks }}
      deploy-sha: ${{ steps.determine-sha-prereq.outputs.deploy_sha }}
      deploy-sha-short: ${{ steps.determine-sha-prereq.outputs.deploy_sha_short }}
    steps:
      - name: Determine deployment SHA
        id: determine-sha-prereq
        run: |
          # For workflow_run events, use the triggering workflow's SHA
          # For manual dispatch, use provided SHA or default to latest
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            DEPLOY_SHA="${{ github.event.workflow_run.head_sha }}"
            if [ -z "$DEPLOY_SHA" ] || [ "$DEPLOY_SHA" = "" ]; then
              echo "❌ ERROR: workflow_run.head_sha is empty or missing"
              echo "This indicates the workflow_run event is malformed"
              exit 1
            fi
            echo "Using SHA from workflow_run event: ${DEPLOY_SHA:0:7}"
          else
            # Manual dispatch: use provided SHA or default to current SHA
            PROVIDED_SHA="${{ github.event.inputs.deploy_sha }}"
            if [ -n "$PROVIDED_SHA" ] && [ "$PROVIDED_SHA" != "" ]; then
              DEPLOY_SHA="$PROVIDED_SHA"
              echo "Using provided SHA: ${DEPLOY_SHA:0:7}"
            else
              DEPLOY_SHA="${{ github.sha }}"
              echo "Using latest SHA (default): ${DEPLOY_SHA:0:7}"
            fi
          fi
          
          # Validate SHA format (should be 40 characters for full SHA, or 7+ for short)
          if [ ${#DEPLOY_SHA} -lt 7 ]; then
            echo "❌ ERROR: Invalid SHA format: ${DEPLOY_SHA}"
            echo "SHA must be at least 7 characters"
            exit 1
          fi
          
          echo "deploy_sha=${DEPLOY_SHA}" >> $GITHUB_OUTPUT
          echo "deploy_sha_short=${DEPLOY_SHA:0:7}" >> $GITHUB_OUTPUT
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deployment SHA (for prerequisites check): ${DEPLOY_SHA}"
          echo "Short SHA: ${DEPLOY_SHA:0:7}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Wait for and check required workflows status
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            const skipInput = '${{ github.event.inputs.skip_prerequisites }}';
            const isManualDispatch = '${{ github.event_name }}' === 'workflow_dispatch';
            const deploymentReason = '${{ github.event.inputs.deployment_reason }}';
            const deploySha = '${{ steps.determine-sha-prereq.outputs.deploy_sha }}';
            const deployShaShort = '${{ steps.determine-sha-prereq.outputs.deploy_sha_short }}';
            
            // Fast-fail: Check QA conclusion for workflow_run events first
            if (!isManualDispatch) {
              const triggeringConclusion = '${{ github.event.workflow_run.conclusion }}';
              
              if (triggeringConclusion !== 'success') {
                const conclusion = triggeringConclusion || 'unknown';
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.error(`❌ DEPLOYMENT BLOCKED: QA Tests Did Not Pass`);
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.error(`QA Workflow Conclusion: ${conclusion}`);
                core.error(`Commit SHA: ${deployShaShort}`);
                core.error(``);
                core.error(`Deployment cannot proceed because Quality Assurance tests failed.`);
                core.error(`This workflow will exit immediately - deploy job will not run.`);
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.setFailed('❌ QA Tests did not pass - deployment blocked. Fix QA failures and retry.');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ QA workflow succeeded - proceeding with additional checks`);
              
              const commitSha = deploySha;
              const branchName = '${{ github.event.workflow_run && github.event.workflow_run.head_branch || github.ref_name }}';
              const workflowRunId = '${{ github.event.workflow_run.id }}';
              
              // Verify the workflow_run SHA matches what we determined
              const workflowRunSha = '${{ github.event.workflow_run.head_sha }}';
              if (workflowRunSha !== commitSha) {
                core.error(`❌ CRITICAL: SHA mismatch detected!`);
                core.error(`   workflow_run.head_sha: ${workflowRunSha.substring(0, 7)}`);
                core.error(`   Determined SHA: ${commitSha.substring(0, 7)}`);
                core.error(`   This indicates a potential race condition or workflow_run event issue`);
                core.setFailed('❌ SHA mismatch - cannot proceed with deployment');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ Verified workflow_run SHA matches determined SHA: ${commitSha.substring(0, 7)}`);
              
              if (branchName !== 'main') {
                core.warning(`⚠️  Skipping deployment - not on main branch (branch: ${branchName})`);
                core.warning('Only deployments from main branch are allowed');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              // Verify the commit actually exists on main branch
              try {
                const commit = await github.rest.repos.getCommit({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  ref: commitSha
                });
                
                if (commit.data.commit.message) {
                  core.info(`✓ Commit verified: ${commitSha.substring(0, 7)} - ${commit.data.commit.message.split('\n')[0].substring(0, 60)}`);
                }
              } catch (error) {
                core.error(`❌ ERROR: Commit ${commitSha.substring(0, 7)} does not exist in repository`);
                core.error(`   This may indicate the SHA is invalid or the commit was force-pushed`);
                core.setFailed('❌ Commit verification failed - cannot proceed with deployment');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              const deployWorkflowPath = '.github/workflows/deploy-docker.yml';
              const currentRunId = context.runId;
              
              // Check for active deployments of the SAME commit (concurrency handles different commits)
              const allDeployRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: deployWorkflowPath,
                per_page: 20
              });
              
              const otherRuns = allDeployRuns.data.workflow_runs.filter(run => run.id !== currentRunId);
              
              // Check for active deployments of the same commit
              const activeDeploysSameCommit = otherRuns.filter(run => 
                (run.status === 'in_progress' || run.status === 'queued') &&
                run.head_sha === commitSha
              );
              
              if (activeDeploysSameCommit.length > 0) {
                core.info(`⏭️  Skipping deployment - another deployment for commit ${commitSha.substring(0, 7)} is already active`);
                core.info(`   This prevents duplicate deployments of the same commit`);
                core.setOutput('qa-success', 'false');
                return;
              }
              
              // Check for recent successful deployment of the same commit (within 5 minutes)
              const now = Date.now();
              const recentSuccessfulDeploys = otherRuns.filter(run => {
                if (run.status !== 'completed' || run.conclusion !== 'success') return false;
                if (run.head_sha !== commitSha) return false;  // Only check same commit
                const completedAt = new Date(run.updated_at || run.created_at).getTime();
                const ageMinutes = (now - completedAt) / (1000 * 60);
                return ageMinutes < 5;
              });
              
              if (recentSuccessfulDeploys.length > 0) {
                core.info(`⏭️  Skipping deployment - commit ${commitSha.substring(0, 7)} was recently deployed`);
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ All checks passed - ready to deploy commit ${commitSha.substring(0, 7)}`);
              core.setOutput('qa-success', 'true');
              core.setOutput('skip-checks', 'false');
              return;
            }
            
            if (isManualDispatch) {
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              core.info(`Manual deployment triggered`);
              core.info(`Reason: ${deploymentReason || 'Not provided'}`);
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              
              if (skipInput === 'true') {
                core.warning('');
                core.warning('⚠️  WARNING: Prerequisite checks are being SKIPPED');
                core.warning('This is DANGEROUS and should only be used in emergencies');
                core.warning('Deployment will proceed without verifying tests passed');
                core.warning('');
                core.setOutput('skip-checks', 'true');
                core.setOutput('qa-success', 'true');
                return;
              } else {
                core.setOutput('skip-checks', 'false');
              }
            } else {
              core.setOutput('skip-checks', 'false');
            }
            
            // Manual dispatch: verify QA workflow passed
            if (isManualDispatch && skipInput !== 'true') {
              // Use the determined deployment SHA
              const commitSha = deploySha;
              const branchName = '${{ github.ref_name }}';
              
              core.info(`Checking QA tests for commit: ${deployShaShort}`);
              const providedSha = '${{ github.event.inputs.deploy_sha }}';
              if (providedSha && providedSha.trim() !== '') {
                core.info(`Using provided SHA: ${deployShaShort}`);
              } else {
                core.info(`Using default SHA (latest): ${deployShaShort}`);
              }
              
              const triggeringWorkflowRunIdRaw = '${{ github.event.workflow_run && github.event.workflow_run.id }}';
              const triggeringWorkflowRunId = (triggeringWorkflowRunIdRaw && triggeringWorkflowRunIdRaw !== '') ? triggeringWorkflowRunIdRaw : null;
              
              const qaWorkflowPath = '.github/workflows/quality-assurance-tests.yml';
              const maxWaitTime = 30 * 60 * 1000;
              const checkInterval = 30 * 1000;
              const startTime = Date.now();
              
              let qaRun = null;
              let qaSuccess = false;
              
              while (Date.now() - startTime < maxWaitTime) {
                const qaRuns = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: qaWorkflowPath,
                  head_sha: commitSha,
                  branch: branchName,
                  per_page: 10
                });
                
                if (triggeringWorkflowRunId) {
                  qaRun = qaRuns.data.workflow_runs.find(run => 
                    run.id.toString() === triggeringWorkflowRunId.toString()
                  );
                }
                
                if (!qaRun) {
                  qaRun = qaRuns.data.workflow_runs.find(run => 
                    run.head_sha === commitSha && 
                    run.head_branch === branchName &&
                    (run.event === 'push' || run.event === 'workflow_dispatch')
                  );
                }
                
                if (qaRun && qaRun.status === 'completed') {
                  qaSuccess = qaRun.conclusion === 'success';
                  break;
                }
                
                await new Promise(resolve => setTimeout(resolve, checkInterval));
              }
              
              if (!qaRun || !qaSuccess) {
                core.error(`❌ Quality Assurance Tests workflow did not succeed for commit ${deployShaShort}`);
                const providedSha = '${{ github.event.inputs.deploy_sha }}';
                if (providedSha && providedSha.trim() !== '') {
                  core.error(`Provided SHA: ${deployShaShort}`);
                  core.error(`Make sure QA tests have passed for this specific commit before deploying.`);
                }
                core.setFailed('❌ QA tests must pass before manual deployment (unless skip flag is used)');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✅ Quality Assurance Tests workflow succeeded for commit ${deployShaShort}`);
              core.setOutput('qa-success', 'true');
            }

  deploy:
    runs-on: ubuntu-latest
    needs: check-prerequisites
    timeout-minutes: 60
    # Only deploy if QA succeeded (or manual dispatch with skip)
    if: |
      (needs.check-prerequisites.outputs.qa-success == 'true') || 
      (github.event_name == 'workflow_dispatch' && needs.check-prerequisites.outputs.skip-checks == 'true')
    steps:
      - name: Use deployment SHA from prerequisites check
        id: determine-sha
        run: |
          # Use the SHA determined in the check-prerequisites job
          # This ensures both jobs use the exact same SHA
          DEPLOY_SHA="${{ needs.check-prerequisites.outputs.deploy-sha }}"
          DEPLOY_SHA_SHORT="${{ needs.check-prerequisites.outputs.deploy-sha-short }}"
          
          if [ -z "$DEPLOY_SHA" ]; then
            echo "❌ ERROR: Deployment SHA not found from prerequisites check"
            echo "This should not happen - check-prerequisites job should have determined the SHA"
            exit 1
          fi
          
          echo "deploy_sha=${DEPLOY_SHA}" >> $GITHUB_OUTPUT
          echo "deploy_sha_short=${DEPLOY_SHA_SHORT}" >> $GITHUB_OUTPUT
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deployment SHA (from prerequisites): ${DEPLOY_SHA}"
          echo "Short SHA: ${DEPLOY_SHA_SHORT}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ steps.determine-sha.outputs.deploy_sha }}
          fetch-depth: 0

      - name: Log deployment SHA for tracking
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "DEPLOYMENT SHA TRACKING"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Event: ${{ github.event_name }}"
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            echo "QA Workflow Run ID: ${{ github.event.workflow_run.id }}"
            echo "QA Workflow SHA: ${{ github.event.workflow_run.head_sha }}"
            echo "QA Workflow Branch: ${{ github.event.workflow_run.head_branch }}"
          fi
          echo "Determined SHA: ${{ steps.determine-sha.outputs.deploy_sha }}"
          echo "Short SHA: ${{ steps.determine-sha.outputs.deploy_sha_short }}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Validate SHA exists
        run: |
          DEPLOY_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          CURRENT_SHA=$(git rev-parse HEAD)
          
          # Verify the checked out SHA matches what we intended (or can be resolved)
          if [ "$CURRENT_SHA" != "$DEPLOY_SHA" ]; then
            # Try to resolve the SHA (might be a short SHA or tag)
            RESOLVED_SHA=$(git rev-parse "$DEPLOY_SHA" 2>/dev/null || echo "")
            if [ -z "$RESOLVED_SHA" ]; then
              echo "❌ ERROR: SHA ${DEPLOY_SHA:0:7} does not exist in the repository"
              echo "Please verify the SHA is correct and exists in the repository"
              exit 1
            fi
            # If resolved SHA is different from current, checkout the correct one
            if [ "$RESOLVED_SHA" != "$CURRENT_SHA" ]; then
              echo "Resolved SHA ${DEPLOY_SHA:0:7} to ${RESOLVED_SHA:0:7}, checking out..."
              git checkout "$RESOLVED_SHA"
              CURRENT_SHA=$(git rev-parse HEAD)
            fi
          fi
          
          echo "✓ Validated SHA: ${CURRENT_SHA:0:7}"
          echo "Full SHA: ${CURRENT_SHA}"

      - name: Get previous commit SHA for rollback
        id: previous-commit
        run: |
          # Get the previous commit SHA (parent of current commit)
          # This will be used for rollback if deployment fails
          CURRENT_SHA=$(git rev-parse HEAD)
          
          # Get the parent commit (previous commit)
          PREVIOUS_SHA=$(git rev-parse HEAD^ 2>/dev/null || echo "")
          
          if [ -z "$PREVIOUS_SHA" ]; then
            # If no parent (first commit), try to get from git log
            PREVIOUS_SHA=$(git log --format=%H -n 2 | tail -1 2>/dev/null || echo "")
          fi
          
          if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "$CURRENT_SHA" ]; then
            echo "previous_commit_sha=${PREVIOUS_SHA}" >> $GITHUB_OUTPUT
            echo "Previous commit SHA for rollback: ${PREVIOUS_SHA:0:7}"
          else
            echo "⚠️  Could not determine previous commit SHA (may be first commit)"
            echo "previous_commit_sha=" >> $GITHUB_OUTPUT
          fi

      - name: Load deployment helpers
        run: |
          # Source deployment helper functions for consistent error handling
          source scripts/deploy-helpers.sh
          echo "✓ Deployment helpers loaded"

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: |
            ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add host to known_hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.HOST }} >> ~/.ssh/known_hosts

      - name: Pre-deployment validation (block bad code)
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Pre-Deployment Validation"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Blocking bad code before it reaches production..."
          echo ""
          
          VALIDATION_FAILED=false
          FAILURE_REASONS=()
          
          # Validation 1: Check for common silent failure patterns
          echo "1. Checking for critical files..."
          CRITICAL_FILES=(
            "api/weather.php"
            "lib/config.php"
            "lib/rate-limit.php"
            "lib/circuit-breaker.php"
            "lib/constants.php"
            "scripts/process-push-webcams.php"
            "scripts/sync-push-config.php"
            "scripts/create-sftp-user.sh"
            "scripts/service-watchdog.sh"
            "lib/push-webcam-validator.php"
            "pages/config-generator.php"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-compose.prod.yml"
            "docker/Dockerfile"
          )
          
          for file in "${CRITICAL_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Critical file missing: $file")
            fi
          done
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ All critical files present"
          fi
          
          # Validation 2: Check for syntax errors that might be missed
          echo ""
          echo "2. Checking PHP syntax..."
          PHP_ERRORS=$(find . -name "*.php" -not -path "./vendor/*" -exec php -l {} \; 2>&1 | grep -v "No syntax errors" || true)
          if [ -n "$PHP_ERRORS" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("PHP syntax errors found")
            echo "❌ PHP syntax errors:"
            echo "$PHP_ERRORS"
          else
            echo "✓ PHP syntax valid"
          fi
          
          # Validation 3: Verify required functions exist
          echo ""
          echo "3. Verifying required functions exist..."
          if ! grep -q "function checkRateLimit" lib/rate-limit.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkRateLimit function missing in lib/rate-limit.php")
          fi
          if ! grep -q "function validateAirportId" lib/config.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("validateAirportId function missing in lib/config.php")
          fi
          if ! grep -q "function checkWeatherCircuitBreaker" lib/circuit-breaker.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkWeatherCircuitBreaker function missing in lib/circuit-breaker.php")
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Required functions present"
          fi
          
          # Validation 4: Check Docker configuration
          echo ""
          echo "4. Validating Docker configuration..."
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("docker-compose.prod.yml missing")
          fi
          if [ ! -f "docker/Dockerfile" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("Dockerfile missing")
          fi
          
          # Check Dockerfile has required components
          if [ -f "docker/Dockerfile" ]; then
            if ! grep -q "FROM php:" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing PHP base image")
            fi
            if ! grep -q "fail2ban" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing fail2ban installation")
            fi
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Docker configuration valid"
          fi
          
          # Validation 5: Check JSON files are valid
          echo ""
          echo "5. Validating JSON configuration files..."
          if [ -f "config/airports.json.example" ]; then
            if ! php -r "json_decode(file_get_contents('config/airports.json.example'), true); if (json_last_error() !== JSON_ERROR_NONE) { exit(1); }" 2>/dev/null; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("config/airports.json.example is invalid JSON")
            else
              echo "✓ config/airports.json.example is valid JSON"
            fi
          fi
          
          # Validation 6: Validate airports.json using master validation script
          echo ""
          echo "6. Validating airports.json (ICAO codes and other checks)..."
          if [ -f "config/airports.json" ]; then
            if php scripts/validate-airports-json.php config/airports.json; then
              echo "✓ All airports.json validations passed"
            else
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("airports.json validation failed - see errors above")
            fi
          else
            echo "⚠ config/airports.json not found - skipping validation"
          fi
          
          # Final check
          if [ "$VALIDATION_FAILED" = "true" ]; then
            echo ""
            # Use deployment helper for consistent error formatting
            REASONS_TEXT=$(printf "  ❌ %s\n" "${FAILURE_REASONS[@]}")
            deployment_error \
              "Pre-Deployment Validation" \
              "The following issues were found:\n${REASONS_TEXT}" \
              "1. Review the validation errors above\n2. Fix syntax errors, missing files, or configuration issues\n3. Ensure all required functions exist in their respective files\n4. Verify Docker configuration files are present and valid\n5. This validation prevents silent failures from reaching production"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Pre-deployment validation passed - code is ready for production"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      
      - name: Update cache version for cache busting
        run: |
          # Generate a unique version based on commit SHA and timestamp
          # Use the determined deployment SHA
          DEPLOY_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          DEPLOY_VERSION="${DEPLOY_SHA:0:7}-$(date +%s)"
          echo "Updating cache version to: ${DEPLOY_VERSION}"
          
          # Update the cache version in public/js/service-worker.js
          # The script handles missing files and verification
          bash scripts/deploy-update-cache-version.sh "${DEPLOY_VERSION}"
          
          # Note: We don't commit service-worker.js changes during deployment
          # The updated file will be synced to the server via rsync
          # This avoids creating unnecessary commits and deployment loops
          echo "✓ Cache version updated (will be synced to server)"

      - name: Clean up old directory structure on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Clean up leftover docker/docker directory from old path issue (now fixed)
          # This is defensive cleanup - the directory should no longer be created
          if [ -d docker/docker ] || [ -f docker/docker/nginx.conf ]; then
            echo "Cleaning up old docker/docker directory structure..."
            rm -rf docker/docker 2>/dev/null || true
            echo "✓ Cleanup complete"
          else
            echo "✓ No cleanup needed (old directory structure not found)"
          fi
          EOF

      - name: Ensure required tools are installed on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          echo "Checking for required tools on production server..."
          
          # Check and install jq if missing (required for JSON parsing in deployment scripts)
          if ! command -v jq >/dev/null 2>&1; then
            echo "⚠️  jq is not installed. Installing..."
            if command -v apt-get >/dev/null 2>&1; then
              if sudo apt-get update -qq && sudo apt-get install -y jq; then
                # Verify installation succeeded
                if command -v jq >/dev/null 2>&1; then
                  echo "✓ jq installed successfully"
                else
                  echo "❌ ERROR: jq installation failed - command not found after install"
                  exit 1
                fi
              else
                echo "❌ ERROR: Failed to install jq via apt-get"
                exit 1
              fi
            elif command -v yum >/dev/null 2>&1; then
              if sudo yum install -y jq; then
                # Verify installation succeeded
                if command -v jq >/dev/null 2>&1; then
                  echo "✓ jq installed successfully"
                else
                  echo "❌ ERROR: jq installation failed - command not found after install"
                  exit 1
                fi
              else
                echo "❌ ERROR: Failed to install jq via yum"
                exit 1
              fi
            else
              echo "❌ ERROR: Cannot install jq - unsupported package manager"
              echo "Please install jq manually: apt-get install jq (Ubuntu/Debian) or yum install jq (CentOS/RHEL)"
              exit 1
            fi
          else
            echo "✓ jq is already installed"
          fi
          EOF

      - name: Tag current working deployment and save state
        run: |
          # Pass previous commit SHA to server
          PREVIOUS_COMMIT_SHA="${{ steps.previous-commit.outputs.previous_commit_sha }}"
          
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} bash -s << 'EOF' "$PREVIOUS_COMMIT_SHA"
          set -euo pipefail
          PREVIOUS_COMMIT_SHA="$1"
          cd ~/aviationwx
          
          # Use persistent location for rollback state (survives reboots)
          ROLLBACK_STATE_FILE="$HOME/aviationwx-rollback-state.json"
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Saving Rollback State"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Capturing current working state before deployment..."
          echo ""
          
          # Get current image tag (if exists)
          CURRENT_IMAGE=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].Image // "null"' || echo "null")
          
          # Get current commit SHA from container (if available)
          CURRENT_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          # Get current container status
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].State // "unknown"' || echo "unknown")
          
          # Generate rollback tag
          ROLLBACK_TAG="rollback-$(date +%Y%m%d-%H%M%S)"
          
          # Save rollback state
          ROLLBACK_STATE=$(cat << STATE_JSON
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "image": "${CURRENT_IMAGE}",
            "commit_sha": "${CURRENT_SHA}",
            "previous_commit_sha": "${PREVIOUS_COMMIT_SHA}",
            "container_status": "${CONTAINER_STATUS}",
            "rollback_tag": "${ROLLBACK_TAG}"
          }
          STATE_JSON
          )
          
          echo "$ROLLBACK_STATE" > "$ROLLBACK_STATE_FILE"
          
          if [ "$CURRENT_IMAGE" != "null" ] && [ -n "$CURRENT_IMAGE" ]; then
            # Tag current image as rollback target
            echo "Tagging current image for rollback: $ROLLBACK_TAG"
            if docker tag "$CURRENT_IMAGE" "aviationwx:$ROLLBACK_TAG" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
              
              # Keep only last 3 rollback tags (cleanup old ones)
              docker images "aviationwx:rollback-*" --format "{{.Tag}}" 2>/dev/null | sort -r | tail -n +4 | while read tag; do
                echo "Removing old rollback tag: $tag"
                docker rmi "aviationwx:$tag" 2>/dev/null || true
              done
            else
              echo "⚠️  Warning: Failed to tag current image (may not exist yet)"
            fi
          else
            echo "⚠️  Warning: No current image found - cannot create rollback image"
            echo "Rollback will restore files only (no image rollback)"
          fi
          
          echo ""
          echo "Rollback state saved:"
          echo "$ROLLBACK_STATE" | jq '.' 2>/dev/null || echo "$ROLLBACK_STATE"
          echo ""
          echo "✓ Rollback state saved to: $ROLLBACK_STATE_FILE"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF
          # Note: PREVIOUS_COMMIT_SHA_FROM_RUNNER is passed via SSH heredoc substitution

      - name: Ensure SSL certificates are in place
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Verify we're running as the correct user
          CURRENT_USER=$(whoami)
          echo "Running as user: $CURRENT_USER"
          
          # Create SSL directory if it doesn't exist
          mkdir -p ssl
          
          # Copy SSL certificates from Let's Encrypt if they exist
          # This ensures certificates are available for Nginx container
          # All operations run as aviationwx user, using sudo for protected operations
          # Use sudo test to check if files exist (directory permissions are root-only)
          if sudo test -f /etc/letsencrypt/live/aviationwx.org/fullchain.pem && sudo test -f /etc/letsencrypt/live/aviationwx.org/privkey.pem; then
            echo "Copying SSL certificates from Let's Encrypt..."
            # Copy certificates (requires sudo to read from /etc/letsencrypt/)
            if sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ssl/; then
              if sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ssl/; then
                # Set correct ownership and permissions
                sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/
                sudo chmod 644 ssl/fullchain.pem
                sudo chmod 600 ssl/privkey.pem
                echo "✓ SSL certificates copied successfully"
              else
                echo "⚠️  Failed to copy privkey.pem"
                echo "This may indicate permission issues or missing source file"
                exit 1
              fi
            else
              echo "⚠️  Failed to copy fullchain.pem"
              echo "This may indicate permission issues or missing source file"
              exit 1
            fi
          elif [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates already exist in ~/aviationwx/ssl/"
            # Verify permissions are correct
            sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/ || true
            sudo chmod 644 ssl/fullchain.pem || true
            sudo chmod 600 ssl/privkey.pem || true
          else
            echo "⚠️  SSL certificates not found in /etc/letsencrypt/live/aviationwx.org/ or ~/aviationwx/ssl/"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "SSL CERTIFICATES MISSING"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Nginx container will fail to start without SSL certificates."
            echo "You need to generate certificates with Certbot first."
            echo ""
            
            # Check if Certbot is installed
            if command -v certbot >/dev/null 2>&1; then
              echo "✓ Certbot is installed"
              echo ""
              echo "To generate wildcard certificates (DNS-01 challenge):"
              echo ""
              echo "  1. Install Cloudflare DNS plugin:"
              echo "     sudo apt install certbot python3-certbot-dns-cloudflare -y"
              echo ""
              echo "  2. Create Cloudflare API token (scoped to zone):"
              echo "     - Permissions: Zone → DNS → Edit; Zone → Zone → Read"
              echo "     - Resources: Include → Specific zone → aviationwx.org"
              echo ""
              echo "  3. Store token:"
              echo "     mkdir -p ~/.secrets"
              echo "     printf 'dns_cloudflare_api_token = %s\\n' 'YOUR_TOKEN' > ~/.secrets/cloudflare.ini"
              echo "     chmod 600 ~/.secrets/cloudflare.ini"
              echo ""
              echo "  4. Generate certificates:"
              echo "     sudo certbot certonly \\"
              echo "       --dns-cloudflare \\"
              echo "       --dns-cloudflare-credentials ~/.secrets/cloudflare.ini \\"
              echo "       -d aviationwx.org -d '*.aviationwx.org' \\"
              echo "       --non-interactive --agree-tos -m your@email.com"
              echo ""
              echo "  5. Copy certificates:"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ~/aviationwx/ssl/"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ~/aviationwx/ssl/"
              echo "     sudo chown -R $CURRENT_USER:$CURRENT_USER ~/aviationwx/ssl"
              echo "     sudo chmod 644 ~/aviationwx/ssl/fullchain.pem"
              echo "     sudo chmod 600 ~/aviationwx/ssl/privkey.pem"
            else
              echo "⚠️  Certbot is not installed"
              echo ""
              echo "To install Certbot and generate certificates:"
              echo ""
              echo "  1. Install Certbot with Cloudflare DNS plugin:"
              echo "     sudo apt update && sudo apt install -y certbot python3-certbot-dns-cloudflare"
              echo ""
              echo "  2. Follow steps 2-5 above (create Cloudflare token, generate certs, copy them)"
            fi
            
            echo ""
            echo "After certificates are in place, redeploy to continue."
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            # Fail the deployment since Nginx won't start without certificates
            exit 1
          fi
          EOF

      - name: Configure firewall ports
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Configuring firewall ports for all services..."
          
          # Run firewall configuration script
          if [ -f scripts/deploy-configure-firewall.sh ]; then
            chmod +x scripts/deploy-configure-firewall.sh
            ./scripts/deploy-configure-firewall.sh
          else
            echo "⚠️  Firewall script not found, configuring ports manually..."
            # Fallback: configure ports directly (should match deploy-configure-firewall.sh)
            sudo ufw allow 80/tcp comment 'HTTP (Nginx)' || true
            sudo ufw allow 443/tcp comment 'HTTPS (Nginx)' || true
            sudo ufw allow 2121/tcp comment 'FTP/FTPS (Push webcams)' || true
            sudo ufw allow 2222/tcp comment 'SFTP (Push webcams)' || true
            sudo ufw allow 50000:50100/tcp comment 'FTP passive mode (Push webcams)' || true
            sudo ufw allow 22/tcp comment 'SSH (System access)' || true
            sudo ufw allow 500/udp comment 'IPsec IKE (VPN)' || true
            sudo ufw allow 4500/udp comment 'IPsec NAT-T (VPN)' || true
            sudo ufw status numbered
          fi
          EOF

      - name: Ensure cache directory exists
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          # Get www-data UID/GID from container (typically 33:33)
          # If container is running, get actual UID/GID from it
          WWW_DATA_UID=33
          WWW_DATA_GID=33
          if docker ps --format '{{.Names}}' | grep -q '^aviationwx-web$'; then
            # Container is running - get actual UID/GID
            CONTAINER_UID=$(docker exec aviationwx-web id -u www-data 2>/dev/null || echo "33")
            CONTAINER_GID=$(docker exec aviationwx-web id -g www-data 2>/dev/null || echo "33")
            WWW_DATA_UID=${CONTAINER_UID}
            WWW_DATA_GID=${CONTAINER_GID}
            echo "Detected www-data UID/GID from container: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          else
            echo "Container not running, using default www-data UID/GID: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          fi
          
          # Check if parent directory exists and who owns it
          if [ -d /tmp/aviationwx-cache ]; then
            CURRENT_OWNER=$(stat -c '%U:%G' /tmp/aviationwx-cache 2>/dev/null || stat -f '%Su:%Sg' /tmp/aviationwx-cache 2>/dev/null || echo "unknown")
            echo "Existing cache directory owner: ${CURRENT_OWNER}"
          fi
          
          # Try to create directory structure without sudo first
          if mkdir -p /tmp/aviationwx-cache/webcams 2>/dev/null; then
            echo "✓ Created cache directory without sudo"
          else
            echo "⚠️  Failed to create directory without sudo, trying with sudo..."
            # Use sudo to create directory structure
            sudo mkdir -p /tmp/aviationwx-cache/webcams || {
              echo "❌ Failed to create cache directory even with sudo"
              exit 1
            }
            echo "✓ Created cache directory with sudo"
          fi
          
          # Set ownership to match container's www-data user
          # Try without sudo first, then with sudo if needed
          if chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache 2>/dev/null; then
            echo "✓ Changed ownership without sudo"
          else
            echo "⚠️  Failed to change ownership without sudo, trying with sudo..."
            sudo chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache || {
              echo "⚠️  Failed to change ownership with sudo, using fallback permissions"
              # Fallback: make world-writable (less secure but works)
              sudo chmod -R 777 /tmp/aviationwx-cache || true
            }
          fi
          
          # Ensure directory permissions allow www-data to write
          # Use 775 for webcams (group writable) and 755 for parent
          sudo chmod 755 /tmp/aviationwx-cache 2>/dev/null || chmod 755 /tmp/aviationwx-cache || true
            sudo chmod 775 /tmp/aviationwx-cache/webcams 2>/dev/null || chmod 775 /tmp/aviationwx-cache/webcams || true
          
          # Verify directory was created and is writable
          if [ ! -d /tmp/aviationwx-cache/webcams ]; then
            echo "❌ Failed to create cache directory"
            echo "This is critical - webcam caching will not work without this directory"
            echo "Check permissions and disk space, then retry deployment"
            exit 1
          fi
          
          # Show final permissions
          echo "Final directory permissions:"
          ls -ld /tmp/aviationwx-cache
          ls -ld /tmp/aviationwx-cache/webcams
          
          # Test write access (as current user, which should work if permissions are correct)
          if touch /tmp/aviationwx-cache/webcams/.test_write 2>/dev/null; then
            rm -f /tmp/aviationwx-cache/webcams/.test_write
            echo "✓ Cache directory is writable by deployment user"
          else
            echo "⚠️  Cache directory may not be writable by deployment user"
            echo "   (This is OK if container user can write)"
          fi
          
          # Final verification
          echo "✓ Cache directory created at /tmp/aviationwx-cache/webcams"
          echo "✓ Ownership set to ${WWW_DATA_UID}:${WWW_DATA_GID} (www-data)"
          EOF

      - name: Sync repository to server
        run: |
          echo "Syncing repository to server..."
          # Exclude docker/docker from rsync (defensive - should not exist after path fix)
          # Exclude uploads directory (ephemeral, created inside container)
          # Exclude ssl directory (contains SSL certificates copied from Let's Encrypt, not in git)
          if ! rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' --exclude 'ssl' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/; then
            deployment_error \
              "File Synchronization (Rsync)" \
              "Rsync failed - files may not be synced correctly. This is critical - deployment cannot proceed without files being synced." \
              "1. Check network connectivity: ping ${{ secrets.HOST }}\n2. Verify SSH connection: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'echo Connection OK'\n3. Check server disk space: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'df -h'\n4. Verify permissions on server: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'ls -la ~/aviationwx'\n5. Check rsync logs for specific errors\n6. Retry deployment after resolving issues"
          fi
          echo "✓ Repository synced successfully"

      - name: Verify and restore SSL certificates after rsync
        run: |
          # Verify SSL certificates are still present after rsync
          # (rsync --delete could remove them if ssl/ wasn't excluded)
          echo ""
          echo "Verifying SSL certificates are still present after rsync..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          # Check if certificates exist
          if [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates verified after rsync"
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  WARNING: SSL certificates missing after rsync"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Rsync may have deleted the ssl/ directory. Restoring certificates..."
            
            # Use helper script to restore certificates
            if [ -f scripts/restore-ssl-certificates.sh ]; then
              chmod +x scripts/restore-ssl-certificates.sh
              ./scripts/restore-ssl-certificates.sh
            else
              # Source deployment helper if available
              if [ -f scripts/deploy-helpers.sh ]; then
                source scripts/deploy-helpers.sh
                deployment_error \
                  "SSL Certificate Restoration" \
                  "restore-ssl-certificates.sh script not found. This should not happen - script should be synced via rsync." \
                  "1. Verify script exists: ls -la scripts/restore-ssl-certificates.sh\n2. Check if rsync excluded the scripts directory\n3. Verify file permissions\n4. Manually restore certificates if needed: sudo cp /etc/letsencrypt/live/aviationwx.org/*.pem ssl/"
              else
                echo "❌ ERROR: restore-ssl-certificates.sh script not found"
                echo "This should not happen - script should be synced via rsync"
                exit 1
              fi
            fi
          fi
          EOF

      - name: Check for Docker config changes
        id: docker-config-changed
        run: |
          echo "Checking for Docker configuration file changes..."
          
          # Get the previous commit SHA (if available)
          # Note: github.event.before is only available for push events
          # For workflow_run/workflow_dispatch, we'll use git to find the parent
          PREV_SHA=""
          if [ "${{ github.event_name }}" = "push" ]; then
            PREV_SHA="${{ github.event.before }}"
          else
            # For workflow_run/workflow_dispatch, try to get parent from git
            # This will be set if we have the previous commit info
            PREV_SHA="${{ steps.previous-commit.outputs.previous_commit_sha }}"
          fi
          CURRENT_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          
          # Docker config files that require image rebuild
          DOCKER_CONFIG_FILES=(
            "docker/Dockerfile"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-entrypoint.sh"
            "docker/pam-vsftpd"
            "docker/logrotate-vsftpd"
            "docker/logrotate-sshd"
            "docker/fail2ban-jail.conf"
            "docker/fail2ban-vsftpd.conf"
            "docker/fail2ban-sshd.conf"
            "docker/docker-compose.prod.yml"
            "scripts/service-watchdog.sh"
            "scripts/setup-letsencrypt.sh"
            "scripts/enable-vsftpd-ssl.sh"
            "scripts/create-sftp-user.sh"
            "config/crontab"
          )
          
          CONFIG_CHANGED="false"
          
          # If we have a previous commit, check what changed
          if [ -n "$PREV_SHA" ] && [ "$PREV_SHA" != "0000000000000000000000000000000000000000" ]; then
            echo "Comparing changes from $PREV_SHA to $CURRENT_SHA..."
            for file in "${DOCKER_CONFIG_FILES[@]}"; do
              if git diff --name-only "$PREV_SHA" "$CURRENT_SHA" | grep -q "^${file}$"; then
                echo "  ✓ $file changed"
                CONFIG_CHANGED="true"
              fi
            done
          else
            # No previous commit (initial deploy or can't determine), assume config might have changed
            echo "⚠️  Cannot determine previous commit, will check for config changes on server"
            CONFIG_CHANGED="unknown"
          fi
          
          if [ "$CONFIG_CHANGED" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  Docker configuration files changed - will force rebuild"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          elif [ "$CONFIG_CHANGED" = "unknown" ]; then
            echo "⚠️  Will verify config changes on server and rebuild if needed"
          else
            echo "✓ No Docker configuration file changes detected"
          fi
          
          echo "config_changed=$CONFIG_CHANGED" >> $GITHUB_OUTPUT

      - name: Deploy via Docker Compose
        run: |
          # Get commit SHA from determined deployment SHA (7 characters to match GitHub's short SHA display)
          # Note: .git directory is excluded during rsync, so we can't get SHA from server's git repo
          GIT_SHA="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          echo "Deploying with GIT_SHA: ${GIT_SHA}"
          
          # Determine if we need to force rebuild
          FORCE_REBUILD="${{ steps.docker-config-changed.outputs.config_changed }}"
          
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          # Set GIT_SHA as environment variable for docker compose
          export GIT_SHA="$GIT_SHA"
          # Enable BuildKit for better caching (if not already enabled)
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Check for Docker config changes on server (fallback if git diff didn't work)
          if [ "$FORCE_REBUILD" = "unknown" ] || [ "$FORCE_REBUILD" = "false" ]; then
            echo "Verifying Docker config files on server..."
            CONFIG_CHANGED="false"
            
            # Docker config files that require image rebuild
            DOCKER_CONFIG_FILES=(
              "docker/Dockerfile"
              "docker/vsftpd.conf"
              "docker/sshd_config"
              "docker/docker-entrypoint.sh"
              "docker/pam-vsftpd"
              "docker/logrotate-vsftpd"
              "docker/logrotate-sshd"
              "docker/fail2ban-jail.conf"
              "docker/fail2ban-vsftpd.conf"
              "docker/fail2ban-sshd.conf"
              "docker/docker-compose.prod.yml"
              "scripts/service-watchdog.sh"
              "scripts/setup-letsencrypt.sh"
              "scripts/enable-vsftpd-ssl.sh"
              "scripts/create-sftp-user.sh"
              "config/crontab"
            )
            
            # Check if any config file was modified more recently than the Docker image
            if docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1 | grep -q .; then
              IMAGE_TIME=\$(docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1)
              IMAGE_TIMESTAMP=\$(date -d "\$IMAGE_TIME" +%s 2>/dev/null || echo 0)
              
              for file in "\${DOCKER_CONFIG_FILES[@]}"; do
                if [ -f "\$file" ]; then
                  FILE_TIMESTAMP=\$(stat -c %Y "\$file" 2>/dev/null || stat -f %m "\$file" 2>/dev/null || echo 0)
                  if [ "\$FILE_TIMESTAMP" -gt "\$IMAGE_TIMESTAMP" ]; then
                    echo "  ✓ \$file is newer than Docker image"
                    CONFIG_CHANGED="true"
                  fi
                fi
              done
            else
              # No image exists, must rebuild
              CONFIG_CHANGED="true"
            fi
            
            if [ "\$CONFIG_CHANGED" = "true" ]; then
              FORCE_REBUILD="true"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "⚠️  Docker configuration files changed - will force rebuild"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            fi
          fi
          
          # Build arguments for cache invalidation
          if [ "$FORCE_REBUILD" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Forcing rebuild without cache due to config changes..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            # Force rebuild without cache
            if ! docker compose -f docker/docker-compose.prod.yml build --no-cache web; then
              deployment_error \
                "Docker Build (No Cache)" \
                "Docker build failed when forcing rebuild without cache" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config\n2. Check disk space: df -h\n3. Check Docker daemon: systemctl status docker\n4. Review build logs above for specific errors\n5. Try manual build: docker compose -f docker/docker-compose.prod.yml build --no-cache web"
            fi
            # Start containers with the newly built image
            if ! docker compose -f docker/docker-compose.prod.yml up -d --pull; then
              echo "⚠️  Warning: Start with --pull failed, trying without --pull..."
              if ! docker compose -f docker/docker-compose.prod.yml up -d; then
                deployment_error \
                  "Container Startup" \
                  "Failed to start containers after build" \
                  "1. Check container logs: docker compose -f docker/docker-compose.prod.yml logs\n2. Verify container status: docker compose -f docker/docker-compose.prod.yml ps\n3. Check port conflicts: netstat -tuln | grep -E ':(80|443|2121|2222)'\n4. Review docker-compose.prod.yml configuration\n5. Check Docker daemon logs: journalctl -u docker"
              fi
            fi
            echo "✓ Containers rebuilt and started successfully (no cache)"
          elif ! docker compose -f docker/docker-compose.prod.yml up -d --build --pull; then
            echo "⚠️  Warning: Build with --pull failed, trying without --pull..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d --build; then
              deployment_error \
                "Docker Build/Start" \
                "Both build attempts (with --pull and without) failed" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config\n2. Verify all required files are present (check Dockerfile COPY commands)\n3. Check network connectivity for pulling base images: ping registry-1.docker.io\n4. Check disk space: df -h\n5. Check Docker daemon: systemctl status docker\n6. Review Docker logs: journalctl -u docker\n7. Try manual build: docker compose -f docker/docker-compose.prod.yml build web"
            fi
          fi
          
          echo "✓ Containers built and started successfully"
          
          # Wait for web container to be ready (needed for certificate generation)
          # Use polling instead of fixed sleep for faster startup detection
          echo "Waiting for web container to be ready..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type http \
              --timeout 30 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml \
              --url http://localhost:8080/; then
              echo "✓ Web container is ready and responding"
            else
              echo "⚠️  Warning: Web container did not become ready within timeout"
              echo "   Continuing anyway - certificate generation may fail"
            fi
          else
            # Fallback to simple status check if script not available
            echo "⚠️  Health check script not found, using fallback check..."
            sleep 5
            if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              echo "⚠️  Warning: Web container is not running, skipping certificate generation"
            fi
          fi
          
          # Verify container is running before attempting certificate operations
          if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
            echo "⚠️  Warning: Web container is not running, skipping certificate generation"
          else
            # Generate FTPS SSL certificates (one-time setup)
            # This runs after containers are started so the web server is available for HTTP-01 challenge
            echo "Checking for upload.aviationwx.org SSL certificates..."
            
            # Check if certificates already exist in container
            # This is a one-time setup that only runs if certificates don't exist
            if docker compose -f docker/docker-compose.prod.yml exec -T web test -f /etc/letsencrypt/live/upload.aviationwx.org/fullchain.pem 2>/dev/null && \
               docker compose -f docker/docker-compose.prod.yml exec -T web test -f /etc/letsencrypt/live/upload.aviationwx.org/privkey.pem 2>/dev/null; then
            echo "✓ SSL certificates for upload.aviationwx.org already exist"
            echo "  Skipping certificate generation (one-time setup already complete)"
          else
            echo "SSL certificates for upload.aviationwx.org not found"
            echo "Attempting one-time certificate generation..."
            echo ""
            echo "Requirements:"
            echo "  - Domain upload.aviationwx.org must resolve to this server"
            echo "  - Port 80 must be accessible (for Let's Encrypt validation)"
            echo "  - Web server must be running"
            echo ""
            
            # Try to generate certificates inside the container
            # Use LETSENCRYPT_EMAIL environment variable
            if docker compose -f docker/docker-compose.prod.yml exec -T -e LETSENCRYPT_EMAIL=alex@alexwitherspoon.com web /usr/local/bin/setup-letsencrypt.sh 2>&1; then
              echo "✓ SSL certificates generated successfully for upload.aviationwx.org"
              echo "  FTPS will be automatically enabled on next container start"
              # Restart container to pick up SSL configuration
              docker compose -f docker/docker-compose.prod.yml restart web || true
            else
              echo "⚠️  Warning: Failed to generate SSL certificates for upload.aviationwx.org"
              echo "  This may be due to:"
              echo "    - Domain not resolving to this server"
              echo "    - Port 80 not accessible"
              echo "    - Let's Encrypt rate limits"
              echo ""
              echo "  Deployment will continue, but FTPS will not be available until certificates are generated."
              echo "  You can manually generate certificates by running:"
              echo "    docker compose -f docker/docker-compose.prod.yml exec web /usr/local/bin/setup-letsencrypt.sh"
              echo ""
              # Don't fail deployment - just warn
            fi
          fi
          fi
          
          # Sync FTP/SFTP/FTPS configuration (runs on every deployment)
          # This ensures users and directories are created/updated immediately after deployment
          echo "Syncing FTP/SFTP/FTPS configuration..."
          if docker compose -f docker/docker-compose.prod.yml exec -T web /usr/local/bin/php /var/www/html/scripts/sync-push-config.php 2>&1; then
            echo "✓ FTP/SFTP/FTPS configuration synced successfully"
          else
            echo "⚠️  Warning: FTP/SFTP/FTPS configuration sync failed"
            echo "  This may be due to:"
            echo "    - Config file not yet available"
            echo "    - Permissions issues"
            echo "    - Database corruption (will be auto-recovered on next run)"
            echo ""
            echo "  Deployment will continue, but FTP/SFTP users may not be configured."
            echo "  The sync will retry on container startup."
            # Don't fail deployment - just warn
          fi
          
          # Clean up Docker resources conditionally (only if disk usage is high)
          # Check disk usage before cleanup to avoid unnecessary operations
          DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
          DISK_THRESHOLD=40  # Cleanup if disk usage > 40%
          
          echo "Checking disk usage before cleanup..."
          echo "Current disk usage: ${DISK_USAGE}% (threshold: ${DISK_THRESHOLD}%)"
          
          if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
            echo "Disk usage is above threshold - running Docker cleanup..."
            # Clean up Docker resources using the dedicated cleanup script
            # This modularizes cleanup logic and keeps the workflow concise
            if [ -f scripts/deploy-docker-cleanup.sh ]; then
              echo "Running Docker cleanup script..."
              chmod +x scripts/deploy-docker-cleanup.sh
              ./scripts/deploy-docker-cleanup.sh
            else
              echo "⚠️  Cleanup script not found, running basic cleanup..."
              docker builder prune -f --filter "until=24h" || true
              docker image prune -f || true
              docker system prune -f --filter "until=168h" || true
              docker system df
            fi
          else
            echo "Disk usage is below threshold - skipping cleanup to save time"
            echo "Current Docker disk usage:"
            docker system df 2>/dev/null || echo "Could not retrieve Docker disk usage"
          fi
          EOF

      - name: Restart Nginx container to pick up config changes
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "Restarting Nginx container..."
          # Restart Nginx container to ensure it picks up new nginx.conf
          # This is safer than reload since config file is mounted as volume
          if ! docker compose -f docker/docker-compose.prod.yml restart nginx; then
            echo "Restart failed, trying up -d..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d nginx; then
              if [ "$(type -t deployment_error)" = "function" ]; then
                NGINX_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50 || echo "Could not retrieve logs")
                deployment_error \
                  "Nginx Container Startup" \
                  "Failed to start Nginx container after restart attempt" \
                  "1. Check Nginx container logs:\n   ${NGINX_LOGS}\n2. Verify Nginx configuration: docker compose -f docker/docker-compose.prod.yml exec nginx nginx -t\n3. Check for port conflicts: netstat -tuln | grep -E ':(80|443)'\n4. Verify docker-compose.prod.yml nginx service configuration\n5. Check container status: docker compose -f docker/docker-compose.prod.yml ps nginx"
              else
                echo ""
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "❌ ERROR: Failed to start Nginx container"
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo ""
                echo "Container logs:"
                docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
                exit 1
              fi
            fi
          fi
          
          # Wait for Nginx to be ready (polling instead of fixed sleep)
          echo "Waiting for Nginx to be ready..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            scripts/wait-for-container-health.sh \
              --container nginx \
              --check-type status \
              --timeout 15 \
              --interval 1 \
              --compose-file docker/docker-compose.prod.yml || true
          else
            # Fallback to short sleep if script not available
            sleep 2
          fi
          
          echo "Testing Nginx configuration..."
          # Verify Nginx configuration is valid
          CONFIG_TEST_OUTPUT=$(docker compose -f docker/docker-compose.prod.yml exec -T nginx nginx -t 2>&1 || echo "FAILED")
          
          if echo "$CONFIG_TEST_OUTPUT" | grep -qE "syntax is ok|test is successful"; then
            echo "✓ Nginx configuration is valid"
          else
            if [ "$(type -t deployment_error)" = "function" ]; then
              NGINX_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50 || echo "Could not retrieve logs")
              deployment_error \
                "Nginx Configuration Validation" \
                "Nginx configuration test failed - configuration is invalid" \
                "1. Review Nginx config test output:\n   ${CONFIG_TEST_OUTPUT}\n2. Check container logs:\n   ${NGINX_LOGS}\n3. Check nginx.conf syntax: docker compose -f docker/docker-compose.prod.yml exec nginx nginx -t\n4. Verify all referenced files exist\n5. Check for missing semicolons, brackets, or typos\n6. Review docker/nginx.conf and docker/nginx-main.conf\n7. Fix configuration errors and retry deployment"
            else
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: Nginx configuration is invalid"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "Nginx config test output:"
              echo "$CONFIG_TEST_OUTPUT"
              echo ""
              echo "Container logs:"
              docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
              echo ""
              echo "Fix the Nginx configuration and retry deployment."
              exit 1
            fi
          fi
          
          # Verify Nginx is responding (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Nginx not responding on port 80 (may be expected if SSL only)"
          else
            echo "✓ Nginx is responding"
          fi
          
          # Verify SSL certificates are accessible inside nginx container
          echo ""
          echo "Verifying SSL certificates are accessible in nginx container..."
          if docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/fullchain.pem && \
             docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/privkey.pem; then
            echo "✓ SSL certificates are accessible in nginx container"
          else
            if [ "$(type -t deployment_error)" = "function" ]; then
              HOST_SSL=$(ls -la ssl/ 2>/dev/null || echo "ssl/ directory not found on host")
              CONTAINER_MOUNTS=$(docker inspect aviationwx-nginx 2>/dev/null | grep -A 20 Mounts || echo "Could not inspect container")
              deployment_error \
                "SSL Certificate Accessibility" \
                "SSL certificates not accessible in nginx container. This indicates a volume mount problem." \
                "1. Check host-side certificates:\n   ${HOST_SSL}\n2. Check container volume mounts:\n   ${CONTAINER_MOUNTS}\n3. Verify ssl/ directory exists on host: ls -la ~/aviationwx/ssl/\n4. Check docker-compose.prod.yml volume configuration\n5. Ensure certificates were copied correctly in earlier steps\n6. Verify file permissions: chmod 644 ssl/fullchain.pem && chmod 600 ssl/privkey.pem"
            else
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: SSL certificates not accessible in nginx container"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "This indicates a volume mount problem."
              echo "Checking host-side certificates..."
              ls -la ssl/ || echo "ssl/ directory not found on host"
              echo ""
              echo "Container volume mounts:"
              docker inspect aviationwx-nginx | grep -A 20 Mounts || echo "Could not inspect container"
              echo ""
              echo "Fix: Ensure ssl/ directory exists on host and contains certificates."
              exit 1
            fi
          fi
          EOF

      - name: Purge browser and service worker caches on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Force service worker update by updating public/js/service-worker.js timestamp
          # This ensures clients will download new version
          # Done after Nginx restart to ensure fresh content is served
          touch public/js/service-worker.js || true
          echo "✓ Updated service worker file timestamp to force cache bust"
          
          # If using Nginx proxy_cache, we would purge here
          # Currently not using proxy_cache, but keeping this for future reference
          # docker exec nginx nginx -s reload || true
          EOF

      - name: Post-deployment verification (quick check)
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Verification (Quick Check)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Verify deployed commit SHA
          EXPECTED_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          EXPECTED_SHA_SHORT="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          
          echo "Verifying deployed commit SHA..."
          echo "Expected: ${EXPECTED_SHA_SHORT}"
          
          # Get actual SHA from container environment variable
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -z "$ACTUAL_SHA" ]; then
            echo "⚠️  Warning: GIT_SHA environment variable not set in container"
            echo "This may indicate the deployment didn't set the variable correctly"
          else
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            echo "Actual: ${ACTUAL_SHA_SHORT}"
            
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              if [ "$(type -t deployment_error)" = "function" ]; then
                deployment_error \
                  "Commit SHA Verification" \
                  "Deployed commit SHA does not match expected SHA. Expected: ${EXPECTED_SHA_SHORT}, Actual: ${ACTUAL_SHA_SHORT}" \
                  "1. This indicates a partial deployment or deployment of wrong code\n2. Check if rsync completed successfully\n3. Verify GIT_SHA environment variable was set correctly\n4. Check container logs: docker compose -f docker/docker-compose.prod.yml logs web\n5. Verify deployment completed all steps\n6. Consider rolling back if system is in inconsistent state"
              else
                echo ""
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "❌ ERROR: Deployed commit SHA does not match expected SHA"
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "Expected: ${EXPECTED_SHA_SHORT}"
                echo "Actual: ${ACTUAL_SHA_SHORT}"
                echo ""
                echo "This indicates a partial deployment or deployment of wrong code."
                exit 1
              fi
            fi
            
            echo "✓ Deployed commit SHA matches expected SHA"
          fi
          
          # Quick service check
          echo ""
          echo "Quick service response check..."
          if ! curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Service not responding on https://aviationwx.org/"
            echo "This may be expected if services are still starting"
            echo "Full health check will verify this in the next step"
          else
            echo "✓ Service is responding"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✓ Post-deployment verification complete"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF

      - name: Verify fail2ban in container
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Verifying fail2ban in container..."
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Wait for fail2ban to be ready using polling (faster than fixed sleep)
          echo "Waiting for fail2ban to be ready..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type fail2ban \
              --timeout 30 \
              --interval 3 \
              --compose-file docker/docker-compose.prod.yml; then
              echo "✓ fail2ban is ready"
              FAIL2BAN_RUNNING=true
            else
              echo "❌ fail2ban did not become ready within timeout"
              FAIL2BAN_RUNNING=false
            fi
          else
            # Fallback to manual check if script not available
            echo "⚠️  Health check script not found, using fallback check..."
            sleep 10
            FAIL2BAN_RUNNING=false
            
            # Retry logic: check up to 3 times with 5s intervals
            MAX_RETRIES=3
            RETRY_INTERVAL=5
            
            for i in $(seq 1 $MAX_RETRIES); do
              echo ""
              echo "Attempt $i/$MAX_RETRIES: Checking fail2ban..."
              
              # Check if process is running
              if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x fail2ban-server > /dev/null 2>&1; then
                echo "✓ fail2ban-server process is running"
                
                # Verify we can connect to fail2ban client
                if docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client ping > /dev/null 2>&1; then
                  echo "✓ fail2ban-client is responsive"
                  
                  # Get status and verify jails are active
                  STATUS_OUTPUT=$(docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client status 2>&1 || echo "")
                  
                  if echo "$STATUS_OUTPUT" | grep -q "Jail list"; then
                    JAIL_LIST=$(echo "$STATUS_OUTPUT" | grep -A 10 "Jail list" | tail -n +2 | tr -d ' \t' || echo "")
                    
                    if [ -n "$JAIL_LIST" ]; then
                      echo "✓ Active jails found:"
                      echo "$JAIL_LIST" | sed 's/^/  - /'
                      
                      # Verify at least one jail is enabled (vsftpd or sshd-sftp)
                      if echo "$JAIL_LIST" | grep -qE "(vsftpd|sshd-sftp)"; then
                        echo "✓ Required jails (vsftpd/sshd-sftp) are active"
                        FAIL2BAN_RUNNING=true
                        break
                      else
                        echo "⚠️  Warning: No required jails found in jail list"
                        echo "Jail list: $JAIL_LIST"
                      fi
                    else
                      echo "⚠️  Warning: Jail list is empty"
                    fi
                  else
                    echo "⚠️  Warning: Could not get jail list from status"
                    echo "Status output: $STATUS_OUTPUT"
                  fi
                else
                  echo "⚠️  Warning: fail2ban-client is not responsive"
                fi
              else
                echo "⚠️  Warning: fail2ban-server process not found"
              fi
              
              if [ $i -lt $MAX_RETRIES ]; then
                echo "Waiting ${RETRY_INTERVAL}s before retry..."
                sleep $RETRY_INTERVAL
              fi
            done
          fi
          
          # Fail deployment if fail2ban is not running properly
          if [ "$FAIL2BAN_RUNNING" != "true" ]; then
            # Collect debugging information before calling deployment_error
            FAIL2BAN_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs web | grep -i fail2ban | tail -20 || echo "No fail2ban logs found")
            FAIL2BAN_PROCESSES=$(docker compose -f docker/docker-compose.prod.yml exec -T web ps aux | grep -E "(fail2ban|vsftpd|sshd)" || echo "No related processes found")
            FAIL2BAN_CONFIG=$(docker compose -f docker/docker-compose.prod.yml exec -T web ls -la /etc/fail2ban/jail.d/ 2>/dev/null || echo "Jail directory not found")
            FAIL2BAN_STATUS=$(docker compose -f docker/docker-compose.prod.yml exec -T web systemctl status fail2ban 2>/dev/null || echo "systemctl not available")
            
            deployment_error \
              "fail2ban Verification" \
              "fail2ban verification failed after $MAX_RETRIES attempts. This is a CRITICAL security feature - deployment cannot proceed without fail2ban." \
              "1. Check container logs (fail2ban related):\n   ${FAIL2BAN_LOGS}\n2. Check container processes:\n   ${FAIL2BAN_PROCESSES}\n3. Verify fail2ban configuration:\n   ${FAIL2BAN_CONFIG}\n4. Check fail2ban service status:\n   ${FAIL2BAN_STATUS}\n5. Review Dockerfile for fail2ban installation\n6. Check entrypoint script for fail2ban startup\n7. Verify fail2ban configuration files are copied correctly"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✓ fail2ban verification complete - all checks passed"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo ""
          echo "Note: fail2ban runs inside the container with NET_ADMIN capability"
          echo "to manage iptables rules for brute-force protection."
          EOF
      
      - name: Post-deployment health check
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Health Checks"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Checking for partial deployment failures..."
          echo ""
          
          HEALTH_CHECK_FAILED=false
          FAILURE_REASONS=()
          PARTIAL_DEPLOYMENT=false
          
          # Check 1: Verify deployment completed (not partial)
          echo "1. Checking deployment completeness..."
          
          # Check if files were synced
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Files not synced - partial deployment detected")
          fi
          
          # Check if containers exist
          if ! docker compose -f docker/docker-compose.prod.yml ps web > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Containers not found - partial deployment detected")
          fi
          
          # Wait for containers to be ready using polling (faster than fixed sleep)
          echo ""
          echo "2. Waiting for containers to start..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            # First check container is running
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type status \
              --timeout 20 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml; then
              echo "✓ Container is running"
            else
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Web container not running")
            fi
          else
            # Fallback to simple check if script not available
            sleep 5
            if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Web container not running")
            fi
          fi
          
          # Check 3: Containers running
          echo "3. Checking container status..."
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps --format json 2>/dev/null || docker compose -f docker/docker-compose.prod.yml ps)
          
          # Count running containers (both web and nginx should be up)
          RUNNING_COUNT=$(docker compose -f docker/docker-compose.prod.yml ps --status running --format json 2>/dev/null | grep -c '"State":"running"' || docker compose -f docker/docker-compose.prod.yml ps | grep -c "Up" || echo "0")
          
          # Check that we have at least the web container (nginx is optional but should be running too)
          if [ "$RUNNING_COUNT" -lt "1" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Web container not running")
          else
            echo "✓ Containers are running (${RUNNING_COUNT} container(s) up)"
          fi
          
          # Check 4: Container health status (using polling script)
          echo ""
          echo "4. Checking container health..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type health \
              --timeout 60 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml; then
              echo "✓ Web container is healthy"
            else
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Container did not become healthy within timeout")
            fi
          else
            # Fallback to manual polling if script not available
            MAX_WAIT=60
            WAIT_TIME=0
            while [ $WAIT_TIME -lt $MAX_WAIT ]; do
              HEALTH=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | grep -o '"Health":"[^"]*"' | cut -d'"' -f4 || echo "unknown")
              if [ "$HEALTH" = "healthy" ]; then
                echo "✓ Web container is healthy"
                break
              elif [ "$HEALTH" = "unhealthy" ]; then
                HEALTH_CHECK_FAILED=true
                FAILURE_REASONS+=("Container health status: unhealthy")
                break
              else
                echo "  Container health: $HEALTH (waiting...)"
                sleep 2
                WAIT_TIME=$((WAIT_TIME + 2))
              fi
            done
            
            if [ $WAIT_TIME -ge $MAX_WAIT ]; then
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Container did not become healthy within ${MAX_WAIT} seconds")
            fi
          fi
          
          # Check 5: Service responding (HTTP check)
          echo ""
          echo "5. Checking service response..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type http \
              --timeout 15 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml \
              --url http://localhost:8080/; then
              echo "✓ Web container is responding (internal check passed)"
            else
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Homepage not accessible (internal check failed)")
            fi
          else
            # Fallback to simple check if script not available
            if ! docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s --max-time 5 http://localhost:8080/ > /dev/null 2>&1; then
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Homepage not accessible (internal check failed)")
            else
              echo "✓ Web container is responding (internal check passed)"
            fi
          fi
          
          # Check 6: Verify commit SHA matches (detect partial code deployment)
          echo ""
          echo "6. Verifying deployed code..."
          EXPECTED_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          EXPECTED_SHA_SHORT="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -n "$ACTUAL_SHA" ]; then
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Code mismatch - expected ${EXPECTED_SHA_SHORT}, got ${ACTUAL_SHA_SHORT}")
            else
              echo "✓ Deployed code matches expected commit SHA"
            fi
          fi
          
          # Check 7: API responding
          echo ""
          echo "7. Checking API response..."
          APP_PORT=${APP_PORT:-8080}
          if ! curl -f -s --max-time 10 "https://aviationwx.org/api/weather.php?id=kspb" > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            FAILURE_REASONS+=("Weather API not responding")
          else
            echo "✓ Weather API responding"
          fi
          
          # Final check
          if [ "$HEALTH_CHECK_FAILED" = "true" ]; then
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ HEALTH CHECKS FAILED"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            if [ "$PARTIAL_DEPLOYMENT" = "true" ]; then
              echo "⚠️  PARTIAL DEPLOYMENT DETECTED"
              echo "Some deployment steps completed, others failed."
              echo "System may be in inconsistent state."
              echo ""
            fi
            
            echo "Failure reasons:"
            for reason in "${FAILURE_REASONS[@]}"; do
              echo "  ❌ $reason"
            done
            echo ""
            
            echo "Container status:"
            docker compose -f docker/docker-compose.prod.yml ps
            echo ""
            
            echo "Container logs (last 50 lines):"
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            echo ""
            
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Triggering automatic rollback..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            exit 1  # This will trigger the rollback step
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ All health checks passed"
          echo "✅ Deployment completed successfully (no partial deployment detected)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Check if diagnostics page loads (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1:${APP_PORT}/diagnostics.php > /dev/null 2>&1; then
            echo "⚠️  Diagnostics page not accessible (may be expected)"
          else
            echo "✓ Diagnostics page accessible"
          fi
          
          # Check if config-utils.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/config.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "config.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ config.php not found in container"
              exit 1
            fi
          fi
          
          echo "✓ config.php present"
          
          # Check if rate-limit.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/rate-limit.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "rate-limit.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ rate-limit.php not found in container"
              exit 1
            fi
          fi
          
          echo "✓ rate-limit.php present"
          
          # Check APCu extension
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web php -m | grep -q apcu; then
            echo "⚠️  APCu extension not loaded (rate limiting will be disabled)"
          else
            echo "✓ APCu extension loaded"
          fi
          
          # Check push webcam scripts exist
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/process-push-webcams.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "process-push-webcams.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/scripts/"
            else
              echo "❌ process-push-webcams.php not found in container"
              exit 1
            fi
          fi
          echo "✓ process-push-webcams.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/sync-push-config.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "sync-push-config.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/scripts/"
            else
              echo "❌ sync-push-config.php not found in container"
              exit 1
            fi
          fi
          echo "✓ sync-push-config.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/push-webcam-validator.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "push-webcam-validator.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ push-webcam-validator.php not found in container"
              exit 1
            fi
          fi
          echo "✓ push-webcam-validator.php present"
          
          # Check FTP/SFTP services (may not be running if no push cameras configured)
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x vsftpd > /dev/null 2>&1; then
            echo "✓ vsftpd service running"
          else
            echo "ℹ️  vsftpd service not running (expected if no push cameras configured)"
          fi
          
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x sshd > /dev/null 2>&1; then
            echo "✓ sshd service running"
          else
            echo "ℹ️  sshd service not running (expected if no push cameras configured)"
          fi
          
          echo "✅ All health checks passed"
          
          # Test webcam endpoints (smoke test)
          echo "Testing webcam endpoints..."
          # Test JPG endpoint for sample airport (assuming kspb exists)
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" > /tmp/webcam_test.jpg 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.jpg 2>/dev/null || stat -c%s /tmp/webcam_test.jpg 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/jpeg"; then
              echo "✓ Webcam JPG endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.jpg
            else
              echo "⚠️  Webcam JPG endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "⚠️  Webcam JPG endpoint not accessible (may be expected if no webcams configured)"
          fi
          
          # Test WEBP endpoint
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" > /tmp/webcam_test.webp 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.webp 2>/dev/null || stat -c%s /tmp/webcam_test.webp 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/webp"; then
              echo "✓ Webcam WEBP endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.webp
            else
              echo "⚠️  Webcam WEBP endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "ℹ️  Webcam WEBP endpoint not accessible (may be expected if WEBP not generated yet)"
          fi
          
          EOF

      - name: Rollback to previous version (if needed)
        if: failure()
        run: |
          # First, get rollback state from server to determine what to rollback to
          ROLLBACK_STATE=$(ssh ${{ secrets.USER }}@${{ secrets.HOST }} "cat ~/aviationwx-rollback-state.json 2>/dev/null" || echo "")
          
          if [ -z "$ROLLBACK_STATE" ]; then
            echo "⚠️  No rollback state file found on server"
            echo "This may be the first deployment or rollback state was not saved"
            echo "Manual intervention required"
            exit 1
          fi
          
          ROLLBACK_TAG=$(echo "$ROLLBACK_STATE" | jq -r '.rollback_tag // "null"')
          PREVIOUS_SHA=$(echo "$ROLLBACK_STATE" | jq -r '.commit_sha // "unknown"')
          PREVIOUS_COMMIT_SHA=$(echo "$ROLLBACK_STATE" | jq -r '.previous_commit_sha // ""')
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "AUTOMATIC ROLLBACK INITIATED"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Rolling back to previous working state..."
          echo "Previous commit SHA: ${PREVIOUS_SHA}"
          echo "Previous git commit: ${PREVIOUS_COMMIT_SHA}"
          echo "Rollback tag: ${ROLLBACK_TAG}"
          echo ""
          
          # Step 1: Restore code files from previous commit (on GitHub Actions runner)
          if [ -n "$PREVIOUS_COMMIT_SHA" ] && [ "$PREVIOUS_COMMIT_SHA" != "" ]; then
            echo "Step 1: Restoring code files from previous commit ${PREVIOUS_COMMIT_SHA:0:7}..."
            
            # Checkout previous commit on the runner
            if git checkout "$PREVIOUS_COMMIT_SHA" 2>/dev/null; then
              echo "✓ Checked out previous commit on runner"
              
              # Sync previous commit code to server
              echo "Syncing previous commit code to server..."
              if rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' --exclude 'ssl' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/; then
                echo "✓ Code files restored from previous commit"
              else
                echo "⚠️  Warning: Failed to sync code files (rsync failed)"
                echo "   Will continue with Docker image rollback"
              fi
            else
              echo "⚠️  Warning: Failed to checkout previous commit (git checkout failed)"
              echo "   Will continue with Docker image rollback only"
            fi
          else
            echo "Step 1: Skipping code file restoration (no previous commit SHA available)"
          fi
          
          # Step 2: Restore Docker image and start containers on server
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          # Step 2: Stop current containers (even if partially started)
          echo "Step 2: Stopping current containers (cleaning up partial deployment)..."
          docker compose -f docker/docker-compose.prod.yml down || true
          sleep 2
          
          # Step 3: Restore Docker image
          if [ "$ROLLBACK_TAG" != "null" ] && docker images "aviationwx:$ROLLBACK_TAG" --format "{{.Repository}}:{{.Tag}}" 2>/dev/null | grep -q "aviationwx:$ROLLBACK_TAG"; then
            echo "Step 3: Restoring previous Docker image..."
            if docker tag "aviationwx:$ROLLBACK_TAG" "aviationwx:latest" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
            else
              echo "❌ ERROR: Failed to tag rollback image"
              echo "Will attempt to start containers anyway (may need rebuild)"
            fi
          else
            echo "Step 3: No rollback image available (will use existing or rebuild)"
          fi
          
          # Step 4: Start containers with rollback image and previous commit SHA
          echo "Step 4: Starting containers with rollback configuration..."
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Set GIT_SHA to previous commit for rollback
          if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "unknown" ]; then
            export GIT_SHA="${PREVIOUS_SHA:0:7}"
            echo "Setting GIT_SHA to previous commit: ${GIT_SHA}"
          fi
          
          if ! docker compose -f docker/docker-compose.prod.yml up -d; then
            echo "❌ ERROR: Failed to start containers with rollback configuration"
            echo "Manual intervention required"
            echo ""
            echo "Debugging info:"
            docker compose -f docker/docker-compose.prod.yml ps
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            exit 1
          fi
          
          # Step 5: Wait for services to be ready
          echo "Step 5: Waiting for services to be ready..."
          MAX_WAIT=30
          WAIT_TIME=0
          while [ $WAIT_TIME -lt $MAX_WAIT ]; do
            if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              # Check if container is actually responding
              if docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s http://localhost:8080/ > /dev/null 2>&1; then
                echo "✓ Services are ready after ${WAIT_TIME}s"
                break
              fi
            fi
            sleep 2
            WAIT_TIME=$((WAIT_TIME + 2))
          done
          
          # Step 6: Verify rollback was successful
          if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
            echo "Step 6: Verifying rollback..."
            
            # Verify GIT_SHA matches previous commit
            if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "unknown" ]; then
              ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
              if [ -n "$ACTUAL_SHA" ]; then
                if [ "${ACTUAL_SHA:0:7}" = "${PREVIOUS_SHA:0:7}" ]; then
                  echo "✓ GIT_SHA matches previous commit: ${ACTUAL_SHA:0:7}"
                else
                  echo "⚠️  Warning: GIT_SHA mismatch (expected ${PREVIOUS_SHA:0:7}, got ${ACTUAL_SHA:0:7})"
                fi
              fi
            fi
            
            # Verify service is responding
            if curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
              echo "✓ Service is responding after rollback"
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "✅ Rollback completed successfully"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "System restored to previous working state:"
              echo "  - Code files: ${PREVIOUS_COMMIT_SHA:0:7}"
              echo "  - Docker image: ${ROLLBACK_TAG}"
              echo "  - Commit SHA: ${PREVIOUS_SHA:0:7}"
            else
              echo "⚠️  Warning: Service not responding after rollback"
              echo "Container is running but service may not be fully functional"
              echo "Check logs: docker compose -f docker/docker-compose.prod.yml logs web"
            fi
          else
            echo "❌ ERROR: Containers not running after rollback"
            echo "Manual intervention required"
            echo ""
            echo "Debugging info:"
            docker compose -f docker/docker-compose.prod.yml ps
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            exit 1
          fi
          EOF

