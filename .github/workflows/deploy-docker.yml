name: Deploy to Production
run-name: "Deploy ${{ github.event.workflow_run.head_sha || github.sha }} to Production"

on:
  workflow_run:
    workflows: ["Quality Assurance Tests"]
    types:
      - completed
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      skip_prerequisites:
        description: 'Skip prerequisite checks (DANGEROUS - only use in emergencies)'
        required: false
        default: 'false'
        type: choice
        options:
          - 'false'
          - 'true'
      deployment_reason:
        description: 'Reason for manual deployment'
        required: true
        type: string
      deploy_sha:
        description: 'Commit SHA to deploy (leave empty to deploy latest)'
        required: false
        type: string

# Prevent concurrent deployments - only one deployment at a time
# All deploys share ONE concurrency group to ensure serialization
# This prevents race conditions when multiple commits are pushed rapidly
# The run-name includes the SHA for identification in the UI
concurrency:
  group: deploy-production
  cancel-in-progress: false  # Don't cancel - queue deployments in order

permissions:
  contents: read
  actions: read
  checks: read

jobs:
  check-prerequisites:
    runs-on: ubuntu-latest
    timeout-minutes: 35
    outputs:
      qa-success: ${{ steps.check.outputs.qa-success }}
      skip-checks: ${{ steps.check.outputs.skip-checks }}
      deploy-sha: ${{ steps.determine-sha-prereq.outputs.deploy_sha }}
      deploy-sha-short: ${{ steps.determine-sha-prereq.outputs.deploy_sha_short }}
    steps:
      - name: Determine deployment SHA
        id: determine-sha-prereq
        run: |
          # For workflow_run events, use the triggering workflow's SHA
          # For manual dispatch, use provided SHA or default to latest
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            DEPLOY_SHA="${{ github.event.workflow_run.head_sha }}"
            if [ -z "$DEPLOY_SHA" ] || [ "$DEPLOY_SHA" = "" ]; then
              echo "❌ ERROR: workflow_run.head_sha is empty or missing"
              echo "This indicates the workflow_run event is malformed"
              exit 1
            fi
            echo "Using SHA from workflow_run event: ${DEPLOY_SHA:0:7}"
          else
            # Manual dispatch: use provided SHA or default to current SHA
            PROVIDED_SHA="${{ github.event.inputs.deploy_sha }}"
            if [ -n "$PROVIDED_SHA" ] && [ "$PROVIDED_SHA" != "" ]; then
              DEPLOY_SHA="$PROVIDED_SHA"
              echo "Using provided SHA: ${DEPLOY_SHA:0:7}"
            else
              DEPLOY_SHA="${{ github.sha }}"
              echo "Using latest SHA (default): ${DEPLOY_SHA:0:7}"
            fi
          fi
          
          # Validate SHA format (should be 40 characters for full SHA, or 7+ for short)
          if [ ${#DEPLOY_SHA} -lt 7 ]; then
            echo "❌ ERROR: Invalid SHA format: ${DEPLOY_SHA}"
            echo "SHA must be at least 7 characters"
            exit 1
          fi
          
          echo "deploy_sha=${DEPLOY_SHA}" >> $GITHUB_OUTPUT
          echo "deploy_sha_short=${DEPLOY_SHA:0:7}" >> $GITHUB_OUTPUT
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deployment SHA (for prerequisites check): ${DEPLOY_SHA}"
          echo "Short SHA: ${DEPLOY_SHA:0:7}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Wait for and check required workflows status
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            const skipInput = '${{ github.event.inputs.skip_prerequisites }}';
            const isManualDispatch = '${{ github.event_name }}' === 'workflow_dispatch';
            const deploymentReason = '${{ github.event.inputs.deployment_reason }}';
            const deploySha = '${{ steps.determine-sha-prereq.outputs.deploy_sha }}';
            const deployShaShort = '${{ steps.determine-sha-prereq.outputs.deploy_sha_short }}';
            
            // Fast-fail: Check QA conclusion for workflow_run events first
            if (!isManualDispatch) {
              const triggeringConclusion = '${{ github.event.workflow_run.conclusion }}';
              
              if (triggeringConclusion !== 'success') {
                const conclusion = triggeringConclusion || 'unknown';
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.error(`❌ DEPLOYMENT BLOCKED: QA Tests Did Not Pass`);
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.error(`QA Workflow Conclusion: ${conclusion}`);
                core.error(`Commit SHA: ${deployShaShort}`);
                core.error(``);
                core.error(`Deployment cannot proceed because Quality Assurance tests failed.`);
                core.error(`This workflow will exit immediately - deploy job will not run.`);
                core.error(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
                core.setFailed('❌ QA Tests did not pass - deployment blocked. Fix QA failures and retry.');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ QA workflow succeeded - proceeding with additional checks`);
              
              const commitSha = deploySha;
              const branchName = '${{ github.event.workflow_run && github.event.workflow_run.head_branch || github.ref_name }}';
              const workflowRunId = '${{ github.event.workflow_run.id }}';
              
              // Verify the workflow_run SHA matches what we determined
              const workflowRunSha = '${{ github.event.workflow_run.head_sha }}';
              if (workflowRunSha !== commitSha) {
                core.error(`❌ CRITICAL: SHA mismatch detected!`);
                core.error(`   workflow_run.head_sha: ${workflowRunSha.substring(0, 7)}`);
                core.error(`   Determined SHA: ${commitSha.substring(0, 7)}`);
                core.error(`   This indicates a potential race condition or workflow_run event issue`);
                core.setFailed('❌ SHA mismatch - cannot proceed with deployment');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ Verified workflow_run SHA matches determined SHA: ${commitSha.substring(0, 7)}`);
              
              if (branchName !== 'main') {
                core.warning(`⚠️  Skipping deployment - not on main branch (branch: ${branchName})`);
                core.warning('Only deployments from main branch are allowed');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              // Verify the commit actually exists on main branch
              try {
                const commit = await github.rest.repos.getCommit({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  ref: commitSha
                });
                
                if (commit.data.commit.message) {
                  core.info(`✓ Commit verified: ${commitSha.substring(0, 7)} - ${commit.data.commit.message.split('\n')[0].substring(0, 60)}`);
                }
              } catch (error) {
                core.error(`❌ ERROR: Commit ${commitSha.substring(0, 7)} does not exist in repository`);
                core.error(`   This may indicate the SHA is invalid or the commit was force-pushed`);
                core.setFailed('❌ Commit verification failed - cannot proceed with deployment');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              const deployWorkflowPath = '.github/workflows/deploy-docker.yml';
              const currentRunId = context.runId;
              
              // Check for active deployments of the SAME commit (concurrency handles different commits)
              const allDeployRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: deployWorkflowPath,
                per_page: 20
              });
              
              const otherRuns = allDeployRuns.data.workflow_runs.filter(run => run.id !== currentRunId);
              
              // Check for active deployments of the same commit
              const activeDeploysSameCommit = otherRuns.filter(run => 
                (run.status === 'in_progress' || run.status === 'queued') &&
                run.head_sha === commitSha
              );
              
              if (activeDeploysSameCommit.length > 0) {
                core.info(`⏭️  Skipping deployment - another deployment for commit ${commitSha.substring(0, 7)} is already active`);
                core.info(`   This prevents duplicate deployments of the same commit`);
                core.setOutput('qa-success', 'false');
                return;
              }
              
              // Check for recent successful deployment of the same commit (within 5 minutes)
              const now = Date.now();
              const recentSuccessfulDeploys = otherRuns.filter(run => {
                if (run.status !== 'completed' || run.conclusion !== 'success') return false;
                if (run.head_sha !== commitSha) return false;  // Only check same commit
                const completedAt = new Date(run.updated_at || run.created_at).getTime();
                const ageMinutes = (now - completedAt) / (1000 * 60);
                return ageMinutes < 5;
              });
              
              if (recentSuccessfulDeploys.length > 0) {
                core.info(`⏭️  Skipping deployment - commit ${commitSha.substring(0, 7)} was recently deployed`);
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✓ All checks passed - ready to deploy commit ${commitSha.substring(0, 7)}`);
              core.setOutput('qa-success', 'true');
              core.setOutput('skip-checks', 'false');
              return;
            }
            
            if (isManualDispatch) {
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              core.info(`Manual deployment triggered`);
              core.info(`Reason: ${deploymentReason || 'Not provided'}`);
              core.info(`━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━`);
              
              if (skipInput === 'true') {
                core.warning('');
                core.warning('⚠️  WARNING: Prerequisite checks are being SKIPPED');
                core.warning('This is DANGEROUS and should only be used in emergencies');
                core.warning('Deployment will proceed without verifying tests passed');
                core.warning('');
                core.setOutput('skip-checks', 'true');
                core.setOutput('qa-success', 'true');
                return;
              } else {
                core.setOutput('skip-checks', 'false');
              }
            } else {
              core.setOutput('skip-checks', 'false');
            }
            
            // Manual dispatch: verify QA workflow passed
            if (isManualDispatch && skipInput !== 'true') {
              // Use the determined deployment SHA
              const commitSha = deploySha;
              const branchName = '${{ github.ref_name }}';
              
              core.info(`Checking QA tests for commit: ${deployShaShort}`);
              const providedSha = '${{ github.event.inputs.deploy_sha }}';
              if (providedSha && providedSha.trim() !== '') {
                core.info(`Using provided SHA: ${deployShaShort}`);
              } else {
                core.info(`Using default SHA (latest): ${deployShaShort}`);
              }
              
              const triggeringWorkflowRunIdRaw = '${{ github.event.workflow_run && github.event.workflow_run.id }}';
              const triggeringWorkflowRunId = (triggeringWorkflowRunIdRaw && triggeringWorkflowRunIdRaw !== '') ? triggeringWorkflowRunIdRaw : null;
              
              const qaWorkflowPath = '.github/workflows/quality-assurance-tests.yml';
              const maxWaitTime = 30 * 60 * 1000;
              const checkInterval = 30 * 1000;
              const startTime = Date.now();
              
              let qaRun = null;
              let qaSuccess = false;
              
              while (Date.now() - startTime < maxWaitTime) {
                const qaRuns = await github.rest.actions.listWorkflowRuns({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  workflow_id: qaWorkflowPath,
                  head_sha: commitSha,
                  branch: branchName,
                  per_page: 10
                });
                
                if (triggeringWorkflowRunId) {
                  qaRun = qaRuns.data.workflow_runs.find(run => 
                    run.id.toString() === triggeringWorkflowRunId.toString()
                  );
                }
                
                if (!qaRun) {
                  qaRun = qaRuns.data.workflow_runs.find(run => 
                    run.head_sha === commitSha && 
                    run.head_branch === branchName &&
                    (run.event === 'push' || run.event === 'workflow_dispatch')
                  );
                }
                
                if (qaRun && qaRun.status === 'completed') {
                  qaSuccess = qaRun.conclusion === 'success';
                  break;
                }
                
                await new Promise(resolve => setTimeout(resolve, checkInterval));
              }
              
              if (!qaRun || !qaSuccess) {
                core.error(`❌ Quality Assurance Tests workflow did not succeed for commit ${deployShaShort}`);
                const providedSha = '${{ github.event.inputs.deploy_sha }}';
                if (providedSha && providedSha.trim() !== '') {
                  core.error(`Provided SHA: ${deployShaShort}`);
                  core.error(`Make sure QA tests have passed for this specific commit before deploying.`);
                }
                core.setFailed('❌ QA tests must pass before manual deployment (unless skip flag is used)');
                core.setOutput('qa-success', 'false');
                return;
              }
              
              core.info(`✅ Quality Assurance Tests workflow succeeded for commit ${deployShaShort}`);
              core.setOutput('qa-success', 'true');
            }

  deploy:
    runs-on: ubuntu-latest
    needs: check-prerequisites
    timeout-minutes: 60
    # Only deploy if QA succeeded (or manual dispatch with skip)
    if: |
      (needs.check-prerequisites.outputs.qa-success == 'true') || 
      (github.event_name == 'workflow_dispatch' && needs.check-prerequisites.outputs.skip-checks == 'true')
    steps:
      - name: Use deployment SHA from prerequisites check
        id: determine-sha
        run: |
          # Use the SHA determined in the check-prerequisites job
          # This ensures both jobs use the exact same SHA
          DEPLOY_SHA="${{ needs.check-prerequisites.outputs.deploy-sha }}"
          DEPLOY_SHA_SHORT="${{ needs.check-prerequisites.outputs.deploy-sha-short }}"
          
          if [ -z "$DEPLOY_SHA" ]; then
            echo "❌ ERROR: Deployment SHA not found from prerequisites check"
            echo "This should not happen - check-prerequisites job should have determined the SHA"
            exit 1
          fi
          
          echo "deploy_sha=${DEPLOY_SHA}" >> $GITHUB_OUTPUT
          echo "deploy_sha_short=${DEPLOY_SHA_SHORT}" >> $GITHUB_OUTPUT
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deployment SHA (from prerequisites): ${DEPLOY_SHA}"
          echo "Short SHA: ${DEPLOY_SHA_SHORT}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ steps.determine-sha.outputs.deploy_sha }}
          fetch-depth: 0

      - name: Log deployment SHA for tracking
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "DEPLOYMENT SHA TRACKING"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Event: ${{ github.event_name }}"
          if [ "${{ github.event_name }}" = "workflow_run" ]; then
            echo "QA Workflow Run ID: ${{ github.event.workflow_run.id }}"
            echo "QA Workflow SHA: ${{ github.event.workflow_run.head_sha }}"
            echo "QA Workflow Branch: ${{ github.event.workflow_run.head_branch }}"
          fi
          echo "Determined SHA: ${{ steps.determine-sha.outputs.deploy_sha }}"
          echo "Short SHA: ${{ steps.determine-sha.outputs.deploy_sha_short }}"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"

      - name: Validate SHA exists
        run: |
          DEPLOY_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          CURRENT_SHA=$(git rev-parse HEAD)
          
          # Verify the checked out SHA matches what we intended (or can be resolved)
          if [ "$CURRENT_SHA" != "$DEPLOY_SHA" ]; then
            # Try to resolve the SHA (might be a short SHA or tag)
            RESOLVED_SHA=$(git rev-parse "$DEPLOY_SHA" 2>/dev/null || echo "")
            if [ -z "$RESOLVED_SHA" ]; then
              echo "❌ ERROR: SHA ${DEPLOY_SHA:0:7} does not exist in the repository"
              echo "Please verify the SHA is correct and exists in the repository"
              exit 1
            fi
            # If resolved SHA is different from current, checkout the correct one
            if [ "$RESOLVED_SHA" != "$CURRENT_SHA" ]; then
              echo "Resolved SHA ${DEPLOY_SHA:0:7} to ${RESOLVED_SHA:0:7}, checking out..."
              git checkout "$RESOLVED_SHA"
              CURRENT_SHA=$(git rev-parse HEAD)
            fi
          fi
          
          echo "✓ Validated SHA: ${CURRENT_SHA:0:7}"
          echo "Full SHA: ${CURRENT_SHA}"

      - name: Get previous commit SHA for rollback
        id: previous-commit
        run: |
          # Get the previous commit SHA (parent of current commit)
          # This will be used for rollback if deployment fails
          CURRENT_SHA=$(git rev-parse HEAD)
          
          # Get the parent commit (previous commit)
          PREVIOUS_SHA=$(git rev-parse HEAD^ 2>/dev/null || echo "")
          
          if [ -z "$PREVIOUS_SHA" ]; then
            # If no parent (first commit), try to get from git log
            PREVIOUS_SHA=$(git log --format=%H -n 2 | tail -1 2>/dev/null || echo "")
          fi
          
          if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "$CURRENT_SHA" ]; then
            echo "previous_commit_sha=${PREVIOUS_SHA}" >> $GITHUB_OUTPUT
            echo "Previous commit SHA for rollback: ${PREVIOUS_SHA:0:7}"
          else
            echo "⚠️  Could not determine previous commit SHA (may be first commit)"
            echo "previous_commit_sha=" >> $GITHUB_OUTPUT
          fi

      - name: Load deployment helpers
        run: |
          # Source deployment helper functions for consistent error handling
          source scripts/deploy-helpers.sh
          echo "✓ Deployment helpers loaded"

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.1
        with:
          ssh-private-key: |
            ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add host to known_hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.HOST }} >> ~/.ssh/known_hosts

      - name: Pre-deployment validation (block bad code)
        run: |
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Pre-Deployment Validation"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Blocking bad code before it reaches production..."
          echo ""
          
          VALIDATION_FAILED=false
          FAILURE_REASONS=()
          
          # Validation 1: Check for common silent failure patterns
          echo "1. Checking for critical files..."
          CRITICAL_FILES=(
            "api/weather.php"
            "lib/config.php"
            "lib/rate-limit.php"
            "lib/circuit-breaker.php"
            "lib/constants.php"
            "scripts/sync-push-config.php"
            "scripts/unified-webcam-worker.php"
            "scripts/create-sftp-user.sh"
            "scripts/service-watchdog.sh"
            "lib/push-webcam-validator.php"
            "lib/webcam-worker.php"
            "lib/webcam-acquisition.php"
            "lib/webcam-pipeline.php"
            "pages/config-generator.php"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-compose.prod.yml"
            "docker/Dockerfile"
          )
          
          for file in "${CRITICAL_FILES[@]}"; do
            if [ ! -f "$file" ]; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Critical file missing: $file")
            fi
          done
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ All critical files present"
          fi
          
          # Validation 2: Check for syntax errors that might be missed
          echo ""
          echo "2. Checking PHP syntax..."
          PHP_ERRORS=$(find . -name "*.php" -not -path "./vendor/*" -exec php -l {} \; 2>&1 | grep -v "No syntax errors" || true)
          if [ -n "$PHP_ERRORS" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("PHP syntax errors found")
            echo "❌ PHP syntax errors:"
            echo "$PHP_ERRORS"
          else
            echo "✓ PHP syntax valid"
          fi
          
          # Validation 3: Verify required functions exist
          echo ""
          echo "3. Verifying required functions exist..."
          if ! grep -q "function checkRateLimit" lib/rate-limit.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkRateLimit function missing in lib/rate-limit.php")
          fi
          if ! grep -q "function validateAirportId" lib/config.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("validateAirportId function missing in lib/config.php")
          fi
          if ! grep -q "function checkWeatherCircuitBreaker" lib/circuit-breaker.php 2>/dev/null; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("checkWeatherCircuitBreaker function missing in lib/circuit-breaker.php")
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Required functions present"
          fi
          
          # Validation 4: Check Docker configuration
          echo ""
          echo "4. Validating Docker configuration..."
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("docker-compose.prod.yml missing")
          fi
          if [ ! -f "docker/Dockerfile" ]; then
            VALIDATION_FAILED=true
            FAILURE_REASONS+=("Dockerfile missing")
          fi
          
          # Check Dockerfile has required components
          if [ -f "docker/Dockerfile" ]; then
            if ! grep -q "FROM php:" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing PHP base image")
            fi
            if ! grep -q "fail2ban" docker/Dockerfile; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Dockerfile missing fail2ban installation")
            fi
          fi
          
          if [ "$VALIDATION_FAILED" = "false" ]; then
            echo "✓ Docker configuration valid"
          fi
          
          # Validation 5: Check JSON files are valid
          echo ""
          echo "5. Validating JSON configuration files..."
          if [ -f "config/airports.json.example" ]; then
            if ! php -r "json_decode(file_get_contents('config/airports.json.example'), true); if (json_last_error() !== JSON_ERROR_NONE) { exit(1); }" 2>/dev/null; then
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("config/airports.json.example is invalid JSON")
            else
              echo "✓ config/airports.json.example is valid JSON"
            fi
          fi
          
          # Validation 6: Validate airports.json using master validation script
          echo ""
          echo "6. Validating airports.json (ICAO codes and other checks)..."
          if [ -f "config/airports.json" ]; then
            if php scripts/validate-airports-json.php config/airports.json; then
              echo "✓ All airports.json validations passed"
            else
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("airports.json validation failed - see errors above")
            fi
          else
            echo "⚠ config/airports.json not found - skipping validation"
          fi
          
          # Validation 7: Validate guides (if guides directory exists)
          echo ""
          echo "7. Validating guides..."
          if [ -d "guides" ]; then
            if php scripts/validate-guides.php; then
              echo "✓ Guides validation passed"
            else
              VALIDATION_FAILED=true
              FAILURE_REASONS+=("Guides validation failed - see errors above")
            fi
          else
            echo "⚠ Guides directory not found - skipping validation"
          fi
          
          # Final check
          if [ "$VALIDATION_FAILED" = "true" ]; then
            echo ""
            # Use deployment helper for consistent error formatting
            REASONS_TEXT=$(printf "  ❌ %s\n" "${FAILURE_REASONS[@]}")
            deployment_error \
              "Pre-Deployment Validation" \
              "The following issues were found:\n${REASONS_TEXT}" \
              "1. Review the validation errors above\n2. Fix syntax errors, missing files, or configuration issues\n3. Ensure all required functions exist in their respective files\n4. Verify Docker configuration files are present and valid\n5. This validation prevents silent failures from reaching production"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ Pre-deployment validation passed - code is ready for production"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
      
      - name: Update cache version for cache busting
        run: |
          # Generate a unique version based on commit SHA and timestamp
          # Use the determined deployment SHA
          DEPLOY_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          DEPLOY_VERSION="${DEPLOY_SHA:0:7}-$(date +%s)"
          echo "Updating cache version to: ${DEPLOY_VERSION}"
          
          # Update the cache version in public/js/service-worker.js
          # The script handles missing files and verification
          bash scripts/deploy-update-cache-version.sh "${DEPLOY_VERSION}"
          
          # Note: We don't commit service-worker.js changes during deployment
          # The updated file will be synced to the server via rsync
          # This avoids creating unnecessary commits and deployment loops
          echo "✓ Cache version updated (will be synced to server)"

      - name: Clean up old directory structure on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Clean up leftover docker/docker directory from old path issue (now fixed)
          # This is defensive cleanup - the directory should no longer be created
          if [ -d docker/docker ] || [ -f docker/docker/nginx.conf ]; then
            echo "Cleaning up old docker/docker directory structure..."
            rm -rf docker/docker 2>/dev/null || true
            echo "✓ Cleanup complete"
          else
            echo "✓ No cleanup needed (old directory structure not found)"
          fi
          EOF

      - name: Ensure required tools are installed on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          echo "Checking for required tools on production server..."
          
          # Check and install jq if missing (required for JSON parsing in deployment scripts)
          if ! command -v jq >/dev/null 2>&1; then
            echo "⚠️  jq is not installed. Installing..."
            if command -v apt-get >/dev/null 2>&1; then
              if sudo apt-get update -qq && sudo apt-get install -y jq; then
                # Verify installation succeeded
                if command -v jq >/dev/null 2>&1; then
                  echo "✓ jq installed successfully"
                else
                  echo "❌ ERROR: jq installation failed - command not found after install"
                  exit 1
                fi
              else
                echo "❌ ERROR: Failed to install jq via apt-get"
                exit 1
              fi
            elif command -v yum >/dev/null 2>&1; then
              if sudo yum install -y jq; then
                # Verify installation succeeded
                if command -v jq >/dev/null 2>&1; then
                  echo "✓ jq installed successfully"
                else
                  echo "❌ ERROR: jq installation failed - command not found after install"
                  exit 1
                fi
              else
                echo "❌ ERROR: Failed to install jq via yum"
                exit 1
              fi
            else
              echo "❌ ERROR: Cannot install jq - unsupported package manager"
              echo "Please install jq manually: apt-get install jq (Ubuntu/Debian) or yum install jq (CentOS/RHEL)"
              exit 1
            fi
          else
            echo "✓ jq is already installed"
          fi
          EOF

      - name: Tag current working deployment and save state
        run: |
          # Pass previous commit SHA to server
          PREVIOUS_COMMIT_SHA="${{ steps.previous-commit.outputs.previous_commit_sha }}"
          
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} bash -s << 'EOF' "$PREVIOUS_COMMIT_SHA"
          set -euo pipefail
          PREVIOUS_COMMIT_SHA="$1"
          cd ~/aviationwx
          
          # Use persistent location for rollback state (survives reboots)
          ROLLBACK_STATE_FILE="$HOME/aviationwx-rollback-state.json"
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Saving Rollback State"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Capturing current working state before deployment..."
          echo ""
          
          # Get current image tag (if exists)
          CURRENT_IMAGE=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].Image // "null"' || echo "null")
          
          # Get current commit SHA from container (if available)
          CURRENT_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          # Get current container status
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | jq -r '.[0].State // "unknown"' || echo "unknown")
          
          # Generate rollback tag
          ROLLBACK_TAG="rollback-$(date +%Y%m%d-%H%M%S)"
          
          # Save rollback state
          ROLLBACK_STATE=$(cat << STATE_JSON
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "image": "${CURRENT_IMAGE}",
            "commit_sha": "${CURRENT_SHA}",
            "previous_commit_sha": "${PREVIOUS_COMMIT_SHA}",
            "container_status": "${CONTAINER_STATUS}",
            "rollback_tag": "${ROLLBACK_TAG}"
          }
          STATE_JSON
          )
          
          echo "$ROLLBACK_STATE" > "$ROLLBACK_STATE_FILE"
          
          if [ "$CURRENT_IMAGE" != "null" ] && [ -n "$CURRENT_IMAGE" ]; then
            # Tag current image as rollback target
            echo "Tagging current image for rollback: $ROLLBACK_TAG"
            if docker tag "$CURRENT_IMAGE" "aviationwx:$ROLLBACK_TAG" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
              
              # Keep only last 3 rollback tags (cleanup old ones)
              docker images "aviationwx:rollback-*" --format "{{.Tag}}" 2>/dev/null | sort -r | tail -n +4 | while read tag; do
                echo "Removing old rollback tag: $tag"
                docker rmi "aviationwx:$tag" 2>/dev/null || true
              done
            else
              echo "⚠️  Warning: Failed to tag current image (may not exist yet)"
            fi
          else
            echo "⚠️  Warning: No current image found - cannot create rollback image"
            echo "Rollback will restore files only (no image rollback)"
          fi
          
          echo ""
          echo "Rollback state saved:"
          echo "$ROLLBACK_STATE" | jq '.' 2>/dev/null || echo "$ROLLBACK_STATE"
          echo ""
          echo "✓ Rollback state saved to: $ROLLBACK_STATE_FILE"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF
          # Note: PREVIOUS_COMMIT_SHA_FROM_RUNNER is passed via SSH heredoc substitution

      - name: Ensure SSL certificates are in place (with auto-generation)
        run: |
          # Validate required secrets are present
          if [ -z "${{ secrets.LETSENCRYPT_EMAIL }}" ]; then
            echo "❌ ERROR: LETSENCRYPT_EMAIL secret is required"
            echo "   Add LETSENCRYPT_EMAIL to GitHub repository secrets"
            echo "   See DEPLOYMENT.md for detailed setup instructions"
            exit 1
          fi
          
          if [ -z "${{ secrets.CLOUDFLARE_API_TOKEN }}" ]; then
            echo "❌ ERROR: CLOUDFLARE_API_TOKEN secret is required"
            echo "   Add CLOUDFLARE_API_TOKEN to GitHub repository secrets"
            echo "   See DEPLOYMENT.md for detailed setup instructions"
            exit 1
          fi
          
          # Step 1: Write Cloudflare credentials file via separate SSH command
          # This avoids the pipe+heredoc conflict that was causing the token to not be passed correctly
          # The credentials file is needed by both CI (for bootstrap) and the host's systemd certbot.timer (for renewals)
          echo "Writing Cloudflare credentials to server..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} "mkdir -p ~/.secrets && chmod 700 ~/.secrets"
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} "cat > ~/.secrets/cloudflare.ini && chmod 600 ~/.secrets/cloudflare.ini" << CREDS_EOF
          dns_cloudflare_api_token = ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CREDS_EOF
          echo "✓ Cloudflare credentials written"
          
          # Step 2: Run the main certificate setup script
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'bash -s' -- "${{ secrets.LETSENCRYPT_EMAIL }}" << 'CERT_SCRIPT'
          set -eo pipefail
          cd ~/aviationwx
          # Verify we're running as the correct user
          CURRENT_USER=$(whoami)
          echo "Running as user: $CURRENT_USER"
          
          # Capture email argument
          LETSENCRYPT_EMAIL="${1:-}"
          
          # Re-enable strict mode
          set -euo pipefail
          
          # Validate email is provided and is a valid email format
          if [ -z "${LETSENCRYPT_EMAIL:-}" ] || [ "$LETSENCRYPT_EMAIL" = "" ]; then
            echo "❌ ERROR: Let's Encrypt email not provided"
            echo "   Add LETSENCRYPT_EMAIL to GitHub repository secrets"
            echo "   See: https://github.com/${{ github.repository }}/settings/secrets/actions"
            echo "   See DEPLOYMENT.md for detailed setup instructions"
            exit 1
          fi
          
          # Basic email format validation
          if ! echo "$LETSENCRYPT_EMAIL" | grep -qE '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'; then
            echo "❌ ERROR: Invalid email format: $LETSENCRYPT_EMAIL"
            echo "   Please provide a valid email address for LETSENCRYPT_EMAIL"
            exit 1
          fi
          
          # Verify Cloudflare credentials file exists and has valid content
          CREDENTIALS_FILE="$HOME/.secrets/cloudflare.ini"
          if [ ! -f "$CREDENTIALS_FILE" ]; then
            echo "❌ ERROR: Cloudflare credentials file not found at $CREDENTIALS_FILE"
            exit 1
          fi
          
          # Verify the credentials file has actual token content (not garbage)
          if ! grep -qE '^dns_cloudflare_api_token = [A-Za-z0-9_-]+' "$CREDENTIALS_FILE"; then
            echo "❌ ERROR: Cloudflare credentials file appears invalid"
            echo "   File content does not match expected format"
            exit 1
          fi
          echo "✓ Cloudflare credentials verified"
          
          # Create SSL directory if it doesn't exist
          mkdir -p ssl
          
          # Install certbot if not installed (needed for bootstrap generation)
          if ! command -v certbot >/dev/null 2>&1; then
            echo "Installing certbot and Cloudflare DNS plugin..."
            sudo apt update -qq
            sudo apt install -y certbot python3-certbot-dns-cloudflare
          fi
          
          # Note: Deploy hook installation moved to post-rsync step to ensure latest code is used
          
          # Check if SSL certificates exist in Let's Encrypt directory
          # Note: Certificate RENEWAL is handled by the host's systemd certbot.timer (runs twice daily)
          # This CI step only handles: validation, copying to deployment dir, and bootstrap generation
          if sudo test -f /etc/letsencrypt/live/aviationwx.org/fullchain.pem && sudo test -f /etc/letsencrypt/live/aviationwx.org/privkey.pem; then
            echo "SSL certificates found - checking expiration..."
            
            # Check certificate expiration (informational only - renewal is handled by systemd timer)
            CERT_EXPIRY=$(sudo openssl x509 -in /etc/letsencrypt/live/aviationwx.org/fullchain.pem -noout -enddate 2>/dev/null | cut -d= -f2)
            if [ -n "$CERT_EXPIRY" ]; then
              EXPIRY_EPOCH=$(date -d "$CERT_EXPIRY" +%s 2>/dev/null || echo "")
              if [ -n "$EXPIRY_EPOCH" ]; then
                CURRENT_EPOCH=$(date +%s)
                DAYS_UNTIL_EXPIRY=$(( ($EXPIRY_EPOCH - $CURRENT_EPOCH) / 86400 ))
                
                if [ "$DAYS_UNTIL_EXPIRY" -lt 0 ]; then
                  echo "❌ ERROR: Certificate expired $((-DAYS_UNTIL_EXPIRY)) days ago"
                  echo "   The host's systemd certbot.timer should have renewed this automatically."
                  echo "   Please check: sudo systemctl status certbot.timer"
                  echo "   And logs: sudo journalctl -u certbot.service"
                  rm -f "$CREDENTIALS_FILE"
                  exit 1
                elif [ "$DAYS_UNTIL_EXPIRY" -lt 7 ]; then
                  echo "⚠️  Certificate expires in $DAYS_UNTIL_EXPIRY days"
                  echo "   The host's systemd certbot.timer will renew automatically (threshold: 30 days)"
                  echo "   If concerned, check: sudo systemctl status certbot.timer"
                else
                  echo "✓ Certificate valid for $DAYS_UNTIL_EXPIRY more days"
                fi
              else
                echo "⚠️  Could not parse certificate expiration date"
              fi
            fi
            
            echo "Copying SSL certificates from Let's Encrypt..."
            # Copy certificates (requires sudo to read from /etc/letsencrypt/)
            if sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ssl/; then
              if sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ssl/; then
                # Set correct ownership and permissions
                sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/
                sudo chmod 644 ssl/fullchain.pem
                sudo chmod 600 ssl/privkey.pem
                echo "✓ SSL certificates copied successfully"
              else
                echo "⚠️  Failed to copy privkey.pem"
                echo "This may indicate permission issues or missing source file"
                exit 1
              fi
            else
              echo "⚠️  Failed to copy fullchain.pem"
              echo "This may indicate permission issues or missing source file"
              exit 1
            fi
          elif [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates already exist in ~/aviationwx/ssl/"
            # Verify permissions are correct
            sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/ || true
            sudo chmod 644 ssl/fullchain.pem || true
            sudo chmod 600 ssl/privkey.pem || true
          else
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  Wildcard certificate not found - bootstrapping with automatic generation"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            
            # Generate wildcard certificate (bootstrap only - renewals handled by systemd timer)
            echo "Generating wildcard certificate for aviationwx.org and *.aviationwx.org..."
            echo "Using DNS-01 challenge with Cloudflare API..."
            echo ""
            
            if sudo certbot certonly \
              --dns-cloudflare \
              --dns-cloudflare-credentials "$CREDENTIALS_FILE" \
              -d aviationwx.org \
              -d '*.aviationwx.org' \
              --non-interactive \
              --agree-tos \
              --email "$LETSENCRYPT_EMAIL" \
              --no-eff-email; then
              echo ""
              echo "✓ Wildcard certificate generated successfully"
              
              # Copy to deployment directory
              sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ssl/
              sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ssl/
              sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/
              chmod 644 ssl/fullchain.pem
              chmod 600 ssl/privkey.pem
              
              echo "✓ Certificate copied to deployment directory"
              echo ""
              echo "Note: Future renewals will be handled automatically by the host's"
              echo "      systemd certbot.timer (runs twice daily, renews at 30 days before expiry)"
              
              # Show certificate details
              echo ""
              echo "Certificate details:"
              sudo openssl x509 -in /etc/letsencrypt/live/aviationwx.org/fullchain.pem -noout -subject -dates
            else
              echo ""
              echo "❌ ERROR: Failed to generate wildcard certificate"
              echo ""
              echo "Possible causes:"
              echo "  - Invalid Cloudflare API token"
              echo "  - DNS not properly configured (check A records for @ and *)"
              echo "  - Let's Encrypt rate limits (max 50 certs/week per domain)"
              echo "  - Network connectivity issues"
              echo "  - Cloudflare API permissions insufficient"
              echo ""
              echo "Troubleshooting:"
              echo "  1. Verify Cloudflare API token has correct permissions:"
              echo "     - Zone → DNS → Edit"
              echo "     - Zone → Zone → Read"
              echo "     - Scoped to: aviationwx.org zone"
              echo ""
              echo "  2. Verify DNS is configured:"
              echo "     dig aviationwx.org +short"
              echo "     dig '*.aviationwx.org' +short"
              echo ""
              echo "  3. Test certificate generation manually:"
              echo "     sudo certbot certonly --dns-cloudflare \\"
              echo "       --dns-cloudflare-credentials ~/.secrets/cloudflare.ini \\"
              echo "       -d aviationwx.org -d '*.aviationwx.org' \\"
              echo "       --non-interactive --agree-tos -m $LETSENCRYPT_EMAIL \\"
              echo "       --dry-run"
              echo ""
              
              # Clean up credentials file
              rm -f "$CREDENTIALS_FILE"
              exit 1
            fi
            
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          fi
          
          # Check/bootstrap upload.aviationwx.org certificate (used for FTPS uploads)
          # This is a separate certificate from the wildcard, using DNS-01 authentication
          echo ""
          echo "Checking upload.aviationwx.org certificate..."
          if sudo test -f /etc/letsencrypt/live/upload.aviationwx.org/fullchain.pem; then
            echo "✓ upload.aviationwx.org certificate exists"
            
            # Check if it's using DNS-01 authenticator (not webroot)
            if sudo grep -q "authenticator = dns-cloudflare" /etc/letsencrypt/renewal/upload.aviationwx.org.conf 2>/dev/null; then
              echo "✓ upload.aviationwx.org using DNS-01 authentication"
            else
              echo "⚠️  upload.aviationwx.org not using DNS-01 - reconfiguring..."
              # Reconfigure to use DNS-01 instead of webroot
              if sudo certbot certonly \
                --dns-cloudflare \
                --dns-cloudflare-credentials "$CREDENTIALS_FILE" \
                -d upload.aviationwx.org \
                --cert-name upload.aviationwx.org \
                --non-interactive \
                --agree-tos \
                --email "$LETSENCRYPT_EMAIL" \
                --no-eff-email \
                --force-renewal; then
                echo "✓ upload.aviationwx.org reconfigured to DNS-01"
              else
                echo "⚠️  Failed to reconfigure upload.aviationwx.org - will retry on next deployment"
              fi
            fi
          else
            echo "upload.aviationwx.org certificate not found - bootstrapping..."
            if sudo certbot certonly \
              --dns-cloudflare \
              --dns-cloudflare-credentials "$CREDENTIALS_FILE" \
              -d upload.aviationwx.org \
              --cert-name upload.aviationwx.org \
              --non-interactive \
              --agree-tos \
              --email "$LETSENCRYPT_EMAIL" \
              --no-eff-email; then
              echo "✓ upload.aviationwx.org certificate generated"
            else
              echo "⚠️  Failed to generate upload.aviationwx.org certificate"
              echo "   This is non-critical - FTPS can use the wildcard cert"
            fi
          fi
          
          # Note: We intentionally leave the credentials file in place (~/.secrets/cloudflare.ini)
          # because the host's systemd certbot.timer needs it for automatic renewals
          echo "✓ Cloudflare credentials available for automatic renewals at ~/.secrets/cloudflare.ini"
          CERT_SCRIPT

      - name: Configure firewall ports
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Configuring firewall ports for all services..."
          
          # Run firewall configuration script
          if [ -f scripts/deploy-configure-firewall.sh ]; then
            chmod +x scripts/deploy-configure-firewall.sh
            ./scripts/deploy-configure-firewall.sh
          else
            echo "⚠️  Firewall script not found, configuring ports manually..."
            # Fallback: configure ports directly (should match deploy-configure-firewall.sh)
            sudo ufw allow 80/tcp comment 'HTTP (Nginx)' || true
            sudo ufw allow 443/tcp comment 'HTTPS (Nginx)' || true
            sudo ufw allow 2121/tcp comment 'FTP (Push webcams)' || true
            sudo ufw allow 2122/tcp comment 'FTPS (Push webcams)' || true
            sudo ufw allow 2222/tcp comment 'SFTP (Push webcams)' || true
            sudo ufw allow 50000:51000/tcp comment 'FTP passive mode (Push webcams)' || true
            sudo ufw allow 22/tcp comment 'SSH (System access)' || true
            sudo ufw status numbered
          fi
          EOF

      - name: Ensure cache directory exists
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          # Get www-data UID/GID from container (typically 33:33)
          # If container is running, get actual UID/GID from it
          WWW_DATA_UID=33
          WWW_DATA_GID=33
          if docker ps --format '{{.Names}}' | grep -q '^aviationwx-web$'; then
            # Container is running - get actual UID/GID
            CONTAINER_UID=$(docker exec aviationwx-web id -u www-data 2>/dev/null || echo "33")
            CONTAINER_GID=$(docker exec aviationwx-web id -g www-data 2>/dev/null || echo "33")
            WWW_DATA_UID=${CONTAINER_UID}
            WWW_DATA_GID=${CONTAINER_GID}
            echo "Detected www-data UID/GID from container: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          else
            echo "Container not running, using default www-data UID/GID: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          fi
          
          # Check if parent directory exists and who owns it
          if [ -d /tmp/aviationwx-cache ]; then
            CURRENT_OWNER=$(stat -c '%U:%G' /tmp/aviationwx-cache 2>/dev/null || stat -f '%Su:%Sg' /tmp/aviationwx-cache 2>/dev/null || echo "unknown")
            echo "Existing cache directory owner: ${CURRENT_OWNER}"
          fi
          
          # Try to create directory structure without sudo first
          if mkdir -p /tmp/aviationwx-cache/webcams 2>/dev/null; then
            echo "✓ Created cache directory without sudo"
          else
            echo "⚠️  Failed to create directory without sudo, trying with sudo..."
            # Use sudo to create directory structure
            sudo mkdir -p /tmp/aviationwx-cache/webcams || {
              echo "❌ Failed to create cache directory even with sudo"
              exit 1
            }
            echo "✓ Created cache directory with sudo"
          fi
          
          # Set ownership to match container's www-data user
          # Try without sudo first, then with sudo if needed
          if chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache 2>/dev/null; then
            echo "✓ Changed ownership without sudo"
          else
            echo "⚠️  Failed to change ownership without sudo, trying with sudo..."
            sudo chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache || {
              echo "⚠️  Failed to change ownership with sudo, using fallback permissions"
              # Fallback: make world-writable (less secure but works)
              sudo chmod -R 777 /tmp/aviationwx-cache || true
            }
          fi
          
          # Ensure directory permissions allow www-data to write
          # Use 775 for webcams (group writable) and 755 for parent
          sudo chmod 755 /tmp/aviationwx-cache 2>/dev/null || chmod 755 /tmp/aviationwx-cache || true
            sudo chmod 775 /tmp/aviationwx-cache/webcams 2>/dev/null || chmod 775 /tmp/aviationwx-cache/webcams || true
          
          # Verify directory was created and is writable
          if [ ! -d /tmp/aviationwx-cache/webcams ]; then
            echo "❌ Failed to create cache directory"
            echo "This is critical - webcam caching will not work without this directory"
            echo "Check permissions and disk space, then retry deployment"
            exit 1
          fi
          
          # Show final permissions
          echo "Final directory permissions:"
          ls -ld /tmp/aviationwx-cache
          ls -ld /tmp/aviationwx-cache/webcams
          
          # Test write access (as current user, which should work if permissions are correct)
          if touch /tmp/aviationwx-cache/webcams/.test_write 2>/dev/null; then
            rm -f /tmp/aviationwx-cache/webcams/.test_write
            echo "✓ Cache directory is writable by deployment user"
          else
            echo "⚠️  Cache directory may not be writable by deployment user"
            echo "   (This is OK if container user can write)"
          fi
          
          # Final verification
          echo "✓ Cache directory created at /tmp/aviationwx-cache/webcams"
          echo "✓ Ownership set to ${WWW_DATA_UID}:${WWW_DATA_GID} (www-data)"
          EOF

      - name: Sync repository to server
        run: |
          echo "Syncing repository to server..."
          # Exclude docker/docker from rsync (defensive - should not exist after path fix)
          # Exclude uploads directory (ephemeral, created inside container)
          # Exclude ssl directory (contains SSL certificates copied from Let's Encrypt, not in git)
          if ! rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' --exclude 'ssl' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/; then
            deployment_error \
              "File Synchronization (Rsync)" \
              "Rsync failed - files may not be synced correctly. This is critical - deployment cannot proceed without files being synced." \
              "1. Check network connectivity: ping ${{ secrets.HOST }}\n2. Verify SSH connection: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'echo Connection OK'\n3. Check server disk space: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'df -h'\n4. Verify permissions on server: ssh ${{ secrets.USER }}@${{ secrets.HOST }} 'ls -la ~/aviationwx'\n5. Check rsync logs for specific errors\n6. Retry deployment after resolving issues"
          fi
          echo "✓ Repository synced successfully"

      - name: Verify and restore SSL certificates after rsync
        run: |
          # Verify SSL certificates are still present after rsync
          # (rsync --delete could remove them if ssl/ wasn't excluded)
          echo ""
          echo "Verifying SSL certificates are still present after rsync..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          # Check if certificates exist
          if [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates verified after rsync"
          else
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  WARNING: SSL certificates missing after rsync"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Rsync may have deleted the ssl/ directory. Restoring certificates..."
            
            # Use helper script to restore certificates
            if [ -f scripts/restore-ssl-certificates.sh ]; then
              chmod +x scripts/restore-ssl-certificates.sh
              ./scripts/restore-ssl-certificates.sh
            else
              # Source deployment helper if available
              if [ -f scripts/deploy-helpers.sh ]; then
                source scripts/deploy-helpers.sh
                deployment_error \
                  "SSL Certificate Restoration" \
                  "restore-ssl-certificates.sh script not found. This should not happen - script should be synced via rsync." \
                  "1. Verify script exists: ls -la scripts/restore-ssl-certificates.sh\n2. Check if rsync excluded the scripts directory\n3. Verify file permissions\n4. Manually restore certificates if needed: sudo cp /etc/letsencrypt/live/aviationwx.org/*.pem ssl/"
              else
                echo "❌ ERROR: restore-ssl-certificates.sh script not found"
                echo "This should not happen - script should be synced via rsync"
                exit 1
              fi
            fi
          fi
          
          # Install/update certbot deploy hook (runs AFTER rsync to get latest version)
          # This hook is called by the host's systemd certbot.timer after successful renewal
          DEPLOY_HOOK_SRC="scripts/certbot-deploy-hook.sh"
          DEPLOY_HOOK_DST="/etc/letsencrypt/renewal-hooks/deploy/aviationwx-deploy-hook.sh"
          if [ -f "$DEPLOY_HOOK_SRC" ]; then
            echo "Installing certbot deploy hook from synced code..."
            sudo cp "$DEPLOY_HOOK_SRC" "$DEPLOY_HOOK_DST"
            sudo chmod +x "$DEPLOY_HOOK_DST"
            echo "✓ Certbot deploy hook installed/updated"
          else
            echo "⚠️  Deploy hook script not found at $DEPLOY_HOOK_SRC"
          fi
          EOF

      - name: Deploy host-level cron and logrotate configs
        run: |
          echo "Deploying host-level cron and logrotate configurations..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deploying weekly Docker cleanup cron job"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Make the cleanup script executable
          if [ -f scripts/docker-cleanup-weekly.sh ]; then
            chmod +x scripts/docker-cleanup-weekly.sh
            echo "✓ Made docker-cleanup-weekly.sh executable"
          else
            echo "⚠️  scripts/docker-cleanup-weekly.sh not found (will be created on next rsync)"
          fi
          
          # Deploy cron.d file (weekly Docker cleanup - Sundays at 2 AM)
          if [ -f config/host-cron.d/aviationwx-docker-cleanup ]; then
            sudo cp config/host-cron.d/aviationwx-docker-cleanup /etc/cron.d/aviationwx-docker-cleanup
            sudo chown root:root /etc/cron.d/aviationwx-docker-cleanup
            sudo chmod 644 /etc/cron.d/aviationwx-docker-cleanup
            echo "✓ Deployed /etc/cron.d/aviationwx-docker-cleanup (weekly cleanup at Sunday 2 AM UTC)"
          else
            echo "⚠️  config/host-cron.d/aviationwx-docker-cleanup not found (will be deployed on next rsync)"
          fi
          
          # Deploy logrotate config for cleanup log
          if [ -f config/host-logrotate.d/docker-cleanup-weekly ]; then
            sudo cp config/host-logrotate.d/docker-cleanup-weekly /etc/logrotate.d/docker-cleanup-weekly
            sudo chown root:root /etc/logrotate.d/docker-cleanup-weekly
            sudo chmod 644 /etc/logrotate.d/docker-cleanup-weekly
            echo "✓ Deployed /etc/logrotate.d/docker-cleanup-weekly (4 week retention)"
          else
            echo "⚠️  config/host-logrotate.d/docker-cleanup-weekly not found (will be deployed on next rsync)"
          fi
          
          # Verify cron syntax is valid
          if [ -f /etc/cron.d/aviationwx-docker-cleanup ]; then
            # cron.d files are automatically parsed by crond, but we can validate basic syntax
            if grep -qE '^[0-9*,/-]+ [0-9*,/-]+ [0-9*,/-]+ [0-9*,/-]+ [0-9*,/-]+ ' /etc/cron.d/aviationwx-docker-cleanup; then
              echo "✓ Cron syntax validation passed"
            else
              echo "⚠️  Cron file may have syntax issues - please verify manually"
            fi
          fi
          
          echo ""
          echo "Weekly Docker cleanup configured:"
          echo "  • Script: /home/aviationwx/aviationwx/scripts/docker-cleanup-weekly.sh"
          echo "  • Schedule: Sunday 2:00 AM UTC"
          echo "  • Log: /var/log/docker-cleanup-weekly.log"
          echo "  • Retention: 4 weeks of logs"
          EOF

      - name: Deploy host-level fail2ban configuration
        run: |
          echo "Deploying host-level fail2ban configuration for SSH protection..."
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Deploying host-level fail2ban (strict SSH protection on port 22)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Check if fail2ban is installed on host
          if ! command -v fail2ban-client &> /dev/null; then
            echo "Installing fail2ban on host..."
            sudo apt-get update
            sudo apt-get install -y fail2ban
            echo "✓ fail2ban installed"
          else
            echo "✓ fail2ban already installed"
          fi
          
          # Deploy host-level fail2ban configuration (strict SSH protection)
          if [ -f config/host-fail2ban.d/aviationwx-sshd.conf ]; then
            sudo cp config/host-fail2ban.d/aviationwx-sshd.conf /etc/fail2ban/jail.d/aviationwx-sshd.conf
            sudo chown root:root /etc/fail2ban/jail.d/aviationwx-sshd.conf
            sudo chmod 644 /etc/fail2ban/jail.d/aviationwx-sshd.conf
            echo "✓ Deployed /etc/fail2ban/jail.d/aviationwx-sshd.conf"
          else
            echo "⚠️  config/host-fail2ban.d/aviationwx-sshd.conf not found (will be deployed on next rsync)"
          fi
          
          # Restart fail2ban to pick up config changes
          if [ -f /etc/fail2ban/jail.d/aviationwx-sshd.conf ]; then
            sudo systemctl restart fail2ban
            echo "✓ fail2ban restarted"
            
            # Wait for fail2ban to start
            sleep 2
            
            # Check status
            sudo fail2ban-client status || echo "⚠️  fail2ban status check failed"
            echo ""
            echo "✓ Host-level fail2ban status:"
            sudo fail2ban-client status sshd || echo "  sshd jail not yet active (will activate on first auth.log entry)"
          fi
          
          echo ""
          echo "Host-level fail2ban configured:"
          echo "  • Service: SSH on port 22 (host management)"
          echo "  • Policy: 5 failures in 10 minutes = 7 day ban"
          echo "  • Separate from container fail2ban (forgiving camera upload policies)"
          echo "  • Config: /etc/fail2ban/jail.d/aviationwx-sshd.conf"
          EOF

      - name: Check for Docker config changes
        id: docker-config-changed
        run: |
          echo "Checking for Docker configuration file changes..."
          
          # Get the previous commit SHA (if available)
          # Note: github.event.before is only available for push events
          # For workflow_run/workflow_dispatch, we'll use git to find the parent
          PREV_SHA=""
          if [ "${{ github.event_name }}" = "push" ]; then
            PREV_SHA="${{ github.event.before }}"
          else
            # For workflow_run/workflow_dispatch, try to get parent from git
            # This will be set if we have the previous commit info
            PREV_SHA="${{ steps.previous-commit.outputs.previous_commit_sha }}"
          fi
          CURRENT_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          
          # Docker config files that require image rebuild
          DOCKER_CONFIG_FILES=(
            "docker/Dockerfile"
            "docker/vsftpd.conf"
            "docker/sshd_config"
            "docker/docker-entrypoint.sh"
            "docker/pam-vsftpd"
            "docker/logrotate-vsftpd"
            "docker/logrotate-sshd"
            "docker/fail2ban-jail.conf"
            "docker/fail2ban-vsftpd.conf"
            "docker/fail2ban-sshd.conf"
            "docker/docker-compose.prod.yml"
            "scripts/service-watchdog.sh"
            "scripts/setup-letsencrypt.sh"
            "scripts/certbot-deploy-hook.sh"
            "scripts/enable-vsftpd-ssl.sh"
            "scripts/create-sftp-user.sh"
            "config/crontab"
          )
          
          CONFIG_CHANGED="false"
          
          # If we have a previous commit, check what changed
          if [ -n "$PREV_SHA" ] && [ "$PREV_SHA" != "0000000000000000000000000000000000000000" ]; then
            echo "Comparing changes from $PREV_SHA to $CURRENT_SHA..."
            for file in "${DOCKER_CONFIG_FILES[@]}"; do
              if git diff --name-only "$PREV_SHA" "$CURRENT_SHA" | grep -q "^${file}$"; then
                echo "  ✓ $file changed"
                CONFIG_CHANGED="true"
              fi
            done
          else
            # No previous commit (initial deploy or can't determine), assume config might have changed
            echo "⚠️  Cannot determine previous commit, will check for config changes on server"
            CONFIG_CHANGED="unknown"
          fi
          
          if [ "$CONFIG_CHANGED" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "⚠️  Docker configuration files changed - will force rebuild"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          elif [ "$CONFIG_CHANGED" = "unknown" ]; then
            echo "⚠️  Will verify config changes on server and rebuild if needed"
          else
            echo "✓ No Docker configuration file changes detected"
          fi
          
          echo "config_changed=$CONFIG_CHANGED" >> $GITHUB_OUTPUT

      - name: Deploy environment configuration
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'ENVEOF'
          set -euo pipefail
          
          echo "Creating production environment configuration..."
          
          # Create .env.production file with all environment variables
          cat > /home/aviationwx/.env.production << 'INNEREOF'
# AviationWX Production Environment Configuration
# Generated by GitHub Actions deployment workflow
# Last updated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')

# Core application settings
DOMAIN=aviationwx.org
APP_ENV=production
CONFIG_PATH=/var/www/html/config/airports.json

# Refresh intervals
WEBCAM_REFRESH_DEFAULT=60
WEATHER_REFRESH_DEFAULT=60
WEATHER_REFRESH_URL=http://localhost:8080

# Git release tracking
GIT_SHA=${GIT_SHA}

# Sentry error tracking and performance monitoring
SENTRY_DSN=${{ secrets.SENTRY_DSN }}
SENTRY_ENVIRONMENT=production
SENTRY_SAMPLE_RATE_ERRORS=1.0
SENTRY_SAMPLE_RATE_TRACES=0.05
SENTRY_RELEASE=${GIT_SHA}
INNEREOF
          
          # Secure the file (only owner can read)
          chmod 600 /home/aviationwx/.env.production
          
          echo "✓ Environment configuration deployed"
          echo "  File: /home/aviationwx/.env.production"
          echo "  Permissions: 0600 (owner read/write only)"
          ENVEOF

      - name: Deploy via Docker Compose
        run: |
          # Get commit SHA from determined deployment SHA (7 characters to match GitHub's short SHA display)
          # Note: .git directory is excluded during rsync, so we can't get SHA from server's git repo
          GIT_SHA="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          echo "Deploying with GIT_SHA: ${GIT_SHA}"
          
          # Determine if we need to force rebuild
          FORCE_REBUILD="${{ steps.docker-config-changed.outputs.config_changed }}"
          
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          # Set GIT_SHA as environment variable for docker compose
          export GIT_SHA="$GIT_SHA"
          # Enable BuildKit for better caching (if not already enabled)
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Check for Docker config changes on server (fallback if git diff didn't work)
          if [ "$FORCE_REBUILD" = "unknown" ] || [ "$FORCE_REBUILD" = "false" ]; then
            echo "Verifying Docker config files on server..."
            CONFIG_CHANGED="false"
            
            # Docker config files that require image rebuild
            DOCKER_CONFIG_FILES=(
              "docker/Dockerfile"
              "docker/vsftpd.conf"
              "docker/sshd_config"
              "docker/docker-entrypoint.sh"
              "docker/pam-vsftpd"
              "docker/logrotate-vsftpd"
              "docker/logrotate-sshd"
              "docker/fail2ban-jail.conf"
              "docker/fail2ban-vsftpd.conf"
              "docker/fail2ban-sshd.conf"
              "docker/docker-compose.prod.yml"
              "scripts/service-watchdog.sh"
              "scripts/setup-letsencrypt.sh"
              "scripts/certbot-deploy-hook.sh"
              "scripts/enable-vsftpd-ssl.sh"
              "scripts/create-sftp-user.sh"
              "config/crontab"
            )
            
            # Check if any config file was modified more recently than the Docker image
            if docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1 | grep -q .; then
              IMAGE_TIME=\$(docker images aviationwx:latest --format "{{.CreatedAt}}" 2>/dev/null | head -1)
              IMAGE_TIMESTAMP=\$(date -d "\$IMAGE_TIME" +%s 2>/dev/null || echo 0)
              
              for file in "\${DOCKER_CONFIG_FILES[@]}"; do
                if [ -f "\$file" ]; then
                  FILE_TIMESTAMP=\$(stat -c %Y "\$file" 2>/dev/null || stat -f %m "\$file" 2>/dev/null || echo 0)
                  if [ "\$FILE_TIMESTAMP" -gt "\$IMAGE_TIMESTAMP" ]; then
                    echo "  ✓ \$file is newer than Docker image"
                    CONFIG_CHANGED="true"
                  fi
                fi
              done
            else
              # No image exists, must rebuild
              CONFIG_CHANGED="true"
            fi
            
            if [ "\$CONFIG_CHANGED" = "true" ]; then
              FORCE_REBUILD="true"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "⚠️  Docker configuration files changed - will force rebuild"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            fi
          fi
          
          # Build arguments for cache invalidation
          if [ "$FORCE_REBUILD" = "true" ]; then
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Forcing rebuild without cache due to config changes..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            # Force rebuild without cache
            if ! docker compose -f docker/docker-compose.prod.yml build --no-cache web; then
              deployment_error \
                "Docker Build (No Cache)" \
                "Docker build failed when forcing rebuild without cache" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config\n2. Check disk space: df -h\n3. Check Docker daemon: systemctl status docker\n4. Review build logs above for specific errors\n5. Try manual build: docker compose -f docker/docker-compose.prod.yml build --no-cache web"
            fi
            # Start containers with the newly built image
            if ! docker compose -f docker/docker-compose.prod.yml up -d --pull; then
              echo "⚠️  Warning: Start with --pull failed, trying without --pull..."
              if ! docker compose -f docker/docker-compose.prod.yml up -d; then
                deployment_error \
                  "Container Startup" \
                  "Failed to start containers after build" \
                  "1. Check container logs: docker compose -f docker/docker-compose.prod.yml logs\n2. Verify container status: docker compose -f docker/docker-compose.prod.yml ps\n3. Check port conflicts: netstat -tuln | grep -E ':(80|443|2121|2222)'\n4. Review docker-compose.prod.yml configuration\n5. Check Docker daemon logs: journalctl -u docker"
              fi
            fi
            echo "✓ Containers rebuilt and started successfully (no cache)"
          elif ! docker compose -f docker/docker-compose.prod.yml up -d --build --pull; then
            echo "⚠️  Warning: Build with --pull failed, trying without --pull..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d --build; then
              deployment_error \
                "Docker Build/Start" \
                "Both build attempts (with --pull and without) failed" \
                "1. Check Dockerfile syntax: docker compose -f docker/docker-compose.prod.yml config\n2. Verify all required files are present (check Dockerfile COPY commands)\n3. Check network connectivity for pulling base images: ping registry-1.docker.io\n4. Check disk space: df -h\n5. Check Docker daemon: systemctl status docker\n6. Review Docker logs: journalctl -u docker\n7. Try manual build: docker compose -f docker/docker-compose.prod.yml build web"
            fi
          fi
          
          echo "✓ Containers built and started successfully"
          
          # Wait for web container to be ready (needed for certificate generation)
          # Use polling instead of fixed sleep for faster startup detection
          echo "Waiting for web container to be ready..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type http \
              --timeout 30 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml \
              --url http://localhost:8080/; then
              echo "✓ Web container is ready and responding"
            else
              echo "⚠️  Warning: Web container did not become ready within timeout"
              echo "   Continuing anyway - certificate generation may fail"
            fi
          else
            # Fallback to simple status check if script not available
            echo "⚠️  Health check script not found, using fallback check..."
            sleep 5
            if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              echo "⚠️  Warning: Web container is not running, skipping certificate generation"
            fi
          fi
          
          # FTPS uses wildcard certificate (*.aviationwx.org) which is already managed
          # The wildcard certificate covers upload.aviationwx.org, so no separate certificate generation needed
          # Container entrypoint will automatically enable SSL if wildcard certificate exists
          echo "✓ FTPS will use wildcard certificate (*.aviationwx.org) which covers upload.aviationwx.org"
          echo "  Certificate is managed separately and should already exist at /etc/letsencrypt/live/aviationwx.org/"
          
          # Sync FTP/SFTP/FTPS configuration (runs on every deployment)
          # This ensures users and directories are created/updated immediately after deployment
          echo "Syncing FTP/SFTP/FTPS configuration..."
          if docker compose -f docker/docker-compose.prod.yml exec -T web /usr/local/bin/php /var/www/html/scripts/sync-push-config.php 2>&1; then
            echo "✓ FTP/SFTP/FTPS configuration synced successfully"
          else
            echo "⚠️  Warning: FTP/SFTP/FTPS configuration sync failed"
            echo "  This may be due to:"
            echo "    - Config file not yet available"
            echo "    - Permissions issues"
            echo "    - Database corruption (will be auto-recovered on next run)"
            echo ""
            echo "  Deployment will continue, but FTP/SFTP users may not be configured."
            echo "  The sync will retry on container startup."
            # Don't fail deployment - just warn
          fi
          
          # Clean up Docker resources conditionally (only if disk usage is high)
          # Check disk usage before cleanup to avoid unnecessary operations
          DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')
          DISK_THRESHOLD=40  # Cleanup if disk usage > 40%
          
          echo "Checking disk usage before cleanup..."
          echo "Current disk usage: ${DISK_USAGE}% (threshold: ${DISK_THRESHOLD}%)"
          
          if [ "$DISK_USAGE" -gt "$DISK_THRESHOLD" ]; then
            echo "Disk usage is above threshold - running Docker cleanup..."
            # Clean up Docker resources using the dedicated cleanup script
            # This modularizes cleanup logic and keeps the workflow concise
            if [ -f scripts/deploy-docker-cleanup.sh ]; then
              echo "Running Docker cleanup script..."
              chmod +x scripts/deploy-docker-cleanup.sh
              ./scripts/deploy-docker-cleanup.sh
            else
              echo "⚠️  Cleanup script not found, running basic cleanup..."
              docker builder prune -f --filter "until=24h" || true
              docker image prune -f || true
              docker system prune -f --filter "until=168h" || true
              docker system df
            fi
          else
            echo "Disk usage is below threshold - skipping cleanup to save time"
            echo "Current Docker disk usage:"
            docker system df 2>/dev/null || echo "Could not retrieve Docker disk usage"
          fi
          EOF

      - name: Restart Nginx container to pick up config changes
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "Restarting Nginx container..."
          # Restart Nginx container to ensure it picks up new nginx.conf
          # This is safer than reload since config file is mounted as volume
          if ! docker compose -f docker/docker-compose.prod.yml restart nginx; then
            echo "Restart failed, trying up -d..."
            if ! docker compose -f docker/docker-compose.prod.yml up -d nginx; then
              if [ "$(type -t deployment_error)" = "function" ]; then
                NGINX_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50 || echo "Could not retrieve logs")
                deployment_error \
                  "Nginx Container Startup" \
                  "Failed to start Nginx container after restart attempt" \
                  "1. Check Nginx container logs:\n   ${NGINX_LOGS}\n2. Verify Nginx configuration: docker compose -f docker/docker-compose.prod.yml exec nginx nginx -t\n3. Check for port conflicts: netstat -tuln | grep -E ':(80|443)'\n4. Verify docker-compose.prod.yml nginx service configuration\n5. Check container status: docker compose -f docker/docker-compose.prod.yml ps nginx"
              else
                echo ""
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "❌ ERROR: Failed to start Nginx container"
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo ""
                echo "Container logs:"
                docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
                exit 1
              fi
            fi
          fi
          
          # Wait for Nginx to be ready (polling instead of fixed sleep)
          echo "Waiting for Nginx to be ready..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            scripts/wait-for-container-health.sh \
              --container nginx \
              --check-type status \
              --timeout 15 \
              --interval 1 \
              --compose-file docker/docker-compose.prod.yml || true
          else
            # Fallback to short sleep if script not available
            sleep 2
          fi
          
          echo "Testing Nginx configuration..."
          # Verify Nginx configuration is valid
          CONFIG_TEST_OUTPUT=$(docker compose -f docker/docker-compose.prod.yml exec -T nginx nginx -t 2>&1 || echo "FAILED")
          
          if echo "$CONFIG_TEST_OUTPUT" | grep -qE "syntax is ok|test is successful"; then
            echo "✓ Nginx configuration is valid"
          else
            if [ "$(type -t deployment_error)" = "function" ]; then
              NGINX_LOGS=$(docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50 || echo "Could not retrieve logs")
              deployment_error \
                "Nginx Configuration Validation" \
                "Nginx configuration test failed - configuration is invalid" \
                "1. Review Nginx config test output:\n   ${CONFIG_TEST_OUTPUT}\n2. Check container logs:\n   ${NGINX_LOGS}\n3. Check nginx.conf syntax: docker compose -f docker/docker-compose.prod.yml exec nginx nginx -t\n4. Verify all referenced files exist\n5. Check for missing semicolons, brackets, or typos\n6. Review docker/nginx.conf and docker/nginx-main.conf\n7. Fix configuration errors and retry deployment"
            else
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: Nginx configuration is invalid"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "Nginx config test output:"
              echo "$CONFIG_TEST_OUTPUT"
              echo ""
              echo "Container logs:"
              docker compose -f docker/docker-compose.prod.yml logs nginx | tail -50
              echo ""
              echo "Fix the Nginx configuration and retry deployment."
              exit 1
            fi
          fi
          
          # Verify Nginx is responding (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Nginx not responding on port 80 (may be expected if SSL only)"
          else
            echo "✓ Nginx is responding"
          fi
          
          # Verify SSL certificates are accessible inside nginx container
          echo ""
          echo "Verifying SSL certificates are accessible in nginx container..."
          if docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/fullchain.pem && \
             docker compose -f docker/docker-compose.prod.yml exec -T nginx test -f /etc/nginx/ssl/privkey.pem; then
            echo "✓ SSL certificates are accessible in nginx container"
          else
            if [ "$(type -t deployment_error)" = "function" ]; then
              HOST_SSL=$(ls -la ssl/ 2>/dev/null || echo "ssl/ directory not found on host")
              CONTAINER_MOUNTS=$(docker inspect aviationwx-nginx 2>/dev/null | grep -A 20 Mounts || echo "Could not inspect container")
              deployment_error \
                "SSL Certificate Accessibility" \
                "SSL certificates not accessible in nginx container. This indicates a volume mount problem." \
                "1. Check host-side certificates:\n   ${HOST_SSL}\n2. Check container volume mounts:\n   ${CONTAINER_MOUNTS}\n3. Verify ssl/ directory exists on host: ls -la ~/aviationwx/ssl/\n4. Check docker-compose.prod.yml volume configuration\n5. Ensure certificates were copied correctly in earlier steps\n6. Verify file permissions: chmod 644 ssl/fullchain.pem && chmod 600 ssl/privkey.pem"
            else
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "❌ ERROR: SSL certificates not accessible in nginx container"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "This indicates a volume mount problem."
              echo "Checking host-side certificates..."
              ls -la ssl/ || echo "ssl/ directory not found on host"
              echo ""
              echo "Container volume mounts:"
              docker inspect aviationwx-nginx | grep -A 20 Mounts || echo "Could not inspect container"
              echo ""
              echo "Fix: Ensure ssl/ directory exists on host and contains certificates."
              exit 1
            fi
          fi
          EOF

      - name: Purge browser and service worker caches on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Force service worker update by updating public/js/service-worker.js timestamp
          # This ensures clients will download new version
          # Done after Nginx restart to ensure fresh content is served
          touch public/js/service-worker.js || true
          echo "✓ Updated service worker file timestamp to force cache bust"
          
          # If using Nginx proxy_cache, we would purge here
          # Currently not using proxy_cache, but keeping this for future reference
          # docker exec nginx nginx -s reload || true
          EOF

      - name: Post-deployment verification (quick check)
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Verification (Quick Check)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Verify deployed commit SHA
          EXPECTED_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          EXPECTED_SHA_SHORT="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          
          echo "Verifying deployed commit SHA..."
          echo "Expected: ${EXPECTED_SHA_SHORT}"
          
          # Get actual SHA from container environment variable
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -z "$ACTUAL_SHA" ]; then
            echo "⚠️  Warning: GIT_SHA environment variable not set in container"
            echo "This may indicate the deployment didn't set the variable correctly"
          else
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            echo "Actual: ${ACTUAL_SHA_SHORT}"
            
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              if [ "$(type -t deployment_error)" = "function" ]; then
                deployment_error \
                  "Commit SHA Verification" \
                  "Deployed commit SHA does not match expected SHA. Expected: ${EXPECTED_SHA_SHORT}, Actual: ${ACTUAL_SHA_SHORT}" \
                  "1. This indicates a partial deployment or deployment of wrong code\n2. Check if rsync completed successfully\n3. Verify GIT_SHA environment variable was set correctly\n4. Check container logs: docker compose -f docker/docker-compose.prod.yml logs web\n5. Verify deployment completed all steps\n6. Consider rolling back if system is in inconsistent state"
              else
                echo ""
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "❌ ERROR: Deployed commit SHA does not match expected SHA"
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "Expected: ${EXPECTED_SHA_SHORT}"
                echo "Actual: ${ACTUAL_SHA_SHORT}"
                echo ""
                echo "This indicates a partial deployment or deployment of wrong code."
                exit 1
              fi
            fi
            
            echo "✓ Deployed commit SHA matches expected SHA"
          fi
          
          # Quick service check
          echo ""
          echo "Quick service response check..."
          if ! curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
            echo "⚠️  Warning: Service not responding on https://aviationwx.org/"
            echo "This may be expected if services are still starting"
            echo "Full health check will verify this in the next step"
          else
            echo "✓ Service is responding"
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✓ Post-deployment verification complete"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          EOF

      - name: Post-deployment health check
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Post-Deployment Health Checks"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Checking for partial deployment failures..."
          echo ""
          
          HEALTH_CHECK_FAILED=false
          FAILURE_REASONS=()
          PARTIAL_DEPLOYMENT=false
          
          # Check 1: Verify deployment completed (not partial)
          echo "1. Checking deployment completeness..."
          
          # Check if files were synced
          if [ ! -f "docker/docker-compose.prod.yml" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Files not synced - partial deployment detected")
          fi
          
          # Check if containers exist
          if ! docker compose -f docker/docker-compose.prod.yml ps web > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Containers not found - partial deployment detected")
          fi
          
          # Wait for containers to be ready using polling (faster than fixed sleep)
          echo ""
          echo "2. Waiting for containers to start..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            # First check container is running
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type status \
              --timeout 20 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml; then
              echo "✓ Container is running"
            else
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Web container not running")
            fi
          else
            # Fallback to simple check if script not available
            sleep 5
            if ! docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Web container not running")
            fi
          fi
          
          # Check 3: Containers running
          echo "3. Checking container status..."
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps --format json 2>/dev/null || docker compose -f docker/docker-compose.prod.yml ps)
          
          # Count running containers (both web and nginx should be up)
          RUNNING_COUNT=$(docker compose -f docker/docker-compose.prod.yml ps --status running --format json 2>/dev/null | grep -c '"State":"running"' || docker compose -f docker/docker-compose.prod.yml ps | grep -c "Up" || echo "0")
          
          # Check that we have at least the web container (nginx is optional but should be running too)
          if [ "$RUNNING_COUNT" -lt "1" ]; then
            HEALTH_CHECK_FAILED=true
            PARTIAL_DEPLOYMENT=true
            FAILURE_REASONS+=("Web container not running")
          else
            echo "✓ Containers are running (${RUNNING_COUNT} container(s) up)"
          fi
          
          # Check 4: Container health status (using polling script)
          echo ""
          echo "4. Checking container health..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type health \
              --timeout 60 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml; then
              echo "✓ Web container is healthy"
            else
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Container did not become healthy within timeout")
            fi
          else
            # Fallback to manual polling if script not available
            MAX_WAIT=60
            WAIT_TIME=0
            while [ $WAIT_TIME -lt $MAX_WAIT ]; do
              HEALTH=$(docker compose -f docker/docker-compose.prod.yml ps web --format json 2>/dev/null | grep -o '"Health":"[^"]*"' | cut -d'"' -f4 || echo "unknown")
              if [ "$HEALTH" = "healthy" ]; then
                echo "✓ Web container is healthy"
                break
              elif [ "$HEALTH" = "unhealthy" ]; then
                HEALTH_CHECK_FAILED=true
                FAILURE_REASONS+=("Container health status: unhealthy")
                break
              else
                echo "  Container health: $HEALTH (waiting...)"
                sleep 2
                WAIT_TIME=$((WAIT_TIME + 2))
              fi
            done
            
            if [ $WAIT_TIME -ge $MAX_WAIT ]; then
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Container did not become healthy within ${MAX_WAIT} seconds")
            fi
          fi
          
          # Check 5: Service responding (HTTP check)
          echo ""
          echo "5. Checking service response..."
          if [ -f scripts/wait-for-container-health.sh ]; then
            chmod +x scripts/wait-for-container-health.sh
            if scripts/wait-for-container-health.sh \
              --container web \
              --check-type http \
              --timeout 15 \
              --interval 2 \
              --compose-file docker/docker-compose.prod.yml \
              --url http://localhost:8080/; then
              echo "✓ Web container is responding (internal check passed)"
            else
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Homepage not accessible (internal check failed)")
            fi
          else
            # Fallback to simple check if script not available
            if ! docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s --max-time 5 http://localhost:8080/ > /dev/null 2>&1; then
              HEALTH_CHECK_FAILED=true
              FAILURE_REASONS+=("Homepage not accessible (internal check failed)")
            else
              echo "✓ Web container is responding (internal check passed)"
            fi
          fi
          
          # Check 6: Verify commit SHA matches (detect partial code deployment)
          echo ""
          echo "6. Verifying deployed code..."
          EXPECTED_SHA="${{ steps.determine-sha.outputs.deploy_sha }}"
          EXPECTED_SHA_SHORT="${{ steps.determine-sha.outputs.deploy_sha_short }}"
          ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
          
          if [ -n "$ACTUAL_SHA" ]; then
            ACTUAL_SHA_SHORT="${ACTUAL_SHA:0:7}"
            if [ "$EXPECTED_SHA_SHORT" != "$ACTUAL_SHA_SHORT" ]; then
              HEALTH_CHECK_FAILED=true
              PARTIAL_DEPLOYMENT=true
              FAILURE_REASONS+=("Code mismatch - expected ${EXPECTED_SHA_SHORT}, got ${ACTUAL_SHA_SHORT}")
            else
              echo "✓ Deployed code matches expected commit SHA"
            fi
          fi
          
          # Check 7: API responding
          echo ""
          echo "7. Checking API response..."
          APP_PORT=${APP_PORT:-8080}
          if ! curl -f -s --max-time 10 "https://aviationwx.org/api/weather.php?id=kspb" > /dev/null 2>&1; then
            HEALTH_CHECK_FAILED=true
            FAILURE_REASONS+=("Weather API not responding")
          else
            echo "✓ Weather API responding"
          fi
          
          # Final check
          if [ "$HEALTH_CHECK_FAILED" = "true" ]; then
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "❌ HEALTH CHECKS FAILED"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            
            if [ "$PARTIAL_DEPLOYMENT" = "true" ]; then
              echo "⚠️  PARTIAL DEPLOYMENT DETECTED"
              echo "Some deployment steps completed, others failed."
              echo "System may be in inconsistent state."
              echo ""
            fi
            
            echo "Failure reasons:"
            for reason in "${FAILURE_REASONS[@]}"; do
              echo "  ❌ $reason"
            done
            echo ""
            
            echo "Container status:"
            docker compose -f docker/docker-compose.prod.yml ps
            echo ""
            
            echo "Container logs (last 50 lines):"
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            echo ""
            
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "Triggering automatic rollback..."
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            exit 1  # This will trigger the rollback step
          fi
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "✅ All health checks passed"
          echo "✅ Deployment completed successfully (no partial deployment detected)"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          
          # Check if diagnostics page loads (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1:${APP_PORT}/diagnostics.php > /dev/null 2>&1; then
            echo "⚠️  Diagnostics page not accessible (may be expected)"
          else
            echo "✓ Diagnostics page accessible"
          fi
          
          # Check if config-utils.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/config.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "config.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ config.php not found in container"
              exit 1
            fi
          fi
          
          echo "✓ config.php present"
          
          # Check if rate-limit.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/rate-limit.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "rate-limit.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ rate-limit.php not found in container"
              exit 1
            fi
          fi
          
          echo "✓ rate-limit.php present"
          
          # Check APCu extension
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web php -m | grep -q apcu; then
            echo "⚠️  APCu extension not loaded (rate limiting will be disabled)"
          else
            echo "✓ APCu extension loaded"
          fi
          
          # Check unified webcam worker exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/unified-webcam-worker.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "unified-webcam-worker.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/scripts/"
            else
              echo "❌ unified-webcam-worker.php not found in container"
              exit 1
            fi
          fi
          echo "✓ unified-webcam-worker.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/sync-push-config.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "sync-push-config.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/scripts/"
            else
              echo "❌ sync-push-config.php not found in container"
              exit 1
            fi
          fi
          echo "✓ sync-push-config.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/push-webcam-validator.php; then
            if [ "$(type -t deployment_error)" = "function" ]; then
              deployment_error \
                "File Verification" \
                "push-webcam-validator.php not found in container" \
                "1. Verify file was synced via rsync\n2. Check Dockerfile COPY commands\n3. Verify file exists in repository\n4. Check container filesystem: docker compose -f docker/docker-compose.prod.yml exec web ls -la /var/www/html/lib/"
            else
              echo "❌ push-webcam-validator.php not found in container"
              exit 1
            fi
          fi
          echo "✓ push-webcam-validator.php present"
          
          # Check FTP/SFTP services (may not be running if no push cameras configured)
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x vsftpd > /dev/null 2>&1; then
            echo "✓ vsftpd service running"
          else
            echo "ℹ️  vsftpd service not running (expected if no push cameras configured)"
          fi
          
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x sshd > /dev/null 2>&1; then
            echo "✓ sshd service running"
          else
            echo "ℹ️  sshd service not running (expected if no push cameras configured)"
          fi
          
          echo "✅ All health checks passed"
          
          # Test webcam endpoints (smoke test)
          echo "Testing webcam endpoints..."
          # Test JPG endpoint for sample airport (assuming kspb exists)
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" > /tmp/webcam_test.jpg 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.jpg 2>/dev/null || stat -c%s /tmp/webcam_test.jpg 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/jpeg"; then
              echo "✓ Webcam JPG endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.jpg
            else
              echo "⚠️  Webcam JPG endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "⚠️  Webcam JPG endpoint not accessible (may be expected if no webcams configured)"
          fi
          
          # Test WEBP endpoint
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" > /tmp/webcam_test.webp 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.webp 2>/dev/null || stat -c%s /tmp/webcam_test.webp 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/webp"; then
              echo "✓ Webcam WEBP endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.webp
            else
              echo "⚠️  Webcam WEBP endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "ℹ️  Webcam WEBP endpoint not accessible (may be expected if WEBP not generated yet)"
          fi
          
          EOF

      - name: Rollback to previous version (if needed)
        if: failure()
        run: |
          # First, get rollback state from server to determine what to rollback to
          ROLLBACK_STATE=$(ssh ${{ secrets.USER }}@${{ secrets.HOST }} "cat ~/aviationwx-rollback-state.json 2>/dev/null" || echo "")
          
          if [ -z "$ROLLBACK_STATE" ]; then
            echo "⚠️  No rollback state file found on server"
            echo "This may be the first deployment or rollback state was not saved"
            echo "Manual intervention required"
            exit 1
          fi
          
          ROLLBACK_TAG=$(echo "$ROLLBACK_STATE" | jq -r '.rollback_tag // "null"')
          PREVIOUS_SHA=$(echo "$ROLLBACK_STATE" | jq -r '.commit_sha // "unknown"')
          PREVIOUS_COMMIT_SHA=$(echo "$ROLLBACK_STATE" | jq -r '.previous_commit_sha // ""')
          
          echo ""
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "AUTOMATIC ROLLBACK INITIATED"
          echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
          echo "Rolling back to previous working state..."
          echo "Previous commit SHA: ${PREVIOUS_SHA}"
          echo "Previous git commit: ${PREVIOUS_COMMIT_SHA}"
          echo "Rollback tag: ${ROLLBACK_TAG}"
          echo ""
          
          # Step 1: Restore code files from previous commit (on GitHub Actions runner)
          if [ -n "$PREVIOUS_COMMIT_SHA" ] && [ "$PREVIOUS_COMMIT_SHA" != "" ]; then
            echo "Step 1: Restoring code files from previous commit ${PREVIOUS_COMMIT_SHA:0:7}..."
            
            # Checkout previous commit on the runner
            if git checkout "$PREVIOUS_COMMIT_SHA" 2>/dev/null; then
              echo "✓ Checked out previous commit on runner"
              
              # Sync previous commit code to server
              echo "Syncing previous commit code to server..."
              if rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' --exclude 'ssl' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/; then
                echo "✓ Code files restored from previous commit"
              else
                echo "⚠️  Warning: Failed to sync code files (rsync failed)"
                echo "   Will continue with Docker image rollback"
              fi
            else
              echo "⚠️  Warning: Failed to checkout previous commit (git checkout failed)"
              echo "   Will continue with Docker image rollback only"
            fi
          else
            echo "Step 1: Skipping code file restoration (no previous commit SHA available)"
          fi
          
          # Step 2: Restore Docker image and start containers on server
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          
          # Source deployment helper functions for consistent error handling
          if [ -f scripts/deploy-helpers.sh ]; then
            source scripts/deploy-helpers.sh
          fi
          
          # Step 2: Stop current containers (even if partially started)
          echo "Step 2: Stopping current containers (cleaning up partial deployment)..."
          docker compose -f docker/docker-compose.prod.yml down || true
          sleep 2
          
          # Step 3: Restore Docker image
          if [ "$ROLLBACK_TAG" != "null" ] && docker images "aviationwx:$ROLLBACK_TAG" --format "{{.Repository}}:{{.Tag}}" 2>/dev/null | grep -q "aviationwx:$ROLLBACK_TAG"; then
            echo "Step 3: Restoring previous Docker image..."
            if docker tag "aviationwx:$ROLLBACK_TAG" "aviationwx:latest" 2>/dev/null; then
              echo "✓ Rollback image tagged successfully"
            else
              echo "❌ ERROR: Failed to tag rollback image"
              echo "Will attempt to start containers anyway (may need rebuild)"
            fi
          else
            echo "Step 3: No rollback image available (will use existing or rebuild)"
          fi
          
          # Step 4: Start containers with rollback image and previous commit SHA
          echo "Step 4: Starting containers with rollback configuration..."
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Set GIT_SHA to previous commit for rollback
          if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "unknown" ]; then
            export GIT_SHA="${PREVIOUS_SHA:0:7}"
            echo "Setting GIT_SHA to previous commit: ${GIT_SHA}"
          fi
          
          if ! docker compose -f docker/docker-compose.prod.yml up -d; then
            echo "❌ ERROR: Failed to start containers with rollback configuration"
            echo "Manual intervention required"
            echo ""
            echo "Debugging info:"
            docker compose -f docker/docker-compose.prod.yml ps
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            exit 1
          fi
          
          # Step 5: Wait for services to be ready
          echo "Step 5: Waiting for services to be ready..."
          MAX_WAIT=30
          WAIT_TIME=0
          while [ $WAIT_TIME -lt $MAX_WAIT ]; do
            if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
              # Check if container is actually responding
              if docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s http://localhost:8080/ > /dev/null 2>&1; then
                echo "✓ Services are ready after ${WAIT_TIME}s"
                break
              fi
            fi
            sleep 2
            WAIT_TIME=$((WAIT_TIME + 2))
          done
          
          # Step 6: Verify rollback was successful
          if docker compose -f docker/docker-compose.prod.yml ps web | grep -q "Up"; then
            echo "Step 6: Verifying rollback..."
            
            # Verify GIT_SHA matches previous commit
            if [ -n "$PREVIOUS_SHA" ] && [ "$PREVIOUS_SHA" != "unknown" ]; then
              ACTUAL_SHA=$(docker compose -f docker/docker-compose.prod.yml exec -T web printenv GIT_SHA 2>/dev/null || echo "")
              if [ -n "$ACTUAL_SHA" ]; then
                if [ "${ACTUAL_SHA:0:7}" = "${PREVIOUS_SHA:0:7}" ]; then
                  echo "✓ GIT_SHA matches previous commit: ${ACTUAL_SHA:0:7}"
                else
                  echo "⚠️  Warning: GIT_SHA mismatch (expected ${PREVIOUS_SHA:0:7}, got ${ACTUAL_SHA:0:7})"
                fi
              fi
            fi
            
            # Verify service is responding
            if curl -f -s --max-time 10 https://aviationwx.org/ > /dev/null 2>&1; then
              echo "✓ Service is responding after rollback"
              echo ""
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo "✅ Rollback completed successfully"
              echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
              echo ""
              echo "System restored to previous working state:"
              echo "  - Code files: ${PREVIOUS_COMMIT_SHA:0:7}"
              echo "  - Docker image: ${ROLLBACK_TAG}"
              echo "  - Commit SHA: ${PREVIOUS_SHA:0:7}"
            else
              echo "⚠️  Warning: Service not responding after rollback"
              echo "Container is running but service may not be fully functional"
              echo "Check logs: docker compose -f docker/docker-compose.prod.yml logs web"
            fi
          else
            echo "❌ ERROR: Containers not running after rollback"
            echo "Manual intervention required"
            echo ""
            echo "Debugging info:"
            docker compose -f docker/docker-compose.prod.yml ps
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            exit 1
          fi
          EOF

