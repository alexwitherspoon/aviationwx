name: Deploy to Production

on:
  workflow_run:
    # Only trigger on Quality Assurance Tests completion (typically runs last)
    # The check-prerequisites job will verify BOTH workflows have succeeded
    workflows: ["Quality Assurance Tests"]
    types:
      - completed
    branches: [ "main" ]
  workflow_dispatch: {} # Allow manual trigger

# Ensure only one deployment runs at a time
concurrency:
  group: deploy-production
  cancel-in-progress: false

# Permissions for deployment
permissions:
  contents: read
  actions: read
  checks: read

jobs:
  check-prerequisites:
    runs-on: ubuntu-latest
    outputs:
      test-success: ${{ steps.check.outputs.test-success }}
      qa-success: ${{ steps.check.outputs.qa-success }}
    steps:
      - name: Wait for and check required workflows status
        id: check
        uses: actions/github-script@v7
        with:
          script: |
            // Only deploy on push events (commits/merges to main), not on pull_request events
            // workflow_run event contains the event that triggered the source workflow
            const triggeringEvent = '${{ github.event.workflow_run && github.event.workflow_run.event || github.event_name }}';
            
            core.info(`Triggering event: ${triggeringEvent}`);
            
            // Skip deployment if triggered by a pull_request event
            if (triggeringEvent === 'pull_request') {
              core.info('⏭️  Skipping deployment - triggered by pull_request event (only deploy on push/merge to main)');
              core.setOutput('test-success', 'false');
              core.setOutput('qa-success', 'false');
              return;
            }
            
            // Verify this is a push event (direct commit or merge to main)
            if (triggeringEvent !== 'push') {
              core.warning(`⚠️  Unexpected event type: ${triggeringEvent}. Only push events trigger deployment.`);
              core.setOutput('test-success', 'false');
              core.setOutput('qa-success', 'false');
              return;
            }
            
            // Get the commit SHA - use workflow_run head_sha if available, otherwise context.sha
            const commitSha = '${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}';
            
            // Get the triggering workflow run ID if available (from workflow_run event)
            // GitHub Actions expressions can't use empty strings directly, so we check in JavaScript
            const triggeringWorkflowRunIdRaw = '${{ github.event.workflow_run && github.event.workflow_run.id }}';
            const triggeringWorkflowRunId = (triggeringWorkflowRunIdRaw && triggeringWorkflowRunIdRaw !== '') ? triggeringWorkflowRunIdRaw : null;
            
            core.info(`Checking workflow statuses for commit: ${commitSha}`);
            if (triggeringWorkflowRunId) {
              core.info(`Triggered by workflow run ID: ${triggeringWorkflowRunId}`);
            }
            
            const maxWaitTime = 30 * 60 * 1000; // 30 minutes in milliseconds
            const checkInterval = 30 * 1000; // Check every 30 seconds
            const startTime = Date.now();
            
            let testRun = null;
            let qaRun = null;
            let testSuccess = false;
            let qaSuccess = false;
            
            // Use workflow file paths instead of IDs (more reliable)
            const testWorkflowPath = '.github/workflows/test.yml';
            const qaWorkflowPath = '.github/workflows/quality-assurance-tests.yml';
            
            core.info(`Looking for workflows: ${testWorkflowPath} and ${qaWorkflowPath}`);
            
            // Wait for both workflows to complete
            while (Date.now() - startTime < maxWaitTime) {
              // Get workflow runs by workflow file path for more specific filtering
              // This ensures we get runs for the specific workflow, not just by name
              const testRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: testWorkflowPath,
                head_sha: commitSha,
                per_page: 10
              });
              
              const qaRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: qaWorkflowPath,
                head_sha: commitSha,
                per_page: 10
              });
              
              // Find the most recent run for this commit (should be the first one)
              // Filter to ensure it's from a push event (not pull_request) and matches the exact commit SHA
              testRun = testRuns.data.workflow_runs.find(run => 
                run.head_sha === commitSha && 
                (run.event === 'push' || run.event === 'workflow_dispatch')
              );
              
              // If no exact match, try the first run (might be a race condition)
              if (!testRun && testRuns.data.workflow_runs.length > 0) {
                const firstRun = testRuns.data.workflow_runs[0];
                // Only use if it's a push event and matches commit (within reasonable time window)
                if (firstRun.event === 'push' && firstRun.head_sha === commitSha) {
                  testRun = firstRun;
                }
              }
              
              // If we have a triggering workflow run ID, use that to find the exact QA run
              if (triggeringWorkflowRunId) {
                qaRun = qaRuns.data.workflow_runs.find(run => 
                  run.id.toString() === triggeringWorkflowRunId
                );
              }
              
              // If not found by ID, try finding by commit SHA and event type
              if (!qaRun) {
                qaRun = qaRuns.data.workflow_runs.find(run => 
                  run.head_sha === commitSha && 
                  (run.event === 'push' || run.event === 'workflow_dispatch')
                );
              }
              
              // If no exact match, try the first run
              if (!qaRun && qaRuns.data.workflow_runs.length > 0) {
                const firstRun = qaRuns.data.workflow_runs[0];
                // Only use if it's a push event and matches commit (within reasonable time window)
                if (firstRun.event === 'push' && firstRun.head_sha === commitSha) {
                  qaRun = firstRun;
                }
              }
              
              const testStatus = testRun?.status; // queued, in_progress, completed
              const testConclusion = testRun?.conclusion; // success, failure, cancelled, etc.
              const qaStatus = qaRun?.status;
              const qaConclusion = qaRun?.conclusion;
              
              core.info(`Test and Lint: ${testStatus || 'not found'} (${testConclusion || 'pending'}) - Run ID: ${testRun?.id || 'N/A'}`);
              core.info(`Quality Assurance Tests: ${qaStatus || 'not found'} (${qaConclusion || 'pending'}) - Run ID: ${qaRun?.id || 'N/A'}`);
              
              // If workflows haven't started yet (queued), wait a bit longer
              if ((testStatus === 'queued' || !testRun) && (qaStatus === 'queued' || !qaRun)) {
                core.info('Workflows are queued or not found yet, waiting...');
                await new Promise(resolve => setTimeout(resolve, checkInterval));
                continue;
              }
              
              // Check if both workflows have completed
              if (testStatus === 'completed' && qaStatus === 'completed') {
                testSuccess = testConclusion === 'success';
                qaSuccess = qaConclusion === 'success';
                break;
              }
              
              // If one has failed, we can stop waiting (but still check both conclusions)
              if (testStatus === 'completed' && testConclusion !== 'success' && testConclusion !== null) {
                testSuccess = false;
                if (qaStatus === 'completed') {
                  qaSuccess = qaConclusion === 'success';
                  break;
                }
              }
              
              if (qaStatus === 'completed' && qaConclusion !== 'success' && qaConclusion !== null) {
                qaSuccess = false;
                if (testStatus === 'completed') {
                  testSuccess = testConclusion === 'success';
                  break;
                }
              }
              
              // Wait before checking again
              core.info('Waiting for both workflows to complete...');
              await new Promise(resolve => setTimeout(resolve, checkInterval));
            }
            
            // Final check - ensure we have both workflows
            if (!testRun || !qaRun) {
              // Get diagnostic info for debugging
              const allTestRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: testWorkflowPath,
                per_page: 5
              });
              
              const allQaRuns = await github.rest.actions.listWorkflowRuns({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: qaWorkflowPath,
                per_page: 5
              });
              
              core.error(`❌ Could not find one or both required workflows for commit ${commitSha}`);
              core.error(`Recent Test and Lint runs: ${allTestRuns.data.workflow_runs.map(r => `${r.head_sha.substring(0, 7)} (${r.status})`).join(', ')}`);
              core.error(`Recent Quality Assurance Tests runs: ${allQaRuns.data.workflow_runs.map(r => `${r.head_sha.substring(0, 7)} (${r.status})`).join(', ')}`);
              core.error(`Looking for commit: ${commitSha}`);
              
              if (!testRun) {
                core.error(`Test and Lint: No run found for commit ${commitSha}`);
              }
              if (!qaRun) {
                core.error(`Quality Assurance Tests: No run found for commit ${commitSha}`);
              }
              
              core.setFailed('❌ Could not find one or both required workflows for this commit');
              return;
            }
            
            if (testRun.status !== 'completed' || qaRun.status !== 'completed') {
              core.setFailed(`❌ One or both workflows are still running after wait period:
                - Test and Lint: ${testRun.status} (${testRun.conclusion || 'pending'})
                - Quality Assurance Tests: ${qaRun.status} (${qaRun.conclusion || 'pending'})`);
              return;
            }
            
            core.setOutput('test-success', testSuccess ? 'true' : 'false');
            core.setOutput('qa-success', qaSuccess ? 'true' : 'false');
            
            core.info(`Test and Lint: ${testRun.conclusion || 'unknown'} (ID: ${testRun.id})`);
            core.info(`Quality Assurance Tests: ${qaRun.conclusion || 'unknown'} (ID: ${qaRun.id})`);
            
            if (!testSuccess) {
              core.error(`❌ Test and Lint workflow did not succeed for commit ${commitSha}`);
            }
            if (!qaSuccess) {
              core.error(`❌ Quality Assurance Tests workflow did not succeed for commit ${commitSha}`);
            }
            
            if (testSuccess && qaSuccess) {
              core.info('✅ Both required workflows succeeded - deployment will proceed');
            } else {
              core.setFailed('❌ Not all required workflows succeeded. Deployment blocked.');
            }

  deploy:
    runs-on: ubuntu-latest
    needs: check-prerequisites
    # Only deploy if both workflows succeeded (or manual dispatch which bypasses checks)
    if: |
      (needs.check-prerequisites.outputs.test-success == 'true' && 
       needs.check-prerequisites.outputs.qa-success == 'true') || 
      github.event_name == 'workflow_dispatch'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          # When triggered by workflow_run, checkout the commit from that workflow
          # Otherwise use the commit SHA from the prerequisites check
          ref: ${{ github.event.workflow_run.head_sha || github.sha }}
          fetch-depth: 0

      - name: Setup SSH
        uses: webfactory/ssh-agent@v0.9.0
        with:
          ssh-private-key: |
            ${{ secrets.SSH_PRIVATE_KEY }}

      - name: Add host to known_hosts
        run: |
          mkdir -p ~/.ssh
          ssh-keyscan -H ${{ secrets.HOST }} >> ~/.ssh/known_hosts

      - name: Update cache version for cache busting
        run: |
          # Generate a unique version based on commit SHA and timestamp
          DEPLOY_VERSION="$(git rev-parse --short HEAD)-$(date +%s)"
          echo "Updating cache version to: ${DEPLOY_VERSION}"
          
          # Update the cache version in public/js/service-worker.js
          # The script handles missing files and verification
          bash scripts/update-cache-version.sh "${DEPLOY_VERSION}"
          
          # Note: We don't commit service-worker.js changes during deployment
          # The updated file will be synced to the server via rsync
          # This avoids creating unnecessary commits and deployment loops
          echo "✓ Cache version updated (will be synced to server)"
      
      - name: Verify push webcam files are present
        run: |
          echo "Verifying push webcam implementation files..."
          required_files=(
            "scripts/process-push-webcams.php"
            "scripts/sync-push-config.php"
            "scripts/create-sftp-user.sh"
            "scripts/service-watchdog.sh"
            "lib/push-webcam-validator.php"
            "pages/config-generator.php"
            "docker/vsftpd.conf"
            "docker/sshd_config"
          )
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Required push webcam file missing: $file"
              exit 1
            fi
          done
          echo "✓ All push webcam files present"

      - name: Clean up old directory structure on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Clean up leftover docker/docker directory from old path issue (now fixed)
          # This is defensive cleanup - the directory should no longer be created
          if [ -d docker/docker ] || [ -f docker/docker/nginx.conf ]; then
            echo "Cleaning up old docker/docker directory structure..."
            rm -rf docker/docker 2>/dev/null || true
            echo "✓ Cleanup complete"
          else
            echo "✓ No cleanup needed (old directory structure not found)"
          fi
          EOF

      - name: Sync repository to server
        run: |
          # Exclude docker/docker from rsync (defensive - should not exist after path fix)
          # Exclude uploads directory (ephemeral, created inside container)
          rsync -az --delete --exclude '.git' --exclude 'cache' --exclude 'docker/docker' --exclude 'uploads' ./ ${{ secrets.USER }}@${{ secrets.HOST }}:~/aviationwx/ || {
            echo "⚠️  Rsync had some errors, but continuing..."
            # Continue even if rsync fails on some deletions
          }

      - name: Ensure cache directory exists
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          
          # Get www-data UID/GID from container (typically 33:33)
          # If container is running, get actual UID/GID from it
          WWW_DATA_UID=33
          WWW_DATA_GID=33
          if docker ps --format '{{.Names}}' | grep -q '^aviationwx-web$'; then
            # Container is running - get actual UID/GID
            CONTAINER_UID=$(docker exec aviationwx-web id -u www-data 2>/dev/null || echo "33")
            CONTAINER_GID=$(docker exec aviationwx-web id -g www-data 2>/dev/null || echo "33")
            WWW_DATA_UID=${CONTAINER_UID}
            WWW_DATA_GID=${CONTAINER_GID}
            echo "Detected www-data UID/GID from container: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          else
            echo "Container not running, using default www-data UID/GID: ${WWW_DATA_UID}:${WWW_DATA_GID}"
          fi
          
          # Check if parent directory exists and who owns it
          if [ -d /tmp/aviationwx-cache ]; then
            CURRENT_OWNER=$(stat -c '%U:%G' /tmp/aviationwx-cache 2>/dev/null || stat -f '%Su:%Sg' /tmp/aviationwx-cache 2>/dev/null || echo "unknown")
            echo "Existing cache directory owner: ${CURRENT_OWNER}"
          fi
          
          # Try to create directory structure without sudo first
          if mkdir -p /tmp/aviationwx-cache/webcams 2>/dev/null; then
            echo "✓ Created cache directory without sudo"
          else
            echo "⚠️  Failed to create directory without sudo, trying with sudo..."
            # Use sudo to create directory structure
            sudo mkdir -p /tmp/aviationwx-cache/webcams || {
              echo "❌ Failed to create cache directory even with sudo"
              exit 1
            }
            echo "✓ Created cache directory with sudo"
          fi
          
          # Set ownership to match container's www-data user
          # Try without sudo first, then with sudo if needed
          if chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache 2>/dev/null; then
            echo "✓ Changed ownership without sudo"
          else
            echo "⚠️  Failed to change ownership without sudo, trying with sudo..."
            sudo chown -R ${WWW_DATA_UID}:${WWW_DATA_GID} /tmp/aviationwx-cache || {
              echo "⚠️  Failed to change ownership with sudo, using fallback permissions"
              # Fallback: make world-writable (less secure but works)
              sudo chmod -R 777 /tmp/aviationwx-cache || true
            }
          fi
          
          # Ensure directory permissions allow www-data to write
          # Use 775 for webcams (group writable) and 755 for parent
          sudo chmod 755 /tmp/aviationwx-cache 2>/dev/null || chmod 755 /tmp/aviationwx-cache || true
          sudo chmod 775 /tmp/aviationwx-cache/webcams 2>/dev/null || chmod 775 /tmp/aviationwx-cache/webcams || true
          
          # Verify directory was created and is writable
          if [ ! -d /tmp/aviationwx-cache/webcams ]; then
            echo "❌ Failed to create cache directory"
            exit 1
          fi
          
          # Show final permissions
          echo "Final directory permissions:"
          ls -ld /tmp/aviationwx-cache
          ls -ld /tmp/aviationwx-cache/webcams
          
          # Test write access (as current user, which should work if permissions are correct)
          if touch /tmp/aviationwx-cache/webcams/.test_write 2>/dev/null; then
            rm -f /tmp/aviationwx-cache/webcams/.test_write
            echo "✓ Cache directory is writable by deployment user"
          else
            echo "⚠️  Cache directory may not be writable by deployment user"
            echo "   (This is OK if container user can write)"
          fi
          
          # Final verification
          echo "✓ Cache directory created at /tmp/aviationwx-cache/webcams"
          echo "✓ Ownership set to ${WWW_DATA_UID}:${WWW_DATA_GID} (www-data)"
          EOF

      - name: Note uploads directory is ephemeral
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          # Uploads directory is now ephemeral (inside container only)
          # Files are uploaded via SFTP/FTP/FTPS, processed, and moved to cache
          # No host-side directory setup needed
          echo "ℹ️  Uploads directory is ephemeral (inside container only)"
          echo "ℹ️  Camera-specific directories are created automatically by sync-push-config.php"
          echo "ℹ️  Files are processed and moved to cache, then removed from uploads"
          # Optional: Clean up old host directory if it exists (harmless if left)
          if [ -d ~/aviationwx/uploads ] || [ -d /var/www/html/uploads ]; then
            echo "ℹ️  Old uploads directory found on host (will be ignored, safe to remove)"
            echo "   To remove: rm -rf ~/aviationwx/uploads /var/www/html/uploads"
          fi
          EOF

      - name: Note cache location
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          # Cache is now in /tmp/aviationwx-cache (ephemeral, cleared on reboot)
          # Daily tracking files (peak_gusts.json, temp_extremes.json) will be recreated
          # if missing. Cache is ephemeral by design - useful for resetting state if needed.
          echo "ℹ️  Cache directory: /tmp/aviationwx-cache (ephemeral, cleared on reboot)"
          echo "ℹ️  Application will automatically recreate cache files if missing"
          EOF

      - name: Ensure SSL certificates are in place
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Verify we're running as the correct user
          CURRENT_USER=$(whoami)
          echo "Running as user: $CURRENT_USER"
          
          # Create SSL directory if it doesn't exist
          mkdir -p ssl
          
          # Copy SSL certificates from Let's Encrypt if they exist
          # This ensures certificates are available for Nginx container
          # All operations run as aviationwx user, using sudo for protected operations
          # Use sudo test to check if files exist (directory permissions are root-only)
          if sudo test -f /etc/letsencrypt/live/aviationwx.org/fullchain.pem && sudo test -f /etc/letsencrypt/live/aviationwx.org/privkey.pem; then
            echo "Copying SSL certificates from Let's Encrypt..."
            # Copy certificates (requires sudo to read from /etc/letsencrypt/)
            if sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ssl/; then
              if sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ssl/; then
                # Set correct ownership and permissions
                sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/
                sudo chmod 644 ssl/fullchain.pem
                sudo chmod 600 ssl/privkey.pem
                echo "✓ SSL certificates copied successfully"
              else
                echo "⚠️  Failed to copy privkey.pem"
                exit 1
              fi
            else
              echo "⚠️  Failed to copy fullchain.pem"
              exit 1
            fi
          elif [ -f ssl/fullchain.pem ] && [ -f ssl/privkey.pem ]; then
            echo "✓ SSL certificates already exist in ~/aviationwx/ssl/"
            # Verify permissions are correct
            sudo chown -R $CURRENT_USER:$CURRENT_USER ssl/ || true
            sudo chmod 644 ssl/fullchain.pem || true
            sudo chmod 600 ssl/privkey.pem || true
          else
            echo "⚠️  SSL certificates not found in /etc/letsencrypt/live/aviationwx.org/ or ~/aviationwx/ssl/"
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo "SSL CERTIFICATES MISSING"
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            echo "Nginx container will fail to start without SSL certificates."
            echo "You need to generate certificates with Certbot first."
            echo ""
            
            # Check if Certbot is installed
            if command -v certbot >/dev/null 2>&1; then
              echo "✓ Certbot is installed"
              echo ""
              echo "To generate wildcard certificates (DNS-01 challenge):"
              echo ""
              echo "  1. Install Cloudflare DNS plugin:"
              echo "     sudo apt install certbot python3-certbot-dns-cloudflare -y"
              echo ""
              echo "  2. Create Cloudflare API token (scoped to zone):"
              echo "     - Permissions: Zone → DNS → Edit; Zone → Zone → Read"
              echo "     - Resources: Include → Specific zone → aviationwx.org"
              echo ""
              echo "  3. Store token:"
              echo "     mkdir -p ~/.secrets"
              echo "     printf 'dns_cloudflare_api_token = %s\\n' 'YOUR_TOKEN' > ~/.secrets/cloudflare.ini"
              echo "     chmod 600 ~/.secrets/cloudflare.ini"
              echo ""
              echo "  4. Generate certificates:"
              echo "     sudo certbot certonly \\"
              echo "       --dns-cloudflare \\"
              echo "       --dns-cloudflare-credentials ~/.secrets/cloudflare.ini \\"
              echo "       -d aviationwx.org -d '*.aviationwx.org' \\"
              echo "       --non-interactive --agree-tos -m your@email.com"
              echo ""
              echo "  5. Copy certificates:"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/fullchain.pem ~/aviationwx/ssl/"
              echo "     sudo cp /etc/letsencrypt/live/aviationwx.org/privkey.pem ~/aviationwx/ssl/"
              echo "     sudo chown -R $CURRENT_USER:$CURRENT_USER ~/aviationwx/ssl"
              echo "     sudo chmod 644 ~/aviationwx/ssl/fullchain.pem"
              echo "     sudo chmod 600 ~/aviationwx/ssl/privkey.pem"
            else
              echo "⚠️  Certbot is not installed"
              echo ""
              echo "To install Certbot and generate certificates:"
              echo ""
              echo "  1. Install Certbot with Cloudflare DNS plugin:"
              echo "     sudo apt update && sudo apt install -y certbot python3-certbot-dns-cloudflare"
              echo ""
              echo "  2. Follow steps 2-5 above (create Cloudflare token, generate certs, copy them)"
            fi
            
            echo ""
            echo "After certificates are in place, redeploy to continue."
            echo ""
            echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
            echo ""
            # Fail the deployment since Nginx won't start without certificates
            exit 1
          fi
          EOF

      - name: Deploy via Docker Compose
        run: |
          # Get commit SHA from GitHub Actions context (7 characters to match GitHub's short SHA display)
          # Note: .git directory is excluded during rsync, so we can't get SHA from server's git repo
          GIT_SHA="${{ github.event.workflow_run && github.event.workflow_run.head_sha || github.sha }}"
          GIT_SHA="${GIT_SHA:0:7}"
          echo "Deploying with GIT_SHA: ${GIT_SHA}"
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << EOF
          set -euo pipefail
          cd ~/aviationwx
          # Set GIT_SHA as environment variable for docker compose
          export GIT_SHA="$GIT_SHA"
          # Stop and remove existing containers to avoid name conflicts
          docker compose -f docker/docker-compose.prod.yml down || true
          # Build and start containers
          docker compose -f docker/docker-compose.prod.yml up -d --build
          docker system prune -f
          EOF

      - name: Verify Docker logging configuration
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Logging is now handled by Docker (stdout/stderr)
          # Docker automatically captures logs and handles rotation
          # No host-side log directory setup needed
          echo "✓ Docker logging configured (no host setup required)"
          EOF

      - name: Purge browser and service worker caches on server
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Force service worker update by updating public/js/service-worker.js timestamp
          # This ensures clients will download new version
          touch public/js/service-worker.js || true
          echo "✓ Updated service worker file timestamp to force cache bust"
          
          # If using Nginx proxy_cache, we would purge here
          # Currently not using proxy_cache, but keeping this for future reference
          # docker exec nginx nginx -s reload || true
          EOF

      - name: Restart Nginx container to pick up config changes
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          # Restart Nginx container to ensure it picks up new nginx.conf
          # This is safer than reload since config file is mounted as volume
          docker compose -f docker/docker-compose.prod.yml restart nginx || docker compose -f docker/docker-compose.prod.yml up -d nginx
          # Verify Nginx container is running and config is valid
          sleep 2
          if docker compose -f docker/docker-compose.prod.yml exec -T nginx nginx -t 2>&1 | grep -q "syntax is ok\|test is successful"; then
            echo "✓ Nginx configuration is valid"
          else
            echo "⚠ Nginx config test failed, but container is running"
          fi
          EOF

      - name: Verify fail2ban in container
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Verifying fail2ban in container..."
          
          # Wait for container to be ready
          sleep 5
          
          # Check if fail2ban is running in container
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x fail2ban-server > /dev/null 2>&1; then
            echo "✓ fail2ban is running in container"
            
            # Show fail2ban status
            echo "Fail2ban status:"
            docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client status 2>/dev/null || echo "⚠️  Could not get fail2ban status"
            
            # Show active jails
            echo "Active jails:"
            docker compose -f docker/docker-compose.prod.yml exec -T web fail2ban-client status 2>/dev/null | grep -A 10 "Jail list" || echo "⚠️  Could not get jail list"
          else
            echo "⚠️  Warning: fail2ban is not running in container"
            echo "This may be expected if services are still starting"
            echo "Check container logs: docker compose -f docker/docker-compose.prod.yml logs web | grep fail2ban"
          fi
          
          echo "✓ fail2ban verification complete"
          echo ""
          echo "Note: fail2ban runs inside the container with NET_ADMIN capability"
          echo "to manage iptables rules for brute-force protection."
          EOF
      
      - name: Post-deployment health check
        run: |
          ssh ${{ secrets.USER }}@${{ secrets.HOST }} << 'EOF'
          set -euo pipefail
          cd ~/aviationwx
          
          echo "Running post-deployment health checks..."
          
          # Wait for containers to be ready (give them time to start)
          echo "Waiting for containers to start..."
          sleep 8
          
          # Check if containers are running (check both services explicitly)
          CONTAINER_STATUS=$(docker compose -f docker/docker-compose.prod.yml ps --format json 2>/dev/null || docker compose -f docker/docker-compose.prod.yml ps)
          
          # Count running containers (both web and nginx should be up)
          RUNNING_COUNT=$(docker compose -f docker/docker-compose.prod.yml ps --status running --format json 2>/dev/null | grep -c '"State":"running"' || docker compose -f docker/docker-compose.prod.yml ps | grep -c "Up" || echo "0")
          
          # Check that we have at least the web container (nginx is optional but should be running too)
          if [ "$RUNNING_COUNT" -lt "1" ]; then
            echo "❌ Containers are not running"
            echo "Container status:"
            docker compose -f docker/docker-compose.prod.yml ps
            echo ""
            echo "Recent logs:"
            docker compose -f docker/docker-compose.prod.yml logs --tail=30
            exit 1
          fi
          
          echo "✓ Containers are running (${RUNNING_COUNT} container(s) up)"
          
          # Wait for Apache to be fully ready
          echo "Waiting for services to be ready..."
          sleep 10
          
          # Check if web container is responding (from inside container via exec)
          # This is more reliable than checking from host since Apache might have Host header issues
          echo "Testing web container from inside..."
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web curl -f -s --max-time 5 http://localhost/ > /dev/null 2>&1; then
            echo "❌ Web container is not responding (internal check failed)"
            echo "Checking container logs..."
            docker compose -f docker/docker-compose.prod.yml logs web | tail -50
            echo "Checking container status..."
            docker compose -f docker/docker-compose.prod.yml ps
            exit 1
          fi
          
          echo "✓ Web container is responding (internal check passed)"
          
          # Also check from host port (this might fail if Apache has Host header restrictions)
          APP_PORT=${APP_PORT:-8080}
          echo "Testing web container from host on port ${APP_PORT}..."
          if curl -f -s --max-time 5 http://127.0.0.1:${APP_PORT}/ > /dev/null 2>&1; then
            echo "✓ Web container is also responding from host on port ${APP_PORT}"
          else
            echo "⚠️  Web container not accessible from host (but internal check passed - this may be expected)"
            echo "This is OK if Nginx reverse proxy is handling external requests"
          fi
          
          # Check if diagnostics page loads (optional check)
          if ! curl -f -s --max-time 5 http://127.0.0.1:${APP_PORT}/diagnostics.php > /dev/null 2>&1; then
            echo "⚠️  Diagnostics page not accessible (may be expected)"
          else
            echo "✓ Diagnostics page accessible"
          fi
          
          # Check if config-utils.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/config.php; then
            echo "❌ config-utils.php not found in container"
            exit 1
          fi
          
          echo "✓ config-utils.php present"
          
          # Check if rate-limit.php exists
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/rate-limit.php; then
            echo "❌ rate-limit.php not found in container"
            exit 1
          fi
          
          echo "✓ rate-limit.php present"
          
          # Check APCu extension
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web php -m | grep -q apcu; then
            echo "⚠️  APCu extension not loaded (rate limiting will be disabled)"
          else
            echo "✓ APCu extension loaded"
          fi
          
          # Check push webcam scripts exist
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/process-push-webcams.php; then
            echo "❌ process-push-webcams.php not found in container"
            exit 1
          fi
          echo "✓ process-push-webcams.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/scripts/sync-push-config.php; then
            echo "❌ sync-push-config.php not found in container"
            exit 1
          fi
          echo "✓ sync-push-config.php present"
          
          if ! docker compose -f docker/docker-compose.prod.yml exec -T web test -f /var/www/html/lib/push-webcam-validator.php; then
            echo "❌ push-webcam-validator.php not found in container"
            exit 1
          fi
          echo "✓ push-webcam-validator.php present"
          
          # Check FTP/SFTP services (may not be running if no push cameras configured)
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x vsftpd > /dev/null 2>&1; then
            echo "✓ vsftpd service running"
          else
            echo "ℹ️  vsftpd service not running (expected if no push cameras configured)"
          fi
          
          if docker compose -f docker/docker-compose.prod.yml exec -T web pgrep -x sshd > /dev/null 2>&1; then
            echo "✓ sshd service running"
          else
            echo "ℹ️  sshd service not running (expected if no push cameras configured)"
          fi
          
          echo "✅ All health checks passed"
          
          # Test webcam endpoints (smoke test)
          echo "Testing webcam endpoints..."
          # Test JPG endpoint for sample airport (assuming kspb exists)
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" > /tmp/webcam_test.jpg 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.jpg 2>/dev/null || stat -c%s /tmp/webcam_test.jpg 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=jpg" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/jpeg"; then
              echo "✓ Webcam JPG endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.jpg
            else
              echo "⚠️  Webcam JPG endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "⚠️  Webcam JPG endpoint not accessible (may be expected if no webcams configured)"
          fi
          
          # Test WEBP endpoint
          if curl -f -s --max-time 10 "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" > /tmp/webcam_test.webp 2>/dev/null; then
            SIZE=$(stat -f%z /tmp/webcam_test.webp 2>/dev/null || stat -c%s /tmp/webcam_test.webp 2>/dev/null || echo "0")
            CTYPE=$(curl -sI "http://127.0.0.1:${APP_PORT}/webcam.php?id=kspb&cam=0&fmt=webp" 2>/dev/null | grep -i 'content-type' | cut -d' ' -f2 | tr -d '\r' || echo "")
            if [ "$SIZE" -gt "0" ] && echo "$CTYPE" | grep -qi "image/webp"; then
              echo "✓ Webcam WEBP endpoint working (size: ${SIZE} bytes, type: ${CTYPE})"
              rm -f /tmp/webcam_test.webp
            else
              echo "⚠️  Webcam WEBP endpoint returned invalid response (size: ${SIZE}, type: ${CTYPE})"
            fi
          else
            echo "ℹ️  Webcam WEBP endpoint not accessible (may be expected if WEBP not generated yet)"
          fi
          
          EOF

